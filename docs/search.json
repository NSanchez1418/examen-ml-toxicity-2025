[
  {
    "objectID": "tweets_classification.html",
    "href": "tweets_classification.html",
    "title": "Tweets Classification",
    "section": "",
    "text": "import sys, matplotlib\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Matplotlib backend:\", matplotlib.get_backend())\n\nimport matplotlib.pyplot as plt\nplt.plot([0,1,2],[0,1,0])\nplt.title(\"Gráfico de prueba\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "tweets_classification.html#objetivo",
    "href": "tweets_classification.html#objetivo",
    "title": "Tweets Classification",
    "section": "## Objetivo",
    "text": "## Objetivo\nClasificar cuentas en real vs. bot a partir de tweets (texto + metadatos), creando un target heurístico y armando un pipeline con TF-IDF + OneHotEncoder + StandardScaler + LogisticRegression."
  },
  {
    "objectID": "tweets_classification.html#dataset",
    "href": "tweets_classification.html#dataset",
    "title": "Tweets Classification",
    "section": "Dataset",
    "text": "Dataset\nFuente: CSV público (Twitter) – 158,873 filas; 26 columnas."
  },
  {
    "objectID": "tweets_classification.html#selección-del-target-user_type",
    "href": "tweets_classification.html#selección-del-target-user_type",
    "title": "Tweets Classification",
    "section": "Selección del target (user_type)",
    "text": "Selección del target (user_type)\nSe definió user_type ∈ {bot, real} mediante una heurística débil basada en señales (foto de perfil, seguidores, edad de cuenta, etc.). Esta etiqueta sirve para entrenar un modelo que generalice el patrón."
  },
  {
    "objectID": "tweets_classification.html#columnas-eliminadas",
    "href": "tweets_classification.html#columnas-eliminadas",
    "title": "Tweets Classification",
    "section": "Columnas eliminadas",
    "text": "Columnas eliminadas\nSe eliminaron IDs, URLs, timestamps crudos y campos redundantes (p. ej., tweetId, tweetUrl, authorProfilePic, createdAt, mentions, hashtag s, source, etc.) por no aportar valor predictivo directo."
  },
  {
    "objectID": "tweets_classification.html#pipeline",
    "href": "tweets_classification.html#pipeline",
    "title": "Tweets Classification",
    "section": "Pipeline",
    "text": "Pipeline\n\nTexto: TfidfVectorizer(stop_words=spanish, ngram_range=(1,2), min_df=5, max_df=0.90, max_features=50000)\nCategóricas: OneHotEncoder(drop='if_binary') en isReply, authorVerified\nNuméricas: StandardScaler(with_mean=False) en authorFollowers, mentions_count, hashtags_count, time_response, content_length\nModelo: LogisticRegression(max_iter=10000, class_weight={'bot': 2.5, 'real': 1.0})"
  },
  {
    "objectID": "tweets_classification.html#exploración-del-target-potencial",
    "href": "tweets_classification.html#exploración-del-target-potencial",
    "title": "Tweets Classification",
    "section": "Exploración del target potencial",
    "text": "Exploración del target potencial\n\n# Revisar la variación de sentiment_polarity\nprint(df['sentiment_polarity'].describe())\n\nprint(\"\\nValores únicos de sentiment_polarity y su frecuencia:\")\nprint(df['sentiment_polarity'].value_counts().head(10))"
  },
  {
    "objectID": "tweets_classification.html#borrar-columnas-irrelevantes",
    "href": "tweets_classification.html#borrar-columnas-irrelevantes",
    "title": "Tweets Classification",
    "section": "Borrar columnas irrelevantes",
    "text": "Borrar columnas irrelevantes\n\n# === Eliminación explícita de columnas irrelevantes ===\nirrelevantes = [\n    'tweetId', 'tweetUrl',                 # IDs/URLs\n    'authorId', 'authorName', 'authorUsername', 'authorProfilePic',\n    'replyTo', 'conversationId', 'inReplyToId',                      # IDs de conversación\n    'createdAt', 'Date', 'authorJoinDate',                           # timestamps en texto\n    'mentions', 'hashtags',                                          # ya tenemos *_count\n    'source'                                                         # casi constante\n    # 'content_length'  # &lt;- si NO quieres usarla, descomenta y se elimina\n]\n\n# Columnas relevantes que conservaremos\nrelevantes = [\n    'content', 'isReply', 'authorVerified', 'has_profile_picture',\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'time_response', 'content_length',\n    'sentiment_polarity'\n]\n\ncols_existentes = [c for c in irrelevantes if c in df.columns]\ndf_clean = df.drop(columns=cols_existentes).copy()\ndf_clean = df_clean[relevantes].copy()\n\nprint(\"Eliminadas:\", len(cols_existentes), \"→\", cols_existentes)\nprint(\"Shape original:\", df.shape, \"→ Shape limpio:\", df_clean.shape)\ndisplay(df_clean.head(3))"
  },
  {
    "objectID": "tweets_classification.html#crear-target-user_type",
    "href": "tweets_classification.html#crear-target-user_type",
    "title": "Tweets Classification",
    "section": "Crear target “user_type”",
    "text": "Crear target “user_type”\n\n# Heurística para etiquetar \"bot\" vs \"real\"\ndef label_user_type(row):\n    score = 0\n    # señales fuertes\n    if not row['has_profile_picture']:\n        score += 2\n    if row['authorFollowers'] &lt; 50:\n        score += 2\n    if row['account_age_days'] &lt; 60:\n        score += 2\n    # señales adicionales\n    if row['mentions_count'] &gt;= 3:\n        score += 1\n    if row['hashtags_count'] &gt;= 3:\n        score += 1\n    if row['content_length'] &lt; 20:\n        score += 1\n    if row['isReply']:\n        score += 1\n    # verificado resta (suele ser humano/organización)\n    if row['authorVerified']:\n        score -= 2\n\n    return \"bot\" if score &gt;= 4 else \"real\"\n\ndf_bot = df_clean.copy()\ndf_bot['user_type'] = df_bot.apply(label_user_type, axis=1)\n\nprint(\"Distribución user_type:\")\nprint(df_bot['user_type'].value_counts())\nprint(\"\\nProporciones:\")\nprint(df_bot['user_type'].value_counts(normalize=True).round(3))\n\n# --- Gráfico: Distribución de clases (user_type) ---\nimport matplotlib.pyplot as plt\n\nax = df_bot['user_type'].value_counts().plot(kind='bar')\nplt.title('Distribución de clases (user_type)')\nplt.xlabel('Clase')\nplt.ylabel('Frecuencia')\nplt.tight_layout()\nplt.show()\n\n# (opcional) guardar la figura para tu informe/presentación\nfig = plt.gcf()\nfig.savefig(\"docs/fig_user_type_dist.png\", dpi=150, bbox_inches='tight')"
  },
  {
    "objectID": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "href": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "title": "Tweets Classification",
    "section": "Seleccion de columnas para entrenar",
    "text": "Seleccion de columnas para entrenar\n\n# Definición de columnas\ntext_col = 'content'\ntarget_col = 'user_type'\n\ncat_cols = ['isReply', 'authorVerified']\nnum_cols = ['authorFollowers', 'mentions_count', 'hashtags_count', 'time_response', 'content_length']\n\nkeep_cols = [text_col, target_col] + cat_cols + num_cols\ndf_train = df_bot[keep_cols].copy()\n\n# Limpieza mínima\ndf_train[text_col] = df_train[text_col].fillna(\"\").astype(str).str.strip()\ndf_train = df_train[df_train[text_col] != \"\"]\n\nprint(\"Shape final para entrenamiento:\", df_train.shape)\ndisplay(df_train.head(3))"
  },
  {
    "objectID": "tweets_classification.html#train-test-split-estratificado",
    "href": "tweets_classification.html#train-test-split-estratificado",
    "title": "Tweets Classification",
    "section": "Train/ Test Split estratificado",
    "text": "Train/ Test Split estratificado\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_train.drop(columns=[target_col])\ny = df_train[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\nprint(\"Distrib y_train:\\n\", y_train.value_counts(normalize=True).round(3))"
  },
  {
    "objectID": "tweets_classification.html#limpieza-de-texto-lematización-spacy",
    "href": "tweets_classification.html#limpieza-de-texto-lematización-spacy",
    "title": "Tweets Classification",
    "section": "Limpieza de texto + lematización (spaCy)",
    "text": "Limpieza de texto + lematización (spaCy)\n\n# --- Limpieza de texto + lematización en español (spaCy) ---\nimport re\nimport sys\nimport subprocess\nimport spacy\n\n# --- spaCy ES: carga; solo descarga si NO está instalado ---\nimport sys, subprocess, spacy\ntry:\n    nlp = spacy.load(\"es_core_news_sm\")\nexcept OSError:\n    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"], check=True)\n    nlp = spacy.load(\"es_core_news_sm\")\n\n\n# 2) Stopwords (puedes combinarlas con NLTK si quieres)\nsp_stop = nlp.Defaults.stop_words\n\n# 3) Limpiadores básicos de Twitter\n_url   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n_user  = re.compile(r\"@\\w+\")              # elimina @usuario\n_hash  = re.compile(r\"#\")                  # quita '#' pero deja la palabra\n_emoji = re.compile(r\"[^\\w\\sáéíóúñüÁÉÍÓÚÑÜ]\")  # limpia signos/emojis (ajustable)\n\ndef clean_text(text: str) -&gt; str:\n    text = text.lower().strip()\n    text = _url.sub(\" \", text)\n    text = _user.sub(\" \", text)\n    text = _hash.sub(\"\", text)             # \"#vacunas\" -&gt; \"vacunas\"\n    text = _emoji.sub(\" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\n# 4) Analizador para sklearn que devuelve tokens lematizados sin stopwords\ndef spacy_analyzer(doc: str):\n    doc = clean_text(doc)\n    sp = nlp(doc)\n    return [\n        tok.lemma_\n        for tok in sp\n        if tok.is_alpha                  # solo letras\n        and tok.lemma_ not in sp_stop    # sin stopwords\n        and len(tok.lemma_) &gt; 2          # descarta tokens muy cortos\n    ]\n\n# (opcional) prueba rápida\n# ejemplo = \"RT @usuario: ¡Viendo #Debate2025 en https://x.com! Los candidatos hablando...\"\n# print(spacy_analyzer(ejemplo))"
  },
  {
    "objectID": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "href": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "title": "Tweets Classification",
    "section": "ColumnTransformer (TF-IDF + OHE + escala numérica)",
    "text": "ColumnTransformer (TF-IDF + OHE + escala numérica)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords as nltk_stopwords   # ⬅️ NUEVO\n\nspanish_stop = nltk_stopwords.words('spanish')        # ⬅️ NUEVO\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntext_vectorizer = TfidfVectorizer(\n    analyzer=spacy_analyzer,   # usamos nuestro analizador con limpieza+lemmas\n    ngram_range=(1, 2),\n    min_df=5,\n    max_df=0.90,\n    max_features=50_000,\n    strip_accents='unicode',\n    lowercase=False            # ya pasamos a minúsculas en clean_text()\n)\n\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', text_vectorizer, text_col),\n        ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary'), cat_cols),\n        ('num', StandardScaler(with_mean=False), num_cols)\n    ],\n    remainder='drop',\n    sparse_threshold=0.3\n)"
  },
  {
    "objectID": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "href": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "title": "Tweets Classification",
    "section": "Pipelines (LR con texto+meta, y NB solo texto)",
    "text": "Pipelines (LR con texto+meta, y NB solo texto)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Regresión Logística (texto + categóricas + numéricas)\npipe_lr = Pipeline([\n    ('prep', preprocessor),\n    ('clf', LogisticRegression(\n        max_iter=10000,\n        class_weight={'bot': 2.5, 'real': 1.0},\n        random_state=42\n    ))\n])\n\n\n# Naive Bayes SOLO TEXTO (baseline)\npipe_nb_text_only = Pipeline([\n    ('tfidf', text_vectorizer),\n    ('nb', MultinomialNB())\n])"
  },
  {
    "objectID": "tweets_classification.html#entrenar-y-evaluar",
    "href": "tweets_classification.html#entrenar-y-evaluar",
    "title": "Tweets Classification",
    "section": "Entrenar y Evaluar",
    "text": "Entrenar y Evaluar\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n# 1) Logistic Regression con todas las features\npipe_lr.fit(X_train, y_train)\ny_pred_lr = pipe_lr.predict(X_test)\nprint(\"=== Logistic Regression (texto + meta) ===\")\nprint(classification_report(y_test, y_pred_lr, digits=3))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr)\nplt.show()\n\n# 2) Naive Bayes solo texto\npipe_nb_text_only.fit(X_train[text_col], y_train)\ny_pred_nb = pipe_nb_text_only.predict(X_test[text_col])\nprint(\"\\n=== MultinomialNB (solo texto) ===\")\nprint(classification_report(y_test, y_pred_nb, digits=3))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_nb)\nplt.show()"
  },
  {
    "objectID": "text_classification.html",
    "href": "text_classification.html",
    "title": "Text classification",
    "section": "",
    "text": "Importar librerías\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_20newsgroups\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# vectorizacion textual\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n\n\nCargar el dataset\n\ncategorias = ['comp.graphics','comp.sys.mac.hardware','rec.sport.baseball','talk.politics.misc']\n\nnewsgroups = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\nprint(newsgroups.target_names)\n\n\n\nFeatures y target\n\nX_text = newsgroups.data #features\ny = newsgroups.target #target\n\n\n\nTrain-test split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_text,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n\n\nVectorizacion de text\n\nstopwords = stopwords.words('english')\n# vectorizer = CountVectorizer(stop_words=stopwords) \nvectorizer = TfidfVectorizer(stop_words=stopwords)\n\n\n\nModelo\n\nmodel = MultinomialNB()\n\n\n\nDefinición del Pipeline\n\npipeline = Pipeline(\n    [\n        ('vectorizacion',vectorizer),\n        ('classfier',model)\n    ]\n)\n\n\n\nFit del modelo\n\npipeline.fit(X_train,y_train)\n\n\n\nPredicción\n\ny_pred = pipeline.predict(X_test)\n\n\n\nReporte de clasificación\n\nprint(\"REPORTE DE CLASIFICACION\\n\",classification_report(y_test,y_pred,target_names=newsgroups.target_names))\n\n\n\nMatriz de confusión\n\nConfusionMatrixDisplay(\n    confusion_matrix(y_test,y_pred),display_labels=newsgroups.target_names\n).plot(\n    xticks_rotation='vertical'\n)"
  },
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "test",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')"
  },
  {
    "objectID": "nn.html#fit-aprende-el-vocabulari",
    "href": "nn.html#fit-aprende-el-vocabulari",
    "title": "test",
    "section": "fit: Aprende el vocabulari",
    "text": "fit: Aprende el vocabulari\n\n\nCode\nX = vectorizer.fit_transform(docs)\n\n\n\n\nCode\nvectorizer.get_feature_names_out()"
  },
  {
    "objectID": "nn.html#transform-crea-la-matriz-de-conteos-sparse-matrix",
    "href": "nn.html#transform-crea-la-matriz-de-conteos-sparse-matrix",
    "title": "test",
    "section": "transform: Crea la matriz de conteos (sparse matrix)",
    "text": "transform: Crea la matriz de conteos (sparse matrix)\nCada fila representa un documento, cada columna una palabra del vocabulario\n\n\nCode\nX.toarray()"
  },
  {
    "objectID": "nn.html#visualizar-la-matriz-como-dataframe",
    "href": "nn.html#visualizar-la-matriz-como-dataframe",
    "title": "test",
    "section": "Visualizar la matriz como DataFrame",
    "text": "Visualizar la matriz como DataFrame\n\n\nCode\ndf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)"
  },
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "knn.html#generación-de-pipeline-por-tipo-de-variable",
    "href": "knn.html#generación-de-pipeline-por-tipo-de-variable",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Generación de pipeline por tipo de variable",
    "text": "Generación de pipeline por tipo de variable\n\ncategorical_pipeline = Pipeline([\n    ('imputacion_cat',SimpleImputer(strategy='most_frequent')),\n    ('encodage_cat',OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n# Mujer, Hombre =&gt; (1,0) =&gt; StandarScaler() NO HACER\n\nnumerical_pipeline = Pipeline([\n    ('imputacion_num',SimpleImputer(strategy='mean')),\n    ('escalamiento',StandardScaler())\n])"
  },
  {
    "objectID": "knn.html#aplicar-columntransformer",
    "href": "knn.html#aplicar-columntransformer",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Aplicar ColumnTransformer",
    "text": "Aplicar ColumnTransformer\n\npreprocessor = ColumnTransformer([\n    ('cat',categorical_pipeline,cat_cols),\n    ('num',numerical_pipeline,num_cols)\n])"
  },
  {
    "objectID": "knn.html#definir-el-param_grid",
    "href": "knn.html#definir-el-param_grid",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Definir el param_grid",
    "text": "Definir el param_grid\n\nparam_grid = {\n    'classificador__n_neighbors':list(range(1,5)),\n    'classificador__weights':['uniform','distance'],\n    'classificador__metric':['minkowski','euclidean','manhattan']\n}"
  },
  {
    "objectID": "knn.html#realizar-el-gridseach",
    "href": "knn.html#realizar-el-gridseach",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Realizar el Gridseach",
    "text": "Realizar el Gridseach\n\ngrid = GridSearchCV(pipeline,param_grid,cv=5,scoring='f1')\ngrid.fit(X_train,y_train)"
  },
  {
    "objectID": "knn.html#obtener-el-mejor-best_estimator",
    "href": "knn.html#obtener-el-mejor-best_estimator",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Obtener el mejor best_estimator",
    "text": "Obtener el mejor best_estimator\n\nbest_model = grid.best_estimator_"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proyecto Final — Machine Learning",
    "section": "",
    "text": "Bienvenido\nEste sitio contiene el desarrollo de dos tareas de aprendizaje supervisado:\n\nClasificación: incidentes (accidentes con heridos) con Regresión Logística.\nRegresión: (PM2.5) — lo agregaremos como segunda página.\n\n\nAutor: Noe Sanchez\nAño: 2025\nCurso: ML"
  },
  {
    "objectID": "clasificacion_incidentes.html",
    "href": "clasificacion_incidentes.html",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Code\n# 1. Importar librerías\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    ConfusionMatrixDisplay, RocCurveDisplay\n)\n\n# 2. Cargar dataset\ndf = pd.read_csv(\"data/incidentes_clasificacion_ready.csv\")\n\ny = df[\"accidente_con_heridos\"]\nX = df.drop(columns=[\"accidente_con_heridos\"])\n\nprint(\"Dimensiones:\", X.shape)\nprint(\"Distribución de clases:\")\nprint(y.value_counts())\n\n# 3. Dividir en train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 4. Entrenar modelo SIN pipeline\nmodel = LogisticRegression(max_iter=100000, class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\n\n# 5. Predicciones\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:,1]\n\n# 6. Métricas\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n==== Resultados SIN Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\n# 7. Matriz de confusión\nConfusionMatrixDisplay.from_predictions(y_test, y_pred)\nplt.title(\"Matriz de Confusión — Sin Pipeline\")\nplt.show()\n\n# 8. Curva ROC\nRocCurveDisplay.from_predictions(y_test, y_proba)\nplt.title(\"Curva ROC — Sin Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# ======== CON PIPELINE ========\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=100000, class_weight=\"balanced\"))\n])\n\npipe.fit(X_train, y_train)\ny_pred_pipe = pipe.predict(X_test)\ny_proba_pipe = pipe.predict_proba(X_test)[:,1]\n\naccuracy = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe, zero_division=0)\nrecall = recall_score(y_test, y_pred_pipe, zero_division=0)\nf1 = f1_score(y_test, y_pred_pipe, zero_division=0)\n\nprint(\"\\n==== Resultados CON Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_pipe)\nplt.title(\"Matriz de Confusión — Con Pipeline\")\nplt.show()\n\nRocCurveDisplay.from_predictions(y_test, y_proba_pipe)\nplt.title(\"Curva ROC — Con Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\nResumen del problema.\nSe modeló la probabilidad de accidente con heridos (clase 1) usando Regresión Logística, con un flujo estándar: carga del dataset, división train_test_split con stratify, entrenamiento sin y con Pipeline(StandardScaler + LogisticRegression), evaluación con Accuracy, Precision, Recall, F1 y visualizaciones (Matriz de confusión y Curva ROC).\nResultados clave (lo observado en este experimento): - El accuracy es poco informativo dado el fuerte desbalance (la clase positiva es muy rara).\n- Se obtuvo Recall relativamente alto (capacidad de encontrar positivos) a costa de Precision baja (muchos falsos positivos).\n- Con y sin Pipeline las métricas son muy similares —esperable en un modelo lineal cuando las variables ya están en rangos similares y el desbalance domina el comportamiento."
  },
  {
    "objectID": "clasificacion_incidentes.html#conclusiones-e-interpretación",
    "href": "clasificacion_incidentes.html#conclusiones-e-interpretación",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Resumen del problema.\nSe modeló la probabilidad de accidente con heridos (clase 1) usando Regresión Logística, con un flujo estándar: carga del dataset, división train_test_split con stratify, entrenamiento sin y con Pipeline(StandardScaler + LogisticRegression), evaluación con Accuracy, Precision, Recall, F1 y visualizaciones (Matriz de confusión y Curva ROC).\nResultados clave (lo observado en este experimento): - El accuracy es poco informativo dado el fuerte desbalance (la clase positiva es muy rara).\n- Se obtuvo Recall relativamente alto (capacidad de encontrar positivos) a costa de Precision baja (muchos falsos positivos).\n- Con y sin Pipeline las métricas son muy similares —esperable en un modelo lineal cuando las variables ya están en rangos similares y el desbalance domina el comportamiento."
  },
  {
    "objectID": "altair.html",
    "href": "altair.html",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\nGráfico de barras\ncount() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n)\n\n\nmean() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\nGráfico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n).interactive()"
  },
  {
    "objectID": "altair.html#importar-el-dataset",
    "href": "altair.html#importar-el-dataset",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())"
  },
  {
    "objectID": "altair.html#visualización-en-altair",
    "href": "altair.html#visualización-en-altair",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\nGráfico de barras\ncount() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n)\n\n\nmean() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\nGráfico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n).interactive()"
  },
  {
    "objectID": "bagofwords.html",
    "href": "bagofwords.html",
    "title": "Bag of Words (BoW)",
    "section": "",
    "text": "from sklearn.feature_extraction.text import CountVectorizer #BoW\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization"
  },
  {
    "objectID": "bagofwords.html#ejemplo-de-función-de-limpieza",
    "href": "bagofwords.html#ejemplo-de-función-de-limpieza",
    "title": "Bag of Words (BoW)",
    "section": "Ejemplo de función de limpieza",
    "text": "Ejemplo de función de limpieza\n\ndef get_wordnet_pos(treebank_tag):\n    # print(\"treebank_tag\",treebank_tag)\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN \n\n\ndef preprocessing_document(doc):\n    #1 transformar en minusculas\n    doc = doc.lower()\n    \n    #2 tokenizar\n    tokens = word_tokenize(doc)\n\n    #3 obtener lematizacion con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n\n    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION\n\n    #4 filtrar numeros\n    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]\n\n    #5 Lematizacion usando el pos\n    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "bagofwords.html#fit_transform",
    "href": "bagofwords.html#fit_transform",
    "title": "Bag of Words (BoW)",
    "section": "Fit_transform",
    "text": "Fit_transform\nAprende del Corpus (vocabulario) y del ser caso elimina las stop-words\n\ncorpus_cleaned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus)"
  },
  {
    "objectID": "bagofwords.html#obtener-las-palabras-finales",
    "href": "bagofwords.html#obtener-las-palabras-finales",
    "title": "Bag of Words (BoW)",
    "section": "Obtener las palabras finales",
    "text": "Obtener las palabras finales\n\nvectorizer.get_feature_names_out()"
  },
  {
    "objectID": "hac.html",
    "href": "hac.html",
    "title": "Hierarchical Aglomerative Clustering (HAC)",
    "section": "",
    "text": "Importar librerias\n\nimport pandas as pd\nimport altair as alt\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\n\nCargar el Dataset\n\niris = load_iris()\nX = pd.DataFrame(iris.data,columns=iris.feature_names)[['sepal length (cm)','sepal width (cm)']]\nX"
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "K-means",
    "section": "",
    "text": "import altair as alt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n#---\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow",
    "href": "kmeans.html#método-del-codo-elbow",
    "title": "K-means",
    "section": "Método del codo (Elbow)",
    "text": "Método del codo (Elbow)\n\nCalcular el Sum of Squared Errors (SSE)\n\nsse = [] #inertia\nk_range = range(2,11) #valores posibles de k\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k,random_state=42)\n    kmeans.fit(df[['age','annual_income($)']])\n    sse.append(kmeans.inertia_)\n\nprint(\"Inertia:\\n\",sse)\n\n\n\nVisualizar el Sum of Squared Errors (SSE) vs K\n\nelbow_df = pd.DataFrame(\n    {\n        'K':list(k_range),\n        'SSE':sse\n    }\n)\n\n# elbow_df\n\nalt.Chart(elbow_df).mark_line(point=True).encode(\n    alt.X(\"K\"),\n    alt.Y(\"SSE\"),\n    tooltip=[\"K\",\"SSE\"]\n).properties(\n    title=\"Método del Codo\"\n).interactive()"
  },
  {
    "objectID": "kmeans.html#clusterización",
    "href": "kmeans.html#clusterización",
    "title": "K-means",
    "section": "Clusterización",
    "text": "Clusterización\n\nk_clusters = 3\nclusters = KMeans(n_clusters=k_clusters,random_state=42)\ndf['clusters'] = clusters.fit_predict(df[['age','annual_income($)']])\ndf.head()\n\n\nVisualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters:N'),\n    tooltip=['age','annual_income($)','clusters:N']\n).properties(\n    title='CLUSTERIZACION'\n).interactive()"
  },
  {
    "objectID": "kmeans.html#columntransformer",
    "href": "kmeans.html#columntransformer",
    "title": "K-means",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\n\nnum_cols = ['age','annual_income($)']\n\npreprocessor = ColumnTransformer(\n    [\n        ('standar',StandardScaler(),num_cols)\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#pipeline",
    "href": "kmeans.html#pipeline",
    "title": "K-means",
    "section": "Pipeline",
    "text": "Pipeline\n\npipeline = Pipeline(\n    [\n        ('scaler',preprocessor),\n        ('kmeans',KMeans(random_state=42))\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow-silhouette",
    "href": "kmeans.html#método-del-codo-elbow-silhouette",
    "title": "K-means",
    "section": "Método del codo (Elbow) + Silhouette",
    "text": "Método del codo (Elbow) + Silhouette\n\nCalcular el Sum of Squared Errors (SSE) + Silhouette\n\nk_range = range(2,11)\nsse_standar = []\nsilhouette_scores = []\n\nfor k in k_range:\n    pipeline.set_params(kmeans__n_clusters=k)\n    pipeline.fit(df[num_cols])\n    sse_standar.append(\n        pipeline.named_steps['kmeans'].inertia_\n    )\n\n    pred = pipeline.predict(df[num_cols])\n    print(\"K:\",k)\n    print(\"PRED\",pred)\n    score = silhouette_score(pipeline.named_steps['scaler'].transform(df[num_cols]),pred)\n    print(\"SILHOUETTE SCORE\",score)\n    print(\"\\n\")\n    silhouette_scores.append(score)\n\n#silhouette_score"
  },
  {
    "objectID": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "href": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "title": "K-means",
    "section": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K",
    "text": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K\n\nsse_standard_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SSE_STANDARD':sse_standar\n    }\n)\n\nsilhouette_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SILHOUETTE':silhouette_scores\n    }\n)\n\nviz_sse_standard = alt.Chart(sse_standard_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SSE_STANDARD'),\n    tooltip=['K',\"SSE_STANDARD\"]\n).properties(\n    title='METODO DEL CODO CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_silhouette = alt.Chart(silhouette_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SILHOUETTE'),\n    tooltip=['K',\"SILHOUETTE\"]\n).properties(\n    title='SILHOUETTE SCORE CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_sse_standard | viz_silhouette"
  },
  {
    "objectID": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "href": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "title": "K-means",
    "section": "Clusterizacion con Pipeline + StandardScaler()",
    "text": "Clusterizacion con Pipeline + StandardScaler()\n\nk_optimo = k_range[silhouette_scores.index(max(silhouette_scores))]\nprint(\"K_OPTIMO\",k_optimo)\n\npipeline.set_params(kmeans__n_clusters=k_optimo)\ndf['clusters_standard'] = pipeline.fit_predict(df[num_cols])\ndf.head()"
  },
  {
    "objectID": "kmeans.html#visualizar-la-clusterización-1",
    "href": "kmeans.html#visualizar-la-clusterización-1",
    "title": "K-means",
    "section": "Visualizar la clusterización",
    "text": "Visualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters_standard:N'),\n    tooltip=['age','annual_income($)','clusters_standard:N']\n).properties(\n    title='CLUSTERIZACION CON PIPELINE + STANDARSCALER()'\n).interactive()"
  },
  {
    "objectID": "ml_text.html",
    "href": "ml_text.html",
    "title": "NLP Key Concepts",
    "section": "",
    "text": "Cargar librerías\n\nimport pandas as pd\nimport requests\n\n# Natural Language Toolkit\nimport nltk\n# downloading some additional packages and corpora\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n\n\nCargar data\n\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\nr = requests.get(url)\nr.encoding = 'utf-8'\n\nstory = r.text\nstory\n\n\n\nTokenización\n\nfrom nltk import word_tokenize\n\nwords = word_tokenize(story)\nwords[:20]\n\n\n\nStemming and Lemmatization\n\nfrom nltk.stem import PorterStemmer as stemmer\nfrom nltk.stem import WordNetLemmatizer as lemmatizer\nfrom nltk.corpus import wordnet\n\npalabra = \"changing\"\nprint(\"PALABRA: \", palabra)\n\n#stemming\nprint(\"STEMMING: \",stemmer().stem(palabra))\n\n#lemmatizatio\nprint(\"LEMMATIZATION: \", lemmatizer().lemmatize(palabra,pos=wordnet.VERB))\n\n\n\nPART OF SPEECH - POS TAG\n\nfrom nltk import pos_tag\npos = pos_tag(words)\npos[:20]\n\n\n\nSTOP WORDS\n\nfrom nltk.corpus import stopwords as stop\n\nstopwords = stop.words(\"english\")\nstopwords[:20]\n\n# for item in stop.words(\"spanish\"):\n#     print(item)\n\n\n\nSTOP WORDS IN STORY\n\ntokens = nltk.word_tokenize(story.lower())\n# tokens[:20]\n\n# Limpieza depende del contexto de su problemática\n\n# limpieza de numeros\nlettertokens = [word for word in tokens if word.isalpha()]\n\nwithout_stopwords = [word for word in lettertokens if word not in stopwords]\n\nwithout_stopwords[:20]"
  },
  {
    "objectID": "regresion_pm25.html#cargar-y-explorar",
    "href": "regresion_pm25.html#cargar-y-explorar",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "1) Cargar y explorar",
    "text": "1) Cargar y explorar\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Ruta al dataset limpio que generamos antes\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head(8))\n\n# Nos quedamos SOLO con columnas numéricas (la consigna pide evitar categóricas)\nnum = df.select_dtypes(include=[np.number]).copy()\nprint(\"Numéricas:\", list(num.columns))\ndisplay(num.describe().T)\n\n\n## 2) Seleccionar variables (X, y) y train/test split\nfrom sklearn.model_selection import train_test_split\n\n# Elegir la y:\n# Si existe una columna 'pm25', la usamos. Si no existe, tomamos la última columna numérica como fallback.\ntarget_name = \"pm25\" if \"pm25\" in num.columns else num.columns[-1]\n\ny = num[target_name].values\nX = num.drop(columns=[target_name]).values\n\nprint(\"Objetivo (y):\", target_name)\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\n## 3) Pipeline + LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),  # en regresión lineal no siempre es obligatorio, pero es buena práctica\n    (\"linreg\", LinearRegression())\n])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n\n## 4) Métricas (MSE y R²)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nr2  = r2_score(y_test, y_pred)\n\nprint(f\"MSE : {mse:,.4f}\")\nprint(f\"R²  : {r2:,.4f}\")\n\n\n## 5) Visualización — Real vs Predicho\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(y_test, y_pred, alpha=0.4)\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predicho\")\nplt.title(\"PM2.5 — Real vs Predicho\")\nminv = float(np.min([y_test.min(), y_pred.min()]))\nmaxv = float(np.max([y_test.max(), y_pred.max()]))\nplt.plot([minv, maxv], [minv, maxv])  # línea y=x\nplt.tight_layout()\nplt.show()\n\n\n## 6) Curva de aprendizaje (opcional recomendado)\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    pipe, X, y, cv=5, scoring=\"r2\", n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 6), shuffle=True, random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\nvalid_mean = valid_scores.mean(axis=1)\n\nplt.figure()\nplt.plot(train_sizes, train_mean, marker=\"o\", label=\"Train R²\")\nplt.plot(train_sizes, valid_mean, marker=\"s\", label=\"Valid R²\")\nplt.xlabel(\"Tamaño de entrenamiento\")\nplt.ylabel(\"R²\")\nplt.title(\"Curva de aprendizaje\")\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "regresion_pm25.html#conclusiones-regresión-pm2.5-quito",
    "href": "regresion_pm25.html#conclusiones-regresión-pm2.5-quito",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "Conclusiones — Regresión (PM2.5 Quito)",
    "text": "Conclusiones — Regresión (PM2.5 Quito)\n\n\nCode\n# --- Asegurar que el objetivo (y) sea la medida de PM2.5 ---\n# Ajusta aquí al nombre REAL de tu columna objetivo en data/pm25_ecuador_clean.csv\n# Ejemplos comunes: \"pm25_mean_anual\", \"pm25_max_anual\"\n# Si no existe, hace fallback a \"pm25\" si está; y si tampoco, a la última numérica.\n\nimport pandas as pd, numpy as np\n\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\nnum = df.select_dtypes(include=[np.number]).copy()\n\n# --- Elige tu objetivo aquí ---\ntarget_candidates = [\"pm25_mean_anual\", \"pm25_max_anual\", \"pm25\"]\ntarget_name = None\nfor c in target_candidates:\n    if c in num.columns:\n        target_name = c\n        break\nif target_name is None:\n    target_name = num.columns[-1]  # fallback\n\nX = num.drop(columns=[target_name]).values\ny = num[target_name].values\n\nprint(f\"Objetivo (y): {target_name}\")\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)"
  },
  {
    "objectID": "regresion_pm25.html#resumen-del-flujo-aplicado.",
    "href": "regresion_pm25.html#resumen-del-flujo-aplicado.",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "Resumen del flujo aplicado.",
    "text": "Resumen del flujo aplicado.\nSplit: train_test_split(…, test_size=0.25, random_state=42)\nModelo (Pipeline): StandardScaler() + LinearRegression()\nMétricas: MSE y R² (proporción de varianza explicada).\nGráfico: Real vs Predicho con línea identidad.\nResultados de esta corrida (ejemplo):\nMSE bajo y R² alto indican buen ajuste en el agregado anual por estación. Si no obtienes métricas satisfactorias, revisa el objetivo y las columnas numéricas disponibles.\nInterpretación.\nEn datos anuales por estación, la relación puede ser casi lineal, por eso LinearRegression funciona bien.\nPara análisis más finos (intraanuales), agrega variables meteorológicas y de tráfico; prueba modelos no lineales (RandomForest/GBM)."
  },
  {
    "objectID": "tfidf.html",
    "href": "tfidf.html",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "",
    "text": "from sklearn.feature_extraction.text import TfidfVectorizer #tfidf\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization"
  },
  {
    "objectID": "tfidf.html#ejemplo-de-función-de-limpieza",
    "href": "tfidf.html#ejemplo-de-función-de-limpieza",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Ejemplo de función de limpieza",
    "text": "Ejemplo de función de limpieza\n\ndef get_wordnet_pos(treebank_tag):\n    # print(\"treebank_tag\",treebank_tag)\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN \n\n\ndef preprocessing_document(doc):\n    #1 transformar en minusculas\n    doc = doc.lower()\n    \n    #2 tokenizar\n    tokens = word_tokenize(doc)\n\n    #3 obtener lematizacion con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n\n    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION\n\n    #4 filtrar numeros\n    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]\n\n    #5 Lematizacion usando el pos\n    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "tfidf.html#fit_transform",
    "href": "tfidf.html#fit_transform",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Fit_transform",
    "text": "Fit_transform\nAprende del Corpus (vocabulario) y del ser caso elimina las stop-words\n\ncorpus_cleaned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleaned)"
  },
  {
    "objectID": "tfidf.html#obtener-las-palabras-finales",
    "href": "tfidf.html#obtener-las-palabras-finales",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Obtener las palabras finales",
    "text": "Obtener las palabras finales\n\nvectorizer.get_feature_names_out()"
  }
]