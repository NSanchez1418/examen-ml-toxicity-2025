[
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "K-means",
    "section": "",
    "text": "import altair as alt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n#---\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow",
    "href": "kmeans.html#método-del-codo-elbow",
    "title": "K-means",
    "section": "Método del codo (Elbow)",
    "text": "Método del codo (Elbow)\n\nCalcular el Sum of Squared Errors (SSE)\n\nsse = [] #inertia\nk_range = range(2,11) #valores posibles de k\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k,random_state=42)\n    kmeans.fit(df[['age','annual_income($)']])\n    sse.append(kmeans.inertia_)\n\nprint(\"Inertia:\\n\",sse)\n\nInertia:\n [275802340.8465648, 103879919.55, 82797577.45817567, 38460868.83850575, 27144269.551419172, 19757179.065581053, 14468762.582687736, 11828359.755827505, 9333390.053670242]\n\n\n\n\nVisualizar el Sum of Squared Errors (SSE) vs K\n\nelbow_df = pd.DataFrame(\n    {\n        'K':list(k_range),\n        'SSE':sse\n    }\n)\n\n# elbow_df\n\nalt.Chart(elbow_df).mark_line(point=True).encode(\n    alt.X(\"K\"),\n    alt.Y(\"SSE\"),\n    tooltip=[\"K\",\"SSE\"]\n).properties(\n    title=\"Método del Codo\"\n).interactive()"
  },
  {
    "objectID": "kmeans.html#clusterización",
    "href": "kmeans.html#clusterización",
    "title": "K-means",
    "section": "Clusterización",
    "text": "Clusterización\n\nk_clusters = 3\nclusters = KMeans(n_clusters=k_clusters,random_state=42)\ndf['clusters'] = clusters.fit_predict(df[['age','annual_income($)']])\ndf.head()\n\n\n\n\n\n\n\n\nage\nannual_income($)\nclusters\n\n\n\n\n0\n17\n17123\n2\n\n\n1\n17\n18375\n2\n\n\n2\n16\n18557\n2\n\n\n3\n23\n17721\n2\n\n\n4\n22\n17291\n2\n\n\n\n\n\n\n\n\nVisualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters:N'),\n    tooltip=['age','annual_income($)','clusters:N']\n).properties(\n    title='CLUSTERIZACION'\n).interactive()"
  },
  {
    "objectID": "kmeans.html#columntransformer",
    "href": "kmeans.html#columntransformer",
    "title": "K-means",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\n\nnum_cols = ['age','annual_income($)']\n\npreprocessor = ColumnTransformer(\n    [\n        ('standar',StandardScaler(),num_cols)\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#pipeline",
    "href": "kmeans.html#pipeline",
    "title": "K-means",
    "section": "Pipeline",
    "text": "Pipeline\n\npipeline = Pipeline(\n    [\n        ('scaler',preprocessor),\n        ('kmeans',KMeans(random_state=42))\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow-silhouette",
    "href": "kmeans.html#método-del-codo-elbow-silhouette",
    "title": "K-means",
    "section": "Método del codo (Elbow) + Silhouette",
    "text": "Método del codo (Elbow) + Silhouette\n\nCalcular el Sum of Squared Errors (SSE) + Silhouette\n\nk_range = range(2,11)\nsse_standar = []\nsilhouette_scores = []\n\nfor k in k_range:\n    pipeline.set_params(kmeans__n_clusters=k)\n    pipeline.fit(df[num_cols])\n    sse_standar.append(\n        pipeline.named_steps['kmeans'].inertia_\n    )\n\n    pred = pipeline.predict(df[num_cols])\n    print(\"K:\",k)\n    print(\"PRED\",pred)\n    score = silhouette_score(pipeline.named_steps['scaler'].transform(df[num_cols]),pred)\n    print(\"SILHOUETTE SCORE\",score)\n    print(\"\\n\")\n    silhouette_scores.append(score)\n\n#silhouette_score\n\nK: 2\nPRED [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1]\nSILHOUETTE SCORE 0.5584253334367935\n\n\nK: 3\nPRED [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1]\nSILHOUETTE SCORE 0.48862751188830494\n\n\nK: 4\nPRED [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.6197880502586566\n\n\nK: 5\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5870741352565944\n\n\nK: 6\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 5 5 5 5 5 1 5\n 5 5 5 1 5 1 1 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5496403913716196\n\n\nK: 7\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 6 6 6 5 6 6 5 6 5 6 1 5\n 6 6 5 5 5 5 5 6 5 6 6 6 6 6 6 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5351154400530848\n\n\nK: 8\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 0 0 4 4 4 4 0 4 0 0 0 4 4 4 5 5 7 7 7 5 5 5 5 5 5 5 1 5\n 7 5 5 5 5 5 5 7 5 7 5 5 7 5 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.49714020005833653\n\n\nK: 9\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 0 0 4 4 4 4 0 4 0 0 0 4 4 4 8 5 7 7 7 8 5 5 8 5 5 5 8 8\n 7 5 8 8 5 8 8 7 8 7 5 5 7 5 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.48277202641477174\n\n\nK: 10\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 4 0 4 4 4 4 0 4 0 0 0 4 4 4 9 9 7 7 5 9 5 5 9 5 9 5 8 9\n 5 5 9 8 5 9 9 7 9 5 5 5 7 5 7 5 1 8 8 8 1 1 8 1 8 8 8 8 1 8 1 8 1 8 8 1 1\n 8 8 8 8 8 1 8 8 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.49114927636373373"
  },
  {
    "objectID": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "href": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "title": "K-means",
    "section": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K",
    "text": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K\n\nsse_standard_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SSE_STANDARD':sse_standar\n    }\n)\n\nsilhouette_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SILHOUETTE':silhouette_scores\n    }\n)\n\nviz_sse_standard = alt.Chart(sse_standard_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SSE_STANDARD'),\n    tooltip=['K',\"SSE_STANDARD\"]\n).properties(\n    title='METODO DEL CODO CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_silhouette = alt.Chart(silhouette_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SILHOUETTE'),\n    tooltip=['K',\"SILHOUETTE\"]\n).properties(\n    title='SILHOUETTE SCORE CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_sse_standard | viz_silhouette"
  },
  {
    "objectID": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "href": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "title": "K-means",
    "section": "Clusterizacion con Pipeline + StandardScaler()",
    "text": "Clusterizacion con Pipeline + StandardScaler()\n\nk_optimo = k_range[silhouette_scores.index(max(silhouette_scores))]\nprint(\"K_OPTIMO\",k_optimo)\n\npipeline.set_params(kmeans__n_clusters=k_optimo)\ndf['clusters_standard'] = pipeline.fit_predict(df[num_cols])\ndf.head()\n\nK_OPTIMO 4\n\n\n\n\n\n\n\n\n\nage\nannual_income($)\nclusters\nclusters_standard\n\n\n\n\n0\n17\n17123\n2\n2\n\n\n1\n17\n18375\n2\n2\n\n\n2\n16\n18557\n2\n2\n\n\n3\n23\n17721\n2\n2\n\n\n4\n22\n17291\n2\n2"
  },
  {
    "objectID": "kmeans.html#visualizar-la-clusterización-1",
    "href": "kmeans.html#visualizar-la-clusterización-1",
    "title": "K-means",
    "section": "Visualizar la clusterización",
    "text": "Visualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters_standard:N'),\n    tooltip=['age','annual_income($)','clusters_standard:N']\n).properties(\n    title='CLUSTERIZACION CON PIPELINE + STANDARSCALER()'\n).interactive()"
  },
  {
    "objectID": "tweets_classification.html",
    "href": "tweets_classification.html",
    "title": "Tweets Classification",
    "section": "",
    "text": "import sys, matplotlib\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Matplotlib backend:\", matplotlib.get_backend())\n\nimport matplotlib.pyplot as plt\nplt.plot([0,1,2],[0,1,0])\nplt.title(\"Gráfico de prueba\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "tweets_classification.html#objetivo",
    "href": "tweets_classification.html#objetivo",
    "title": "Tweets Classification",
    "section": "## Objetivo",
    "text": "## Objetivo\nClasificar cuentas en real vs. bot a partir de tweets (texto + metadatos), creando un target heurístico y armando un pipeline con TF-IDF + OneHotEncoder + StandardScaler + LogisticRegression."
  },
  {
    "objectID": "tweets_classification.html#dataset",
    "href": "tweets_classification.html#dataset",
    "title": "Tweets Classification",
    "section": "Dataset",
    "text": "Dataset\nFuente: CSV público (Twitter) – 158,873 filas; 26 columnas."
  },
  {
    "objectID": "tweets_classification.html#selección-del-target-user_type",
    "href": "tweets_classification.html#selección-del-target-user_type",
    "title": "Tweets Classification",
    "section": "Selección del target (user_type)",
    "text": "Selección del target (user_type)\nSe definió user_type ∈ {bot, real} mediante una heurística débil basada en señales (foto de perfil, seguidores, edad de cuenta, etc.). Esta etiqueta sirve para entrenar un modelo que generalice el patrón."
  },
  {
    "objectID": "tweets_classification.html#columnas-eliminadas",
    "href": "tweets_classification.html#columnas-eliminadas",
    "title": "Tweets Classification",
    "section": "Columnas eliminadas",
    "text": "Columnas eliminadas\nSe eliminaron IDs, URLs, timestamps crudos y campos redundantes (p. ej., tweetId, tweetUrl, authorProfilePic, createdAt, mentions, hashtag s, source, etc.) por no aportar valor predictivo directo."
  },
  {
    "objectID": "tweets_classification.html#pipeline",
    "href": "tweets_classification.html#pipeline",
    "title": "Tweets Classification",
    "section": "Pipeline",
    "text": "Pipeline\n\nTexto: TfidfVectorizer(stop_words=spanish, ngram_range=(1,2), min_df=5, max_df=0.90, max_features=50000)\nCategóricas: OneHotEncoder(drop='if_binary') en isReply, authorVerified\nNuméricas: StandardScaler(with_mean=False) en authorFollowers, mentions_count, hashtags_count, time_response, content_length\nModelo: LogisticRegression(max_iter=10000, class_weight={'bot': 2.5, 'real': 1.0})"
  },
  {
    "objectID": "tweets_classification.html#exploración-del-target-potencial",
    "href": "tweets_classification.html#exploración-del-target-potencial",
    "title": "Tweets Classification",
    "section": "Exploración del target potencial",
    "text": "Exploración del target potencial\n\n# Revisar la variación de sentiment_polarity\nprint(df['sentiment_polarity'].describe())\n\nprint(\"\\nValores únicos de sentiment_polarity y su frecuencia:\")\nprint(df['sentiment_polarity'].value_counts().head(10))"
  },
  {
    "objectID": "tweets_classification.html#borrar-columnas-irrelevantes",
    "href": "tweets_classification.html#borrar-columnas-irrelevantes",
    "title": "Tweets Classification",
    "section": "Borrar columnas irrelevantes",
    "text": "Borrar columnas irrelevantes\n\n# === Eliminación explícita de columnas irrelevantes ===\nirrelevantes = [\n    'tweetId', 'tweetUrl',                 # IDs/URLs\n    'authorId', 'authorName', 'authorUsername', 'authorProfilePic',\n    'replyTo', 'conversationId', 'inReplyToId',                      # IDs de conversación\n    'createdAt', 'Date', 'authorJoinDate',                           # timestamps en texto\n    'mentions', 'hashtags',                                          # ya tenemos *_count\n    'source'                                                         # casi constante\n    # 'content_length'  # &lt;- si NO quieres usarla, descomenta y se elimina\n]\n\n# Columnas relevantes que conservaremos\nrelevantes = [\n    'content', 'isReply', 'authorVerified', 'has_profile_picture',\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'time_response', 'content_length',\n    'sentiment_polarity'\n]\n\ncols_existentes = [c for c in irrelevantes if c in df.columns]\ndf_clean = df.drop(columns=cols_existentes).copy()\ndf_clean = df_clean[relevantes].copy()\n\nprint(\"Eliminadas:\", len(cols_existentes), \"→\", cols_existentes)\nprint(\"Shape original:\", df.shape, \"→ Shape limpio:\", df_clean.shape)\ndisplay(df_clean.head(3))"
  },
  {
    "objectID": "tweets_classification.html#crear-target-user_type",
    "href": "tweets_classification.html#crear-target-user_type",
    "title": "Tweets Classification",
    "section": "Crear target “user_type”",
    "text": "Crear target “user_type”\n\n# Heurística para etiquetar \"bot\" vs \"real\"\ndef label_user_type(row):\n    score = 0\n    # señales fuertes\n    if not row['has_profile_picture']:\n        score += 2\n    if row['authorFollowers'] &lt; 50:\n        score += 2\n    if row['account_age_days'] &lt; 60:\n        score += 2\n    # señales adicionales\n    if row['mentions_count'] &gt;= 3:\n        score += 1\n    if row['hashtags_count'] &gt;= 3:\n        score += 1\n    if row['content_length'] &lt; 20:\n        score += 1\n    if row['isReply']:\n        score += 1\n    # verificado resta (suele ser humano/organización)\n    if row['authorVerified']:\n        score -= 2\n\n    return \"bot\" if score &gt;= 4 else \"real\"\n\ndf_bot = df_clean.copy()\ndf_bot['user_type'] = df_bot.apply(label_user_type, axis=1)\n\nprint(\"Distribución user_type:\")\nprint(df_bot['user_type'].value_counts())\nprint(\"\\nProporciones:\")\nprint(df_bot['user_type'].value_counts(normalize=True).round(3))\n\n# --- Gráfico: Distribución de clases (user_type) ---\nimport matplotlib.pyplot as plt\n\nax = df_bot['user_type'].value_counts().plot(kind='bar')\nplt.title('Distribución de clases (user_type)')\nplt.xlabel('Clase')\nplt.ylabel('Frecuencia')\nplt.tight_layout()\nplt.show()\n\n# (opcional) guardar la figura para tu informe/presentación\nfig = plt.gcf()\nfig.savefig(\"docs/fig_user_type_dist.png\", dpi=150, bbox_inches='tight')"
  },
  {
    "objectID": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "href": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "title": "Tweets Classification",
    "section": "Seleccion de columnas para entrenar",
    "text": "Seleccion de columnas para entrenar\n\n# Definición de columnas\ntext_col = 'content'\ntarget_col = 'user_type'\n\ncat_cols = ['isReply', 'authorVerified']\nnum_cols = ['authorFollowers', 'mentions_count', 'hashtags_count', 'time_response', 'content_length']\n\nkeep_cols = [text_col, target_col] + cat_cols + num_cols\ndf_train = df_bot[keep_cols].copy()\n\n# Limpieza mínima\ndf_train[text_col] = df_train[text_col].fillna(\"\").astype(str).str.strip()\ndf_train = df_train[df_train[text_col] != \"\"]\n\nprint(\"Shape final para entrenamiento:\", df_train.shape)\ndisplay(df_train.head(3))"
  },
  {
    "objectID": "tweets_classification.html#train-test-split-estratificado",
    "href": "tweets_classification.html#train-test-split-estratificado",
    "title": "Tweets Classification",
    "section": "Train/ Test Split estratificado",
    "text": "Train/ Test Split estratificado\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_train.drop(columns=[target_col])\ny = df_train[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\nprint(\"Distrib y_train:\\n\", y_train.value_counts(normalize=True).round(3))"
  },
  {
    "objectID": "tweets_classification.html#limpieza-de-texto-lematización-spacy",
    "href": "tweets_classification.html#limpieza-de-texto-lematización-spacy",
    "title": "Tweets Classification",
    "section": "Limpieza de texto + lematización (spaCy)",
    "text": "Limpieza de texto + lematización (spaCy)\n\n# --- Limpieza de texto + lematización en español (spaCy) ---\nimport re\nimport sys\nimport subprocess\nimport spacy\n\n# --- spaCy ES: carga; solo descarga si NO está instalado ---\nimport sys, subprocess, spacy\ntry:\n    nlp = spacy.load(\"es_core_news_sm\")\nexcept OSError:\n    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"], check=True)\n    nlp = spacy.load(\"es_core_news_sm\")\n\n\n# 2) Stopwords (puedes combinarlas con NLTK si quieres)\nsp_stop = nlp.Defaults.stop_words\n\n# 3) Limpiadores básicos de Twitter\n_url   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n_user  = re.compile(r\"@\\w+\")              # elimina @usuario\n_hash  = re.compile(r\"#\")                  # quita '#' pero deja la palabra\n_emoji = re.compile(r\"[^\\w\\sáéíóúñüÁÉÍÓÚÑÜ]\")  # limpia signos/emojis (ajustable)\n\ndef clean_text(text: str) -&gt; str:\n    text = text.lower().strip()\n    text = _url.sub(\" \", text)\n    text = _user.sub(\" \", text)\n    text = _hash.sub(\"\", text)             # \"#vacunas\" -&gt; \"vacunas\"\n    text = _emoji.sub(\" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\n# 4) Analizador para sklearn que devuelve tokens lematizados sin stopwords\ndef spacy_analyzer(doc: str):\n    doc = clean_text(doc)\n    sp = nlp(doc)\n    return [\n        tok.lemma_\n        for tok in sp\n        if tok.is_alpha                  # solo letras\n        and tok.lemma_ not in sp_stop    # sin stopwords\n        and len(tok.lemma_) &gt; 2          # descarta tokens muy cortos\n    ]\n\n# (opcional) prueba rápida\n# ejemplo = \"RT @usuario: ¡Viendo #Debate2025 en https://x.com! Los candidatos hablando...\"\n# print(spacy_analyzer(ejemplo))"
  },
  {
    "objectID": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "href": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "title": "Tweets Classification",
    "section": "ColumnTransformer (TF-IDF + OHE + escala numérica)",
    "text": "ColumnTransformer (TF-IDF + OHE + escala numérica)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords as nltk_stopwords   # ⬅️ NUEVO\n\nspanish_stop = nltk_stopwords.words('spanish')        # ⬅️ NUEVO\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntext_vectorizer = TfidfVectorizer(\n    analyzer=spacy_analyzer,   # usamos nuestro analizador con limpieza+lemmas\n    ngram_range=(1, 2),\n    min_df=5,\n    max_df=0.90,\n    max_features=50_000,\n    strip_accents='unicode',\n    lowercase=False            # ya pasamos a minúsculas en clean_text()\n)\n\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', text_vectorizer, text_col),\n        ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary'), cat_cols),\n        ('num', StandardScaler(with_mean=False), num_cols)\n    ],\n    remainder='drop',\n    sparse_threshold=0.3\n)"
  },
  {
    "objectID": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "href": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "title": "Tweets Classification",
    "section": "Pipelines (LR con texto+meta, y NB solo texto)",
    "text": "Pipelines (LR con texto+meta, y NB solo texto)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Regresión Logística (texto + categóricas + numéricas)\npipe_lr = Pipeline([\n    ('prep', preprocessor),\n    ('clf', LogisticRegression(\n        max_iter=10000,\n        class_weight={'bot': 2.5, 'real': 1.0},\n        random_state=42\n    ))\n])\n\n\n# Naive Bayes SOLO TEXTO (baseline)\npipe_nb_text_only = Pipeline([\n    ('tfidf', text_vectorizer),\n    ('nb', MultinomialNB())\n])"
  },
  {
    "objectID": "tweets_classification.html#entrenar-y-evaluar",
    "href": "tweets_classification.html#entrenar-y-evaluar",
    "title": "Tweets Classification",
    "section": "Entrenar y Evaluar",
    "text": "Entrenar y Evaluar\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n# 1) Logistic Regression con todas las features\npipe_lr.fit(X_train, y_train)\ny_pred_lr = pipe_lr.predict(X_test)\nprint(\"=== Logistic Regression (texto + meta) ===\")\nprint(classification_report(y_test, y_pred_lr, digits=3))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr)\nplt.show()\n\n# 2) Naive Bayes solo texto\npipe_nb_text_only.fit(X_train[text_col], y_train)\ny_pred_nb = pipe_nb_text_only.predict(X_test[text_col])\nprint(\"\\n=== MultinomialNB (solo texto) ===\")\nprint(classification_report(y_test, y_pred_nb, digits=3))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_nb)\nplt.show()"
  }
]