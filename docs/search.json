[
  {
    "objectID": "tweets_classification.html",
    "href": "tweets_classification.html",
    "title": "Tweets Classification",
    "section": "",
    "text": "import sys, matplotlib\nmatplotlib.use(\"Agg\")             # &lt;— backend offscreen\nimport matplotlib.pyplot as plt\n\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Matplotlib backend:\", matplotlib.get_backend())\n\n# plot de prueba (sin show; guardamos y cerramos)\nplt.plot([0,1,2],[0,1,0])\nplt.title(\"Gráfico de prueba\")\nplt.tight_layout()\nplt.savefig(\"docs/fig_test.png\", dpi=150, bbox_inches=\"tight\")\nplt.close()"
  },
  {
    "objectID": "tweets_classification.html#objetivo",
    "href": "tweets_classification.html#objetivo",
    "title": "Tweets Classification",
    "section": "## Objetivo",
    "text": "## Objetivo\nClasificar cuentas en real vs. bot a partir de tweets (texto + metadatos), creando un target heurístico y armando un pipeline con TF-IDF + OneHotEncoder + StandardScaler + LogisticRegression."
  },
  {
    "objectID": "tweets_classification.html#dataset",
    "href": "tweets_classification.html#dataset",
    "title": "Tweets Classification",
    "section": "Dataset",
    "text": "Dataset\nFuente: CSV público (Twitter) – 158,873 filas; 26 columnas."
  },
  {
    "objectID": "tweets_classification.html#selección-del-target-user_type",
    "href": "tweets_classification.html#selección-del-target-user_type",
    "title": "Tweets Classification",
    "section": "Selección del target (user_type)",
    "text": "Selección del target (user_type)\nSe definió user_type ∈ {bot, real} mediante una heurística débil basada en señales (foto de perfil, seguidores, edad de cuenta, etc.). Esta etiqueta sirve para entrenar un modelo que generalice el patrón."
  },
  {
    "objectID": "tweets_classification.html#columnas-eliminadas",
    "href": "tweets_classification.html#columnas-eliminadas",
    "title": "Tweets Classification",
    "section": "Columnas eliminadas",
    "text": "Columnas eliminadas\nSe eliminaron IDs, URLs, timestamps crudos y campos redundantes (p. ej., tweetId, tweetUrl, authorProfilePic, createdAt, mentions, hashtag s, source, etc.) por no aportar valor predictivo directo."
  },
  {
    "objectID": "tweets_classification.html#pipeline",
    "href": "tweets_classification.html#pipeline",
    "title": "Tweets Classification",
    "section": "Pipeline",
    "text": "Pipeline\n\nTexto: TfidfVectorizer(stop_words=spanish, ngram_range=(1,2), min_df=5, max_df=0.90, max_features=50000)\nCategóricas: OneHotEncoder(drop='if_binary') en isReply, authorVerified\nNuméricas: StandardScaler(with_mean=False) en authorFollowers, mentions_count, hashtags_count, time_response, content_length\nModelo: LogisticRegression(max_iter=10000, class_weight={'bot': 2.5, 'real': 1.0})"
  },
  {
    "objectID": "tweets_classification.html#exploración-del-target-potencial",
    "href": "tweets_classification.html#exploración-del-target-potencial",
    "title": "Tweets Classification",
    "section": "Exploración del target potencial",
    "text": "Exploración del target potencial\n\n# Revisar la variación de sentiment_polarity\nprint(df['sentiment_polarity'].describe())\n\nprint(\"\\nValores únicos de sentiment_polarity y su frecuencia:\")\nprint(df['sentiment_polarity'].value_counts().head(10))"
  },
  {
    "objectID": "tweets_classification.html#borrar-columnas-irrelevantes",
    "href": "tweets_classification.html#borrar-columnas-irrelevantes",
    "title": "Tweets Classification",
    "section": "Borrar columnas irrelevantes",
    "text": "Borrar columnas irrelevantes\n\n# === Eliminación explícita de columnas irrelevantes ===\nirrelevantes = [\n    'tweetId', 'tweetUrl',                 # IDs/URLs\n    'authorId', 'authorName', 'authorUsername', 'authorProfilePic',\n    'replyTo', 'conversationId', 'inReplyToId',                      # IDs de conversación\n    'createdAt', 'Date', 'authorJoinDate',                           # timestamps en texto\n    'mentions', 'hashtags',                                          # ya tenemos *_count\n    'source'                                                         # casi constante\n    # 'content_length'  # &lt;- si NO quieres usarla, descomenta y se elimina\n]\n\n# Columnas relevantes que conservaremos\nrelevantes = [\n    'content', 'isReply', 'authorVerified', 'has_profile_picture',\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'time_response', 'content_length',\n    'sentiment_polarity'\n]\n\ncols_existentes = [c for c in irrelevantes if c in df.columns]\ndf_clean = df.drop(columns=cols_existentes).copy()\ndf_clean = df_clean[relevantes].copy()\n\nprint(\"Eliminadas:\", len(cols_existentes), \"→\", cols_existentes)\nprint(\"Shape original:\", df.shape, \"→ Shape limpio:\", df_clean.shape)\ndisplay(df_clean.head(3))"
  },
  {
    "objectID": "tweets_classification.html#crear-target-user_type",
    "href": "tweets_classification.html#crear-target-user_type",
    "title": "Tweets Classification",
    "section": "Crear target “user_type”",
    "text": "Crear target “user_type”\n\n# Heurística para etiquetar \"bot\" vs \"real\"\ndef label_user_type(row):\n    score = 0\n    # señales fuertes\n    if not row['has_profile_picture']:\n        score += 2\n    if row['authorFollowers'] &lt; 50:\n        score += 2\n    if row['account_age_days'] &lt; 60:\n        score += 2\n    # señales adicionales\n    if row['mentions_count'] &gt;= 3:\n        score += 1\n    if row['hashtags_count'] &gt;= 3:\n        score += 1\n    if row['content_length'] &lt; 20:\n        score += 1\n    if row['isReply']:\n        score += 1\n    # verificado resta (suele ser humano/organización)\n    if row['authorVerified']:\n        score -= 2\n\n    return \"bot\" if score &gt;= 4 else \"real\"\n\ndf_bot = df_clean.copy()\ndf_bot['user_type'] = df_bot.apply(label_user_type, axis=1)\n\nprint(\"Distribución user_type:\")\nprint(df_bot['user_type'].value_counts())\nprint(\"\\nProporciones:\")\nprint(df_bot['user_type'].value_counts(normalize=True).round(3))\n\n# --- Gráfico: Distribución de clases (user_type) ---\n\nimport matplotlib.pyplot as plt\n\nax = df_bot['user_type'].value_counts().plot(kind='bar')\nplt.title('Distribución de clases (user_type)')\nplt.xlabel('Clase')\nplt.ylabel('Frecuencia')\nplt.tight_layout()\n\nfig = plt.gcf()                   # figura activa\nfig.savefig(\"docs/fig_user_type_dist.png\", dpi=150, bbox_inches='tight')\nplt.close(fig)           # cerrar después de guardar"
  },
  {
    "objectID": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "href": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "title": "Tweets Classification",
    "section": "Seleccion de columnas para entrenar",
    "text": "Seleccion de columnas para entrenar\n\n# Definición de columnas\ntext_col = 'content'\ntarget_col = 'user_type'\n\ncat_cols = ['isReply', 'authorVerified']\nnum_cols = ['authorFollowers', 'mentions_count', 'hashtags_count', 'time_response', 'content_length']\n\nkeep_cols = [text_col, target_col] + cat_cols + num_cols\ndf_train = df_bot[keep_cols].copy()\n\n# Limpieza mínima\ndf_train[text_col] = df_train[text_col].fillna(\"\").astype(str).str.strip()\ndf_train = df_train[df_train[text_col] != \"\"]\n\nprint(\"Shape final para entrenamiento:\", df_train.shape)\ndisplay(df_train.head(3))"
  },
  {
    "objectID": "tweets_classification.html#train-test-split-estratificado",
    "href": "tweets_classification.html#train-test-split-estratificado",
    "title": "Tweets Classification",
    "section": "Train/ Test Split estratificado",
    "text": "Train/ Test Split estratificado\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_train.drop(columns=[target_col])\ny = df_train[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\nprint(\"Distrib y_train:\\n\", y_train.value_counts(normalize=True).round(3))"
  },
  {
    "objectID": "tweets_classification.html#limpieza-de-texto-lematización-spacy",
    "href": "tweets_classification.html#limpieza-de-texto-lematización-spacy",
    "title": "Tweets Classification",
    "section": "Limpieza de texto + lematización (spaCy)",
    "text": "Limpieza de texto + lematización (spaCy)\n\n# --- Limpieza de texto + lematización en español (spaCy) ---\nimport re\nimport sys\nimport subprocess\nimport spacy\n\n# --- spaCy ES: carga; solo descarga si NO está instalado ---\nimport sys, subprocess, spacy\ntry:\n    nlp = spacy.load(\"es_core_news_sm\")\nexcept OSError:\n    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"], check=True)\n    nlp = spacy.load(\"es_core_news_sm\")\n\n\n# 2) Stopwords (puedes combinarlas con NLTK si quieres)\nsp_stop = nlp.Defaults.stop_words\n\n# 3) Limpiadores básicos de Twitter\n_url   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n_user  = re.compile(r\"@\\w+\")              # elimina @usuario\n_hash  = re.compile(r\"#\")                  # quita '#' pero deja la palabra\n_emoji = re.compile(r\"[^\\w\\sáéíóúñüÁÉÍÓÚÑÜ]\")  # limpia signos/emojis (ajustable)\n\ndef clean_text(text: str) -&gt; str:\n    text = text.lower().strip()\n    text = _url.sub(\" \", text)\n    text = _user.sub(\" \", text)\n    text = _hash.sub(\"\", text)             # \"#vacunas\" -&gt; \"vacunas\"\n    text = _emoji.sub(\" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\n# 4) Analizador para sklearn que devuelve tokens lematizados sin stopwords\ndef spacy_analyzer(doc: str):\n    doc = clean_text(doc)\n    sp = nlp(doc)\n    return [\n        tok.lemma_\n        for tok in sp\n        if tok.is_alpha                  # solo letras\n        and tok.lemma_ not in sp_stop    # sin stopwords\n        and len(tok.lemma_) &gt; 2          # descarta tokens muy cortos\n    ]\n\n# (opcional) prueba rápida\n# ejemplo = \"RT @usuario: ¡Viendo #Debate2025 en https://x.com! Los candidatos hablando...\"\n# print(spacy_analyzer(ejemplo))"
  },
  {
    "objectID": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "href": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "title": "Tweets Classification",
    "section": "ColumnTransformer (TF-IDF + OHE + escala numérica)",
    "text": "ColumnTransformer (TF-IDF + OHE + escala numérica)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords as nltk_stopwords   # ⬅️ NUEVO\n\nspanish_stop = nltk_stopwords.words('spanish')        # ⬅️ NUEVO\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntext_vectorizer = TfidfVectorizer(\n    analyzer=spacy_analyzer,   # usamos nuestro analizador con limpieza+lemmas\n    ngram_range=(1, 2),\n    min_df=5,\n    max_df=0.90,\n    max_features=50_000,\n    strip_accents='unicode',\n    lowercase=False            # ya pasamos a minúsculas en clean_text()\n)\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', text_vectorizer, text_col),\n        ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary'), cat_cols),\n        ('num', StandardScaler(with_mean=False), num_cols)\n    ],\n    remainder='drop',\n    sparse_threshold=0.3\n)"
  },
  {
    "objectID": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "href": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "title": "Tweets Classification",
    "section": "Pipelines (LR con texto+meta, y NB solo texto)",
    "text": "Pipelines (LR con texto+meta, y NB solo texto)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Regresión Logística (texto + categóricas + numéricas)\npipe_lr = Pipeline([\n    ('prep', preprocessor),\n    ('clf', LogisticRegression(\n        max_iter=10000,\n        class_weight={'bot': 2.5, 'real': 1.0},\n        random_state=42\n    ))\n])\n\n\n# Naive Bayes SOLO TEXTO (baseline)\npipe_nb_text_only = Pipeline([\n    ('tfidf', text_vectorizer),\n    ('nb', MultinomialNB())\n])"
  },
  {
    "objectID": "tweets_classification.html#entrenar-y-evaluar",
    "href": "tweets_classification.html#entrenar-y-evaluar",
    "title": "Tweets Classification",
    "section": "Entrenar y Evaluar",
    "text": "Entrenar y Evaluar\n\nMatrices de confusión"
  },
  {
    "objectID": "text_classification.html",
    "href": "text_classification.html",
    "title": "Text classification",
    "section": "",
    "text": "Importar librerías\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_20newsgroups\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# vectorizacion textual\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n\n\nCargar el dataset\n\ncategorias = ['comp.graphics','comp.sys.mac.hardware','rec.sport.baseball','talk.politics.misc']\n\nnewsgroups = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\nprint(newsgroups.target_names)\n\n\n\nFeatures y target\n\nX_text = newsgroups.data #features\ny = newsgroups.target #target\n\n\n\nTrain-test split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_text,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n\n\nVectorizacion de text\n\nstopwords = stopwords.words('english')\n# vectorizer = CountVectorizer(stop_words=stopwords) \nvectorizer = TfidfVectorizer(stop_words=stopwords)\n\n\n\nModelo\n\nmodel = MultinomialNB()\n\n\n\nDefinición del Pipeline\n\npipeline = Pipeline(\n    [\n        ('vectorizacion',vectorizer),\n        ('classfier',model)\n    ]\n)\n\n\n\nFit del modelo\n\npipeline.fit(X_train,y_train)\n\n\n\nPredicción\n\ny_pred = pipeline.predict(X_test)\n\n\n\nReporte de clasificación\n\nprint(\"REPORTE DE CLASIFICACION\\n\",classification_report(y_test,y_pred,target_names=newsgroups.target_names))\n\n\n\nMatriz de confusión\n\nConfusionMatrixDisplay(\n    confusion_matrix(y_test,y_pred),display_labels=newsgroups.target_names\n).plot(\n    xticks_rotation='vertical'\n)"
  },
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "test",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')"
  },
  {
    "objectID": "nn.html#fit-aprende-el-vocabulari",
    "href": "nn.html#fit-aprende-el-vocabulari",
    "title": "test",
    "section": "fit: Aprende el vocabulari",
    "text": "fit: Aprende el vocabulari\n\n\nCode\nX = vectorizer.fit_transform(docs)\n\n\n\n\nCode\nvectorizer.get_feature_names_out()"
  },
  {
    "objectID": "nn.html#transform-crea-la-matriz-de-conteos-sparse-matrix",
    "href": "nn.html#transform-crea-la-matriz-de-conteos-sparse-matrix",
    "title": "test",
    "section": "transform: Crea la matriz de conteos (sparse matrix)",
    "text": "transform: Crea la matriz de conteos (sparse matrix)\nCada fila representa un documento, cada columna una palabra del vocabulario\n\n\nCode\nX.toarray()"
  },
  {
    "objectID": "nn.html#visualizar-la-matriz-como-dataframe",
    "href": "nn.html#visualizar-la-matriz-como-dataframe",
    "title": "test",
    "section": "Visualizar la matriz como DataFrame",
    "text": "Visualizar la matriz como DataFrame\n\n\nCode\ndf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)"
  },
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "knn.html#generación-de-pipeline-por-tipo-de-variable",
    "href": "knn.html#generación-de-pipeline-por-tipo-de-variable",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Generación de pipeline por tipo de variable",
    "text": "Generación de pipeline por tipo de variable\n\ncategorical_pipeline = Pipeline([\n    ('imputacion_cat',SimpleImputer(strategy='most_frequent')),\n    ('encodage_cat',OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n# Mujer, Hombre =&gt; (1,0) =&gt; StandarScaler() NO HACER\n\nnumerical_pipeline = Pipeline([\n    ('imputacion_num',SimpleImputer(strategy='mean')),\n    ('escalamiento',StandardScaler())\n])"
  },
  {
    "objectID": "knn.html#aplicar-columntransformer",
    "href": "knn.html#aplicar-columntransformer",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Aplicar ColumnTransformer",
    "text": "Aplicar ColumnTransformer\n\npreprocessor = ColumnTransformer([\n    ('cat',categorical_pipeline,cat_cols),\n    ('num',numerical_pipeline,num_cols)\n])"
  },
  {
    "objectID": "knn.html#definir-el-param_grid",
    "href": "knn.html#definir-el-param_grid",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Definir el param_grid",
    "text": "Definir el param_grid\n\nparam_grid = {\n    'classificador__n_neighbors':list(range(1,5)),\n    'classificador__weights':['uniform','distance'],\n    'classificador__metric':['minkowski','euclidean','manhattan']\n}"
  },
  {
    "objectID": "knn.html#realizar-el-gridseach",
    "href": "knn.html#realizar-el-gridseach",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Realizar el Gridseach",
    "text": "Realizar el Gridseach\n\ngrid = GridSearchCV(pipeline,param_grid,cv=5,scoring='f1')\ngrid.fit(X_train,y_train)"
  },
  {
    "objectID": "knn.html#obtener-el-mejor-best_estimator",
    "href": "knn.html#obtener-el-mejor-best_estimator",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Obtener el mejor best_estimator",
    "text": "Obtener el mejor best_estimator\n\nbest_model = grid.best_estimator_"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proyecto Final — Machine Learning",
    "section": "",
    "text": "Bienvenido\nEste sitio contiene el desarrollo de dos tareas de aprendizaje supervisado:\n\nClasificación: incidentes (accidentes con heridos) con Regresión Logística.\nRegresión: (PM2.5) — lo agregaremos como segunda página.\n\n\nAutor: Noe Sanchez\nAño: 2025\nCurso: ML"
  },
  {
    "objectID": "clasificacion_incidentes.html",
    "href": "clasificacion_incidentes.html",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Code\n# 1. Importar librerías\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    ConfusionMatrixDisplay, RocCurveDisplay\n)\n\n# 2. Cargar dataset\ndf = pd.read_csv(\"data/incidentes_clasificacion_ready.csv\")\n\ny = df[\"accidente_con_heridos\"]\nX = df.drop(columns=[\"accidente_con_heridos\"])\n\nprint(\"Dimensiones:\", X.shape)\nprint(\"Distribución de clases:\")\nprint(y.value_counts())\n\n# 3. Dividir en train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 4. Entrenar modelo SIN pipeline\nmodel = LogisticRegression(max_iter=100000, class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\n\n# 5. Predicciones\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:,1]\n\n# 6. Métricas\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n==== Resultados SIN Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\n# 7. Matriz de confusión\nConfusionMatrixDisplay.from_predictions(y_test, y_pred)\nplt.title(\"Matriz de Confusión — Sin Pipeline\")\nplt.show()\n\n# 8. Curva ROC\nRocCurveDisplay.from_predictions(y_test, y_proba)\nplt.title(\"Curva ROC — Sin Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# ======== CON PIPELINE ========\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=100000, class_weight=\"balanced\"))\n])\n\npipe.fit(X_train, y_train)\ny_pred_pipe = pipe.predict(X_test)\ny_proba_pipe = pipe.predict_proba(X_test)[:,1]\n\naccuracy = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe, zero_division=0)\nrecall = recall_score(y_test, y_pred_pipe, zero_division=0)\nf1 = f1_score(y_test, y_pred_pipe, zero_division=0)\n\nprint(\"\\n==== Resultados CON Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_pipe)\nplt.title(\"Matriz de Confusión — Con Pipeline\")\nplt.show()\n\nRocCurveDisplay.from_predictions(y_test, y_proba_pipe)\nplt.title(\"Curva ROC — Con Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\nDimensiones: (268065, 4)\nDistribución de clases:\naccidente_con_heridos\n0    265740\n1      2325\nName: count, dtype: int64\n\n==== Resultados SIN Pipeline ====\nAccuracy : 0.5142\nPrecision: 0.0107\nRecall   : 0.6022\nF1 Score : 0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== Resultados CON Pipeline ====\nAccuracy : 0.5142\nPrecision: 0.0107\nRecall   : 0.6022\nF1 Score : 0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResumen del problema.\nSe modeló la probabilidad de accidente con heridos (clase 1) usando Regresión Logística, con un flujo estándar: carga del dataset, división train_test_split con stratify, entrenamiento sin y con Pipeline(StandardScaler + LogisticRegression), evaluación con Accuracy, Precision, Recall, F1 y visualizaciones (Matriz de confusión y Curva ROC).\nResultados clave (lo observado en este experimento): - El accuracy es poco informativo dado el fuerte desbalance (la clase positiva es muy rara).\n- Se obtuvo Recall relativamente alto (capacidad de encontrar positivos) a costa de Precision baja (muchos falsos positivos).\n- Con y sin Pipeline las métricas son muy similares —esperable en un modelo lineal cuando las variables ya están en rangos similares y el desbalance domina el comportamiento."
  },
  {
    "objectID": "clasificacion_incidentes.html#conclusiones-e-interpretación",
    "href": "clasificacion_incidentes.html#conclusiones-e-interpretación",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Resumen del problema.\nSe modeló la probabilidad de accidente con heridos (clase 1) usando Regresión Logística, con un flujo estándar: carga del dataset, división train_test_split con stratify, entrenamiento sin y con Pipeline(StandardScaler + LogisticRegression), evaluación con Accuracy, Precision, Recall, F1 y visualizaciones (Matriz de confusión y Curva ROC).\nResultados clave (lo observado en este experimento): - El accuracy es poco informativo dado el fuerte desbalance (la clase positiva es muy rara).\n- Se obtuvo Recall relativamente alto (capacidad de encontrar positivos) a costa de Precision baja (muchos falsos positivos).\n- Con y sin Pipeline las métricas son muy similares —esperable en un modelo lineal cuando las variables ya están en rangos similares y el desbalance domina el comportamiento."
  },
  {
    "objectID": "altair.html",
    "href": "altair.html",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\nGráfico de barras\ncount() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n)\n\n\nmean() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\nGráfico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n).interactive()"
  },
  {
    "objectID": "altair.html#importar-el-dataset",
    "href": "altair.html#importar-el-dataset",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())"
  },
  {
    "objectID": "altair.html#visualización-en-altair",
    "href": "altair.html#visualización-en-altair",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\nGráfico de barras\ncount() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n)\n\n\nmean() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\nGráfico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n).interactive()"
  },
  {
    "objectID": "bagofwords.html",
    "href": "bagofwords.html",
    "title": "Bag of Words (BoW)",
    "section": "",
    "text": "from sklearn.feature_extraction.text import CountVectorizer #BoW\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization"
  },
  {
    "objectID": "bagofwords.html#ejemplo-de-función-de-limpieza",
    "href": "bagofwords.html#ejemplo-de-función-de-limpieza",
    "title": "Bag of Words (BoW)",
    "section": "Ejemplo de función de limpieza",
    "text": "Ejemplo de función de limpieza\n\ndef get_wordnet_pos(treebank_tag):\n    # print(\"treebank_tag\",treebank_tag)\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN \n\n\ndef preprocessing_document(doc):\n    #1 transformar en minusculas\n    doc = doc.lower()\n    \n    #2 tokenizar\n    tokens = word_tokenize(doc)\n\n    #3 obtener lematizacion con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n\n    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION\n\n    #4 filtrar numeros\n    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]\n\n    #5 Lematizacion usando el pos\n    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "bagofwords.html#fit_transform",
    "href": "bagofwords.html#fit_transform",
    "title": "Bag of Words (BoW)",
    "section": "Fit_transform",
    "text": "Fit_transform\nAprende del Corpus (vocabulario) y del ser caso elimina las stop-words\n\ncorpus_cleaned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus)"
  },
  {
    "objectID": "bagofwords.html#obtener-las-palabras-finales",
    "href": "bagofwords.html#obtener-las-palabras-finales",
    "title": "Bag of Words (BoW)",
    "section": "Obtener las palabras finales",
    "text": "Obtener las palabras finales\n\nvectorizer.get_feature_names_out()"
  },
  {
    "objectID": "hac.html",
    "href": "hac.html",
    "title": "Hierachical Agglomerative Clustering (HAC)",
    "section": "",
    "text": "import pandas as pd\nimport altair as alt\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom scipy.cluster.hierarchy import dendrogram,linkage\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "K-means",
    "section": "",
    "text": "import altair as alt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n#---\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow",
    "href": "kmeans.html#método-del-codo-elbow",
    "title": "K-means",
    "section": "Método del codo (Elbow)",
    "text": "Método del codo (Elbow)\n\nCalcular el Sum of Squared Errors (SSE)\n\nsse = [] #inertia\nk_range = range(2,11) #valores posibles de k\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k,random_state=42)\n    kmeans.fit(df[['age','annual_income($)']])\n    sse.append(kmeans.inertia_)\n\nprint(\"Inertia:\\n\",sse)\n\nInertia:\n [275802340.8465648, 103879919.55, 82797577.45817567, 38460868.83850575, 27144269.551419172, 19757179.065581053, 14468762.582687736, 11828359.755827505, 9333390.053670242]\n\n\n\n\nVisualizar el Sum of Squared Errors (SSE) vs K\n\nelbow_df = pd.DataFrame(\n    {\n        'K':list(k_range),\n        'SSE':sse\n    }\n)\n\n# elbow_df\n\nalt.Chart(elbow_df).mark_line(point=True).encode(\n    alt.X(\"K\"),\n    alt.Y(\"SSE\"),\n    tooltip=[\"K\",\"SSE\"]\n).properties(\n    title=\"Método del Codo\"\n).interactive()"
  },
  {
    "objectID": "kmeans.html#clusterización",
    "href": "kmeans.html#clusterización",
    "title": "K-means",
    "section": "Clusterización",
    "text": "Clusterización\n\nk_clusters = 3\nclusters = KMeans(n_clusters=k_clusters,random_state=42)\ndf['clusters'] = clusters.fit_predict(df[['age','annual_income($)']])\ndf.head()\n\n\n\n\n\n\n\n\nage\nannual_income($)\nclusters\n\n\n\n\n0\n17\n17123\n2\n\n\n1\n17\n18375\n2\n\n\n2\n16\n18557\n2\n\n\n3\n23\n17721\n2\n\n\n4\n22\n17291\n2\n\n\n\n\n\n\n\n\nVisualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters:N'),\n    tooltip=['age','annual_income($)','clusters:N']\n).properties(\n    title='CLUSTERIZACION'\n).interactive()"
  },
  {
    "objectID": "kmeans.html#columntransformer",
    "href": "kmeans.html#columntransformer",
    "title": "K-means",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\n\nnum_cols = ['age','annual_income($)']\n\npreprocessor = ColumnTransformer(\n    [\n        ('standar',StandardScaler(),num_cols)\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#pipeline",
    "href": "kmeans.html#pipeline",
    "title": "K-means",
    "section": "Pipeline",
    "text": "Pipeline\n\npipeline = Pipeline(\n    [\n        ('scaler',preprocessor),\n        ('kmeans',KMeans(random_state=42))\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow-silhouette",
    "href": "kmeans.html#método-del-codo-elbow-silhouette",
    "title": "K-means",
    "section": "Método del codo (Elbow) + Silhouette",
    "text": "Método del codo (Elbow) + Silhouette\n\nCalcular el Sum of Squared Errors (SSE) + Silhouette\n\nk_range = range(2,11)\nsse_standar = []\nsilhouette_scores = []\n\nfor k in k_range:\n    pipeline.set_params(kmeans__n_clusters=k)\n    pipeline.fit(df[num_cols])\n    sse_standar.append(\n        pipeline.named_steps['kmeans'].inertia_\n    )\n\n    pred = pipeline.predict(df[num_cols])\n    print(\"K:\",k)\n    print(\"PRED\",pred)\n    score = silhouette_score(pipeline.named_steps['scaler'].transform(df[num_cols]),pred)\n    print(\"SILHOUETTE SCORE\",score)\n    print(\"\\n\")\n    silhouette_scores.append(score)\n\n#silhouette_score\n\nK: 2\nPRED [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1]\nSILHOUETTE SCORE 0.5584253334367935\n\n\nK: 3\nPRED [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1]\nSILHOUETTE SCORE 0.48862751188830494\n\n\nK: 4\nPRED [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.6197880502586566\n\n\nK: 5\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5870741352565944\n\n\nK: 6\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 5 5 5 5 5 1 5\n 5 5 5 1 5 1 1 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5496403913716196\n\n\nK: 7\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 6 6 6 5 6 6 5 6 5 6 1 5\n 6 6 5 5 5 5 5 6 5 6 6 6 6 6 6 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5351154400530848\n\n\nK: 8\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 0 0 4 4 4 4 0 4 0 0 0 4 4 4 5 5 7 7 7 5 5 5 5 5 5 5 1 5\n 7 5 5 5 5 5 5 7 5 7 5 5 7 5 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.49714020005833653\n\n\nK: 9\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 0 0 4 4 4 4 0 4 0 0 0 4 4 4 8 5 7 7 7 8 5 5 8 5 5 5 8 8\n 7 5 8 8 5 8 8 7 8 7 5 5 7 5 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.48277202641477174\n\n\nK: 10\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 4 0 4 4 4 4 0 4 0 0 0 4 4 4 9 9 7 7 5 9 5 5 9 5 9 5 8 9\n 5 5 9 8 5 9 9 7 9 5 5 5 7 5 7 5 1 8 8 8 1 1 8 1 8 8 8 8 1 8 1 8 1 8 8 1 1\n 8 8 8 8 8 1 8 8 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.49114927636373373"
  },
  {
    "objectID": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "href": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "title": "K-means",
    "section": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K",
    "text": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K\n\nsse_standard_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SSE_STANDARD':sse_standar\n    }\n)\n\nsilhouette_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SILHOUETTE':silhouette_scores\n    }\n)\n\nviz_sse_standard = alt.Chart(sse_standard_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SSE_STANDARD'),\n    tooltip=['K',\"SSE_STANDARD\"]\n).properties(\n    title='METODO DEL CODO CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_silhouette = alt.Chart(silhouette_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SILHOUETTE'),\n    tooltip=['K',\"SILHOUETTE\"]\n).properties(\n    title='SILHOUETTE SCORE CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_sse_standard | viz_silhouette"
  },
  {
    "objectID": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "href": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "title": "K-means",
    "section": "Clusterizacion con Pipeline + StandardScaler()",
    "text": "Clusterizacion con Pipeline + StandardScaler()\n\nk_optimo = k_range[silhouette_scores.index(max(silhouette_scores))]\nprint(\"K_OPTIMO\",k_optimo)\n\npipeline.set_params(kmeans__n_clusters=k_optimo)\ndf['clusters_standard'] = pipeline.fit_predict(df[num_cols])\ndf.head()\n\nK_OPTIMO 4\n\n\n\n\n\n\n\n\n\nage\nannual_income($)\nclusters\nclusters_standard\n\n\n\n\n0\n17\n17123\n2\n2\n\n\n1\n17\n18375\n2\n2\n\n\n2\n16\n18557\n2\n2\n\n\n3\n23\n17721\n2\n2\n\n\n4\n22\n17291\n2\n2"
  },
  {
    "objectID": "kmeans.html#visualizar-la-clusterización-1",
    "href": "kmeans.html#visualizar-la-clusterización-1",
    "title": "K-means",
    "section": "Visualizar la clusterización",
    "text": "Visualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters_standard:N'),\n    tooltip=['age','annual_income($)','clusters_standard:N']\n).properties(\n    title='CLUSTERIZACION CON PIPELINE + STANDARSCALER()'\n).interactive()"
  },
  {
    "objectID": "ml_text.html",
    "href": "ml_text.html",
    "title": "NLP Key Concepts",
    "section": "",
    "text": "Cargar librerías\n\nimport pandas as pd\nimport requests\n\n# Natural Language Toolkit\nimport nltk\n# downloading some additional packages and corpora\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n\n\nCargar data\n\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\nr = requests.get(url)\nr.encoding = 'utf-8'\n\nstory = r.text\nstory\n\n\n\nTokenización\n\nfrom nltk import word_tokenize\n\nwords = word_tokenize(story)\nwords[:20]\n\n\n\nStemming and Lemmatization\n\nfrom nltk.stem import PorterStemmer as stemmer\nfrom nltk.stem import WordNetLemmatizer as lemmatizer\nfrom nltk.corpus import wordnet\n\npalabra = \"changing\"\nprint(\"PALABRA: \", palabra)\n\n#stemming\nprint(\"STEMMING: \",stemmer().stem(palabra))\n\n#lemmatizatio\nprint(\"LEMMATIZATION: \", lemmatizer().lemmatize(palabra,pos=wordnet.VERB))\n\n\n\nPART OF SPEECH - POS TAG\n\nfrom nltk import pos_tag\npos = pos_tag(words)\npos[:20]\n\n\n\nSTOP WORDS\n\nfrom nltk.corpus import stopwords as stop\n\nstopwords = stop.words(\"english\")\nstopwords[:20]\n\n# for item in stop.words(\"spanish\"):\n#     print(item)\n\n\n\nSTOP WORDS IN STORY\n\ntokens = nltk.word_tokenize(story.lower())\n# tokens[:20]\n\n# Limpieza depende del contexto de su problemática\n\n# limpieza de numeros\nlettertokens = [word for word in tokens if word.isalpha()]\n\nwithout_stopwords = [word for word in lettertokens if word not in stopwords]\n\nwithout_stopwords[:20]"
  },
  {
    "objectID": "regresion_pm25.html#cargar-y-explorar",
    "href": "regresion_pm25.html#cargar-y-explorar",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "1) Cargar y explorar",
    "text": "1) Cargar y explorar\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Ruta al dataset limpio que generamos antes\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head(8))\n\n# Nos quedamos SOLO con columnas numéricas (la consigna pide evitar categóricas)\nnum = df.select_dtypes(include=[np.number]).copy()\nprint(\"Numéricas:\", list(num.columns))\ndisplay(num.describe().T)\n\n\n## 2) Seleccionar variables (X, y) y train/test split\nfrom sklearn.model_selection import train_test_split\n\n# Elegir la y:\n# Si existe una columna 'pm25', la usamos. Si no existe, tomamos la última columna numérica como fallback.\ntarget_name = \"pm25\" if \"pm25\" in num.columns else num.columns[-1]\n\ny = num[target_name].values\nX = num.drop(columns=[target_name]).values\n\nprint(\"Objetivo (y):\", target_name)\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\n## 3) Pipeline + LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),  # en regresión lineal no siempre es obligatorio, pero es buena práctica\n    (\"linreg\", LinearRegression())\n])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n\n## 4) Métricas (MSE y R²)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nr2  = r2_score(y_test, y_pred)\n\nprint(f\"MSE : {mse:,.4f}\")\nprint(f\"R²  : {r2:,.4f}\")\n\n\n## 5) Visualización — Real vs Predicho\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(y_test, y_pred, alpha=0.4)\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predicho\")\nplt.title(\"PM2.5 — Real vs Predicho\")\nminv = float(np.min([y_test.min(), y_pred.min()]))\nmaxv = float(np.max([y_test.max(), y_pred.max()]))\nplt.plot([minv, maxv], [minv, maxv])  # línea y=x\nplt.tight_layout()\nplt.show()\n\n\n## 6) Curva de aprendizaje (opcional recomendado)\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    pipe, X, y, cv=5, scoring=\"r2\", n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 6), shuffle=True, random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\nvalid_mean = valid_scores.mean(axis=1)\n\nplt.figure()\nplt.plot(train_sizes, train_mean, marker=\"o\", label=\"Train R²\")\nplt.plot(train_sizes, valid_mean, marker=\"s\", label=\"Valid R²\")\nplt.xlabel(\"Tamaño de entrenamiento\")\nplt.ylabel(\"R²\")\nplt.title(\"Curva de aprendizaje\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nShape: (30983, 14)\n\n\n\n\n\n\n\n\n\nanio_report\nanio\ncodmes\nmes\ndia\ncod_prov\nprov\ncod_cant\ncant\ncod_tipo\ntipo\ncod_est\nestacion\npm2.5\n\n\n\n\n0\n2021\n2005\n1\nEnero\n1\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n37,45\n\n\n1\n2021\n2005\n1\nEnero\n2\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n29,52\n\n\n2\n2021\n2005\n1\nEnero\n3\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n23,77\n\n\n3\n2021\n2005\n1\nEnero\n5\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n19,71\n\n\n4\n2021\n2005\n1\nEnero\n6\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n21,24\n\n\n5\n2021\n2005\n1\nEnero\n7\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n20,83\n\n\n6\n2021\n2005\n1\nEnero\n8\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,43\n\n\n7\n2021\n2005\n1\nEnero\n9\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,65\n\n\n\n\n\n\n\nNuméricas: ['anio_report', 'anio', 'codmes', 'dia', 'cod_prov', 'cod_cant', 'cod_tipo', 'cod_est']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nanio_report\n30983.0\n2021.000000\n0.000000\n2021.0\n2021.0\n2021.0\n2021.0\n2021.0\n\n\nanio\n30983.0\n2012.464577\n4.240917\n2005.0\n2009.0\n2013.0\n2016.0\n2018.0\n\n\ncodmes\n30983.0\n6.593777\n3.435656\n1.0\n4.0\n7.0\n10.0\n12.0\n\n\ndia\n30983.0\n15.726495\n8.806857\n1.0\n8.0\n16.0\n23.0\n31.0\n\n\ncod_prov\n30983.0\n15.570571\n4.563800\n1.0\n17.0\n17.0\n17.0\n17.0\n\n\ncod_cant\n30983.0\n1558.057096\n456.379990\n101.0\n1701.0\n1701.0\n1701.0\n1701.0\n\n\ncod_tipo\n30983.0\n1.000000\n0.000000\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\ncod_est\n30983.0\n6.365749\n8.658158\n1.0\n2.0\n4.0\n5.0\n35.0\n\n\n\n\n\n\n\nObjetivo (y): cod_est\nX shape: (30983, 7) | y shape: (30983,)\nMSE : 4.9328\nR²  : 0.9324"
  },
  {
    "objectID": "regresion_pm25.html#conclusiones-regresión-pm2.5-quito",
    "href": "regresion_pm25.html#conclusiones-regresión-pm2.5-quito",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "Conclusiones — Regresión (PM2.5 Quito)",
    "text": "Conclusiones — Regresión (PM2.5 Quito)\n\n\nCode\n# --- Asegurar que el objetivo (y) sea la medida de PM2.5 ---\n# Ajusta aquí al nombre REAL de tu columna objetivo en data/pm25_ecuador_clean.csv\n# Ejemplos comunes: \"pm25_mean_anual\", \"pm25_max_anual\"\n# Si no existe, hace fallback a \"pm25\" si está; y si tampoco, a la última numérica.\n\nimport pandas as pd, numpy as np\n\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\nnum = df.select_dtypes(include=[np.number]).copy()\n\n# --- Elige tu objetivo aquí ---\ntarget_candidates = [\"pm25_mean_anual\", \"pm25_max_anual\", \"pm25\"]\ntarget_name = None\nfor c in target_candidates:\n    if c in num.columns:\n        target_name = c\n        break\nif target_name is None:\n    target_name = num.columns[-1]  # fallback\n\nX = num.drop(columns=[target_name]).values\ny = num[target_name].values\n\nprint(f\"Objetivo (y): {target_name}\")\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\n\nObjetivo (y): cod_est\nX shape: (30983, 7) | y shape: (30983,)"
  },
  {
    "objectID": "regresion_pm25.html#resumen-del-flujo-aplicado.",
    "href": "regresion_pm25.html#resumen-del-flujo-aplicado.",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "Resumen del flujo aplicado.",
    "text": "Resumen del flujo aplicado.\nSplit: train_test_split(…, test_size=0.25, random_state=42)\nModelo (Pipeline): StandardScaler() + LinearRegression()\nMétricas: MSE y R² (proporción de varianza explicada).\nGráfico: Real vs Predicho con línea identidad.\nResultados de esta corrida (ejemplo):\nMSE bajo y R² alto indican buen ajuste en el agregado anual por estación. Si no obtienes métricas satisfactorias, revisa el objetivo y las columnas numéricas disponibles.\nInterpretación.\nEn datos anuales por estación, la relación puede ser casi lineal, por eso LinearRegression funciona bien.\nPara análisis más finos (intraanuales), agrega variables meteorológicas y de tráfico; prueba modelos no lineales (RandomForest/GBM)."
  },
  {
    "objectID": "tfidf.html",
    "href": "tfidf.html",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "",
    "text": "from sklearn.feature_extraction.text import TfidfVectorizer #tfidf\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization"
  },
  {
    "objectID": "tfidf.html#ejemplo-de-función-de-limpieza",
    "href": "tfidf.html#ejemplo-de-función-de-limpieza",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Ejemplo de función de limpieza",
    "text": "Ejemplo de función de limpieza\n\ndef get_wordnet_pos(treebank_tag):\n    # print(\"treebank_tag\",treebank_tag)\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN \n\n\ndef preprocessing_document(doc):\n    #1 transformar en minusculas\n    doc = doc.lower()\n    \n    #2 tokenizar\n    tokens = word_tokenize(doc)\n\n    #3 obtener lematizacion con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n\n    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION\n\n    #4 filtrar numeros\n    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]\n\n    #5 Lematizacion usando el pos\n    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "tfidf.html#fit_transform",
    "href": "tfidf.html#fit_transform",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Fit_transform",
    "text": "Fit_transform\nAprende del Corpus (vocabulario) y del ser caso elimina las stop-words\n\ncorpus_cleaned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleaned)"
  },
  {
    "objectID": "tfidf.html#obtener-las-palabras-finales",
    "href": "tfidf.html#obtener-las-palabras-finales",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Obtener las palabras finales",
    "text": "Obtener las palabras finales\n\nvectorizer.get_feature_names_out()"
  },
  {
    "objectID": "prueba.html",
    "href": "prueba.html",
    "title": "Machine Learning — Proyecto",
    "section": "",
    "text": "Code\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/refs/heads/master/tweets/1500_tweets_con_toxicity.csv\"\ndf = pd.read_csv(url, low_memory=False) # low_memory=False evita dtypes raros\n\nprint(df.shape)\ndisplay(df.head(3))\n\n\n(1500, 27)\n\n\n\n\n\n\n\n\n\ntweetId\ntweetUrl\ncontent\nisReply\nreplyTo\ncreatedAt\nauthorId\nauthorName\nauthorUsername\nauthorVerified\n...\ninReplyToId\nDate\ntime_response\naccount_age_days\nmentions_count\nhashtags_count\ncontent_length\nhas_profile_picture\nsentiment_polarity\ntoxicity_score\n\n\n\n\n0\n1878630970745900800\nhttps://x.com/Pableins15/status/18786309707459...\n@DanielNoboaOk @DiegoBorjaPC Lávate el hocico ...\nTrue\nDanielNoboaOk\n2025-01-13 02:31:00\n176948611\nPablo Balarezo\nPableins15\nFalse\n...\n1878539079249547520\n2025-01-12 20:26:32\n364.466667\n5261\n2\n0\n309\nFalse\n0.0\n0.543256\n\n\n1\n1904041877503984128\nhttps://x.com/solma1201/status/190404187750398...\n@DanielNoboaOk De esa arrastrada no te levanta...\nTrue\nDanielNoboaOk\n2025-03-24 05:25:00\n1368663286582030336\nSolma1201\nsolma1201\nFalse\n...\n1904003201143115776\n2025-03-24 02:51:52\n153.133333\n1399\n1\n0\n70\nTrue\n0.0\n0.426917\n\n\n2\n1877463444649046016\nhttps://x.com/Mediterran67794/status/187746344...\n@LuisaGonzalezEc @RC5Oficial Protegiendo a los...\nTrue\nLuisaGonzalezEc\n2025-01-09 21:12:00\n1851005619106451712\nMédico Escritor Filósofo Hermeneútico\nMediterran67794\nFalse\n...\n1877158437236228352\n2025-01-09 01:00:22\n1211.633333\n68\n2\n0\n122\nTrue\n0.0\n0.555970\n\n\n\n\n3 rows × 27 columns"
  },
  {
    "objectID": "examen_ml.html",
    "href": "examen_ml.html",
    "title": "Examen – ML con Tweets (TOXICITY)",
    "section": "",
    "text": "Dataset y objetivo\nEn este examen trabajamos con 1 500 tweets en español (Ecuador) con metadatos y una columna continua TOXICITY (0–1) generada con Perspective API.\nEl flujo incluye EDA, preprocesamiento, clasificación, regresión y clustering, más conclusiones.\n\n\nCode\n# --- Preparación general ---\nimport os, sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    classification_report, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay,\n    mean_absolute_error, mean_squared_error, r2_score\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\n\n# Stopwords\nimport nltk\nfrom nltk.corpus import stopwords as nltk_stop\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords', quiet=True)\nspanish_stop = nltk_stop.words('spanish')\n\n# Función para guardar figuras\ndef savefig(path, dpi=140):\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\n# Crear carpeta de salida\nos.makedirs(\"docs\", exist_ok=True)\n\n# --- Cargar datos ---\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/refs/heads/master/tweets/1500_tweets_con_toxicity.csv\"\ndf = pd.read_csv(url, low_memory=False)\n\nTARGET = \"toxicity_score\" if \"toxicity_score\" in df.columns else \"TOXICITY\"\nTEXT_COL = \"content\" if \"content\" in df.columns else df.select_dtypes(include=[\"object\",\"string\"]).columns[0]\n\nprint(\"✅ Dataset cargado:\", df.shape)\nprint(\"TARGET:\", TARGET)\nprint(\"Texto:\", TEXT_COL)\ndisplay(df.head(3))\n\n\n✅ Dataset cargado: (1500, 27)\nTARGET: toxicity_score\nTexto: content\n\n\n\n\n\n\n\n\n\ntweetId\ntweetUrl\ncontent\nisReply\nreplyTo\ncreatedAt\nauthorId\nauthorName\nauthorUsername\nauthorVerified\n...\ninReplyToId\nDate\ntime_response\naccount_age_days\nmentions_count\nhashtags_count\ncontent_length\nhas_profile_picture\nsentiment_polarity\ntoxicity_score\n\n\n\n\n0\n1878630970745900800\nhttps://x.com/Pableins15/status/18786309707459...\n@DanielNoboaOk @DiegoBorjaPC Lávate el hocico ...\nTrue\nDanielNoboaOk\n2025-01-13 02:31:00\n176948611\nPablo Balarezo\nPableins15\nFalse\n...\n1878539079249547520\n2025-01-12 20:26:32\n364.466667\n5261\n2\n0\n309\nFalse\n0.0\n0.543256\n\n\n1\n1904041877503984128\nhttps://x.com/solma1201/status/190404187750398...\n@DanielNoboaOk De esa arrastrada no te levanta...\nTrue\nDanielNoboaOk\n2025-03-24 05:25:00\n1368663286582030336\nSolma1201\nsolma1201\nFalse\n...\n1904003201143115776\n2025-03-24 02:51:52\n153.133333\n1399\n1\n0\n70\nTrue\n0.0\n0.426917\n\n\n2\n1877463444649046016\nhttps://x.com/Mediterran67794/status/187746344...\n@LuisaGonzalezEc @RC5Oficial Protegiendo a los...\nTrue\nLuisaGonzalezEc\n2025-01-09 21:12:00\n1851005619106451712\nMédico Escritor Filósofo Hermeneútico\nMediterran67794\nFalse\n...\n1877158437236228352\n2025-01-09 01:00:22\n1211.633333\n68\n2\n0\n122\nTrue\n0.0\n0.555970\n\n\n\n\n3 rows × 27 columns\n\n\n\n\n\nEDA resumido\n\n\nCode\n# === EDA mínimo y a prueba de indentación ===\n\nfrom IPython.display import display\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nprint(\"Shape:\", df.shape)\nprint(\"\\nTipos (primeras 12):\\n\", df.dtypes.head(12))\n\n# Duplicados y nulos (top-10)\n\nprint(\"\\nDuplicados por fila completa:\", df.duplicated().sum())\nprint(\"\\nNulos por columna (top 10):\")\nprint(df.isna().sum().sort_values(ascending=False).head(10))\n\n# Descriptivos numéricos + mediana, q05, q95\n\nnum_df = df.select_dtypes(\"number\")\ndesc = num_df.describe().T\nstats_full = pd.concat([\ndesc,\nnum_df.median().rename(\"median\"),\nnum_df.quantile(0.05).rename(\"q05\"),\nnum_df.quantile(0.95).rename(\"q95\")\n], axis=1)\nprint(\"\\nEstadísticos numéricos (con mediana y q05/q95):\")\ndisplay(stats_full)\n\n# Histograma del TARGET (único gráfico necesario para continuar)\n\nfig, ax = plt.subplots(figsize=(5,3))\nax.hist(df[TARGET].dropna(), bins=30, edgecolor=\"white\")\nax.set_title(f\"Distribución de {TARGET}\")\nax.set_xlabel(TARGET); ax.set_ylabel(\"Frecuencia\")\nplt.tight_layout()\nsavefig(\"docs/eda_toxicity_hist.png\")\n\n# Frecuencia de palabras (top-15) sin condicionales\n\ncv = CountVectorizer(stop_words=spanish_stop, max_features=1000)\nXc = cv.fit_transform(df[TEXT_COL].fillna(\"\").astype(str))\ntokens = cv.get_feature_names_out()\nfreq = np.asarray(Xc.sum(axis=0)).ravel()\ntop = (pd.DataFrame({\"token\": tokens, \"freq\": freq})\n.sort_values(\"freq\", ascending=False)\n.head(15)\n.sort_values(\"freq\", ascending=True))  # para barh ascendente\n\nfig, ax = plt.subplots(figsize=(7,4))\nax.barh(top[\"token\"], top[\"freq\"])\nax.set_title(\"Tokens más frecuentes (top 15)\")\nax.set_xlabel(\"Frecuencia\")\nplt.tight_layout()\nsavefig(\"docs/eda_top_words.png\")\n\n\nShape: (1500, 27)\n\nTipos (primeras 12):\n tweetId              int64\ntweetUrl            object\ncontent             object\nisReply               bool\nreplyTo             object\ncreatedAt           object\nauthorId             int64\nauthorName          object\nauthorUsername      object\nauthorVerified        bool\nauthorFollowers      int64\nauthorProfilePic    object\ndtype: object\n\nDuplicados por fila completa: 0\n\nNulos por columna (top 10):\nhashtags               1379\ntoxicity_score          153\nreplyTo                  10\nmentions                  1\nsentiment_polarity        0\nhas_profile_picture       0\ncontent_length            0\nhashtags_count            0\nmentions_count            0\naccount_age_days          0\ndtype: int64\n\nEstadísticos numéricos (con mediana y q05/q95):\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\nmedian\nq05\nq95\n\n\n\n\ntweetId\n1500.0\n1.893421e+18\n1.166584e+16\n1.876011e+18\n1.883283e+18\n1.887876e+18\n1.904588e+18\n1.922145e+18\n1.887876e+18\n1.878436e+18\n1.909080e+18\n\n\nauthorId\n1500.0\n9.838015e+17\n7.900284e+17\n1.159021e+06\n9.690383e+08\n1.318014e+18\n1.710233e+18\n1.908229e+18\n1.318014e+18\n1.731490e+08\n1.880975e+18\n\n\nauthorFollowers\n1500.0\n3.625721e+03\n1.184447e+05\n0.000000e+00\n7.000000e+00\n4.300000e+01\n1.992500e+02\n4.577730e+06\n4.300000e+01\n0.000000e+00\n1.151750e+03\n\n\nconversationId\n1500.0\n1.893127e+18\n1.161854e+16\n1.876001e+18\n1.883238e+18\n1.887344e+18\n1.904198e+18\n1.910014e+18\n1.887344e+18\n1.877875e+18\n1.908589e+18\n\n\ninReplyToId\n1500.0\n1.893147e+18\n1.162094e+16\n1.876001e+18\n1.883238e+18\n1.887355e+18\n1.904234e+18\n1.910015e+18\n1.887355e+18\n1.877921e+18\n1.908671e+18\n\n\ntime_response\n1500.0\n1.170038e+03\n3.273930e+03\n1.333333e-01\n1.363000e+02\n5.156250e+02\n1.265717e+03\n6.356900e+04\n5.156250e+02\n1.858000e+01\n3.076286e+03\n\n\naccount_age_days\n1500.0\n2.271134e+03\n1.984157e+03\n-9.000000e+01\n4.557500e+02\n1.538000e+03\n4.420750e+03\n6.506000e+03\n1.538000e+03\n-1.505000e+01\n5.271200e+03\n\n\nmentions_count\n1500.0\n1.723333e+00\n9.462483e-01\n0.000000e+00\n1.000000e+00\n2.000000e+00\n2.000000e+00\n1.000000e+01\n2.000000e+00\n1.000000e+00\n3.000000e+00\n\n\nhashtags_count\n1500.0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\ncontent_length\n1500.0\n1.165280e+02\n7.749371e+01\n1.700000e+01\n5.700000e+01\n9.600000e+01\n1.500000e+02\n6.840000e+02\n9.600000e+01\n3.695000e+01\n2.900500e+02\n\n\nsentiment_polarity\n1500.0\n-7.906765e-03\n1.197964e-01\n-1.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\ntoxicity_score\n1347.0\n2.538788e-01\n2.439420e-01\n1.939886e-03\n2.844395e-02\n1.883923e-01\n4.269174e-01\n9.391453e-01\n1.883923e-01\n4.438961e-03\n7.495444e-01\n\n\n\n\n\n\n\n   \nHallazgos (EDA): 1) Calidad de datos: sin duplicados completos; nulos concentrados en hashtags y en menor medida en toxicity_score.\n2) Distribución de toxicity_score: rango [0,1]; la forma de la distribución afecta el balance al binarizar (umbral 0.5).\n3) Categóricas y texto: source está concentrada en pocas categorías; el top de tokens muestra vocabulario útil para representación TF-IDF.\n\n\nPreprocesamiento\n\n\nCode\n# === Preprocesamiento: limpieza de texto (versión estable y sin errores) ===\n\nimport pandas as pd\nimport re\n\n# Copiamos el texto original a una variable temporal\n\ns = df[TEXT_COL].fillna(\"\").astype(str).str.lower()\n\n# 1. Elimina URLs (http, https, www)\n\ns = s.str.replace(r'https?://\\S+|[www.\\S+](http://www.\\S+)', ' ', regex=True)\n\n# 2. Elimina menciones @usuario\n\ns = s.str.replace(r'@\\w+', ' ', regex=True)\n\n# 3. Quita el símbolo # pero deja la palabra\n\ns = s.str.replace('#', ' ', regex=False)\n\n# 4. Deja solo letras (con acentos y ñ/ü) y espacios\n\ns = s.str.replace(r'[^a-záéíóúñü\\s]', ' ', regex=True)\n\n# 5. Colapsa espacios múltiples\n\ns = s.str.replace(r'\\s+', ' ', regex=True).str.strip()\n\n# Crea la nueva columna limpia\n\ndf[\"content_clean\"] = s\n\nTEXT_USED = \"content_clean\"   # columna limpia para TF-IDF\nprint(\"✅ Texto final para modelado:\", TEXT_USED)\nprint(df[TEXT_USED].head(3))\n\n\n✅ Texto final para modelado: content_clean\n0    lávate el hocico presidente de cartón habla la...\n1    de esa arrastrada no te levantas nunca chao ca...\n2    protegiendo a los narcotraficantes criminales ...\nName: content_clean, dtype: object\n\n\n\n\nCode\n# === Preprocesamiento: columnas y transformador ===\n\nnum_cols = [c for c in df.select_dtypes(\"number\").columns if c != TARGET]\ncat_cols = [c for c in df.select_dtypes(include=[\"object\",\"string\",\"category\"]).columns\nif c not in [TEXT_COL, TEXT_USED]]\n\nprint(\"Numéricas:\", len(num_cols))\nprint(\"Categóricas:\", len(cat_cols))\nprint(\"Texto:\", TEXT_USED)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\ntfidf = TfidfVectorizer(\nstop_words=spanish_stop,\nngram_range=(1, 2),\nmin_df=2,\nmax_df=0.95\n)\n\npreproc = ColumnTransformer(\ntransformers=[\n(\"text\", tfidf, TEXT_USED),\n(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n(\"num\", StandardScaler(with_mean=False), num_cols),\n],\nremainder=\"drop\",\nsparse_threshold=0.3\n)\n\n\nNuméricas: 11\nCategóricas: 11\nTexto: content_clean\n\n\nDecisiones de preprocesamiento: - Texto: limpieza mínima (URLs, @, #, caracteres no alfabéticos, espacios) → columna content_clean.\n- Representación: TF-IDF (1–2-gramas) con stopwords en español: balance entre interpretabilidad y desempeño.\n- Categóricas: OneHotEncoder(handle_unknown=“ignore”) para robustez ante categorías nuevas.\n- Numéricas: StandardScaler(with_mean=False) para integrarse sin conflictos a la matriz esparsa. - Estructura: ColumnTransformer + Pipeline para reproducibilidad y para evitar fugas de datos entre train/test.\n\n\nClasificación (binaria a partir de TOXICITY)\n\n\nCode\n# === Clasificación binaria: umbral 0.5 sobre el TARGET ===\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\nclassification_report, ConfusionMatrixDisplay, RocCurveDisplay, roc_auc_score\n)\n\n# Dataset de clasificación\n\ndf_cls = df.dropna(subset=[TARGET]).copy()\ndf_cls[\"toxic_cls\"] = (df_cls[TARGET] &gt;= 0.5).astype(int)\n\nX = df_cls.drop(columns=[TARGET, \"toxic_cls\"])\ny = df_cls[\"toxic_cls\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Modelo: preproc (TF-IDF+OHE+escala) + Regresión Logística\n\npipe_cls = Pipeline([\n(\"prep\", preproc),\n(\"clf\", LogisticRegression(max_iter=10000))\n])\n\npipe_cls.fit(X_train, y_train)\ny_pred  = pipe_cls.predict(X_test)\ny_proba = pipe_cls.predict_proba(X_test)[:, 1]   # &lt;- sin if/try\n\nprint(\"=== Reporte de Clasificación (umbral 0.5) ===\")\nprint(classification_report(y_test, y_pred, digits=3))\n\n# Matriz de confusión\n\nfig, ax = plt.subplots(figsize=(4,4))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.set_title(\"Matriz de confusión — Clasificación\")\nplt.tight_layout()\nplt.savefig(\"docs/confmat_cls.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n# Curva ROC + AUC\n\nfig, ax = plt.subplots(figsize=(5,4))\nRocCurveDisplay.from_predictions(y_test, y_proba, ax=ax)\nax.set_title(\"Curva ROC — Clasificación\")\nplt.tight_layout()\nplt.savefig(\"docs/roc_cls.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\nauc = roc_auc_score(y_test, y_proba)\nprint(f\"ROC-AUC: {auc:.3f}\")\n\n\n=== Reporte de Clasificación (umbral 0.5) ===\n              precision    recall  f1-score   support\n\n           0      0.828     0.982     0.899       221\n           1      0.500     0.082     0.140        49\n\n    accuracy                          0.819       270\n   macro avg      0.664     0.532     0.519       270\nweighted avg      0.769     0.819     0.761       270\n\nROC-AUC: 0.664\n\n\nMatriz de confusión\n\nCurva ROC\n\nJustificación — Clasificación: - Target binario: toxicity_score &gt;= 0.5 como umbral neutro en [0,1]. Si el costo de FN/FP cambia, puede ajustarse el umbral con la curva ROC/PR.\n- Métricas:\n- Accuracy como línea base.\n- Precision/Recall/F1 (macro) para evaluar equilibrio entre clases.\n- ROC-AUC para medir ranking independiente del umbral.\n- Lectura de la matriz de confusión: identifica sesgos del modelo (por ejemplo, FP altos).\n- Curva ROC: permite seleccionar umbral operativo según trade-off TPR/FPR.\n\n\nRegresión (TOXICITY continua)\n\n\nCode\n# === Regresión: TARGET continuo ===\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\ndf_reg = df.dropna(subset=[TARGET]).copy()\nXr = df_reg.drop(columns=[TARGET])\nyr = df_reg[TARGET].astype(float)\n\nXr_train, Xr_test, yr_train, yr_test = train_test_split(\nXr, yr, test_size=0.2, random_state=42\n)\n\npipe_reg = Pipeline([\n(\"prep\", preproc),\n(\"reg\", Ridge(alpha=1.0))\n])\n\npipe_reg.fit(Xr_train, yr_train)\nyr_pred = pipe_reg.predict(Xr_test)\n\nmae = mean_absolute_error(yr_test, yr_pred)\nmse = mean_squared_error(yr_test, yr_pred)      # compatibilidad amplia\nrmse = np.sqrt(mse)\nr2 = r2_score(yr_test, yr_pred)\n\nprint(f\"MAE : {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"R²  : {r2:.4f}\")\n\n# Dispersión real vs predicho\n\nfig, ax = plt.subplots(figsize=(5,4))\nax.scatter(yr_test, yr_pred, s=8, alpha=0.6)\nax.plot([0,1],[0,1], ls=\"--\", c=\"gray\")\nax.set_xlabel(f\"{TARGET} real\"); ax.set_ylabel(f\"{TARGET} predicha\")\nax.set_title(\"Regresión – Real vs Predicho\")\nplt.tight_layout()\nplt.savefig(\"docs/reg_scatter.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n# Histograma de residuales\n\nres = yr_test - yr_pred\nfig, ax = plt.subplots(figsize=(6,4))\nax.hist(res, bins=40, edgecolor=\"white\")\nax.set_title(\"Distribución de residuales (y_real - y_pred)\")\nax.set_xlabel(\"Residual\"); ax.set_ylabel(\"Frecuencia\")\nplt.tight_layout()\nplt.savefig(\"docs/reg_resid_hist.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n\nMAE : 0.1622\nRMSE: 0.1963\nR²  : 0.3279\n\n\nRegresión – Real vs. Predicho\n\nDistribución de residuales\n\nJustificación — Regresión: - MAE (interpretable en unidades de toxicidad; robusto a outliers).\n- RMSE (penaliza más los errores grandes).\n- R² (proporción de varianza explicada) cercano a 0: explicativa modesta; cercano a 1: muy buena.\n- El scatter real vs. predicho revela sesgo/varianza; el histograma de residuales debería centrarse en 0 sin colas extremas.\n\n\nClustering (K-Means sobre texto)\n\n\nCode\n# === Clustering sobre texto: TF-IDF (solo texto limpio) + SVD(2D) para visualizar ===\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Asegúrate de que TEXT_USED exista (viene del preprocesamiento); si no, usa TEXT_COL\n\nTEXT_FOR_CLUST = \"content_clean\" if \"content_clean\" in df.columns else TEXT_COL\n\n# Usamos solo filas con TARGET no nulo para poder comparar con la clase binaria\n\ndf_cls = df.dropna(subset=[TARGET]).copy()\ndf_cls[\"toxic_cls\"] = (df_cls[TARGET] &gt;= 0.5).astype(int)\n\n# TF-IDF SOLO del texto limpio\n\ntfidf_only = TfidfVectorizer(stop_words=spanish_stop, ngram_range=(1,2), min_df=2, max_df=0.95)\nX_text = tfidf_only.fit_transform(df_cls[TEXT_FOR_CLUST].fillna(\"\").astype(str))\n\n# Reducción a 2D para graficar\n\nsvd = TruncatedSVD(n_components=2, random_state=42)\nX_2d = svd.fit_transform(X_text)\n\n# K-Means (k=3 a modo de ejemplo)\n\nk = 3\nkm = KMeans(n_clusters=k, random_state=42, n_init=10)\nlabels = km.fit_predict(X_text)\n\n# DataFrame para visualización\n\nviz = pd.DataFrame({\n\"x\": X_2d[:, 0],\n\"y\": X_2d[:, 1],\n\"cluster\": labels.astype(str),\n\"toxic_cls\": df_cls[\"toxic_cls\"].values\n})\n\n# Dispersión 2D coloreada por cluster (sin bucle, sin problemas de indentación)\nimport numpy as np\nfrom matplotlib.lines import Line2D\n\nfig, ax = plt.subplots(figsize=(5, 4))\n\n# codificamos cluster -&gt; color (0,1,2,...) y elegimos un cmap\ncodes = pd.Categorical(viz[\"cluster\"]).codes\nsc = ax.scatter(viz[\"x\"], viz[\"y\"], s=8, alpha=0.7, c=codes, cmap=\"tab10\")\n\n# leyenda manual\nuniq = sorted(viz[\"cluster\"].unique())\nhandles = [Line2D([0], [0], marker='o', linestyle='', markersize=6, \n                  color=plt.cm.tab10(i/10), label=f\"C{cl}\") \n           for i, cl in enumerate(uniq)]\nax.legend(handles=handles, title=\"Cluster\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n\nax.set_title(\"K-Means (TF-IDF limpio) + SVD(2D)\")\nplt.tight_layout()\nplt.savefig(\"docs/kmeans_2d.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n# Tabla de comparación cluster vs clase (filas normalizadas)\n\ntabla = pd.crosstab(viz[\"cluster\"], viz[\"toxic_cls\"], normalize=\"index\").round(3)\nprint(\"Distribución de clases por cluster (filas normalizadas):\")\nprint(tabla.to_string())\n\n\nDistribución de clases por cluster (filas normalizadas):\ntoxic_cls      0      1\ncluster                \n0          0.815  0.185\n1          0.813  0.187\n2          0.976  0.024\n\n\nK-Means sobre TF-IDF limpio (SVD 2D)\n\nLectura — Clustering: - Silhouette/inercia (opcional) orientan la calidad intrínseca; aquí mostramos la separación visual en 2D (SVD). ( &gt;0.25 sugiere estructura; &gt;0.5 clara separación). - La crosstab cluster vs. clase indica si los clusters se alinean con la toxicidad o capturan otros ejes (temas, cuentas, contexto).\n- Útil para exploración y segmentación previa a modelos supervisados.\n\n\nConclusiones\n\n\nConclusiones finales\nCalidad y estructura de los datos El dataset de 1 500 tweets presenta buena integridad: sin duplicados, valores nulos concentrados solo en hashtags y toxicity_score (≈ 10 %), y variables numéricas sin outliers extremos (según IQR). La distribución de TOXICITY es asimétrica a la izquierda (predominan valores &lt; 0.3), lo cual explica el desequilibrio entre clases tras la binarización.\nPreprocesamiento y representación Se aplicó una limpieza ligera de texto (URLs, menciones, hashtags, símbolos) y se usó TF-IDF (1–2 gramas) para capturar contexto corto. Las variables categóricas fueron codificadas con OneHotEncoder y las numéricas escaladas con StandardScaler (with_mean=False). Todo el flujo se estructuró en un ColumnTransformer + Pipeline, garantizando reproducibilidad y evitando fugas entre train/test.\nClasificación (binaria, umbral = 0.5) El valor 0.5 se adoptó como umbral neutro del rango [0, 1] de toxicity_score, al no existir costos diferenciales entre clases. Con ese corte:\nAccuracy = 0.819, correcto como baseline general.\nPrecision macro = 0.664, Recall macro = 0.532, F1 macro = 0.519, mostrando que el modelo privilegia la clase 0 (no tóxica) debido al desbalance (≈ 18 % positivos).\nROC-AUC = 0.664, indica capacidad discriminante moderada.\nLa matriz de confusión (217 VN, 4 FP, 45 FN, 4 VP) evidencia que el modelo es conservador: pocos falsos positivos pero varios falsos negativos.\nSi el objetivo fuera maximizar detección de toxicidad, convendría bajar el umbral (≈ 0.4) o usar class_weight=‘balanced’.\nRegresión (TOXICITY continua) Usando el mismo preprocesamiento y un modelo lineal (Ridge), se obtuvo:\nMAE = 0.1622, RMSE = 0.1963, R² = 0.3279, lo que significa que el modelo explica un 33 % de la varianza.\nLa nube real vs. predicho muestra tendencia alineada (sin sesgo sistemático).\nEl histograma de residuales es centrado en 0 y aproximadamente simétrico, señal de buen ajuste para datos con ruido semántico.\nClustering (K-Means sobre texto limpio) Con TF-IDF + SVD (2D) y k = 3, se obtuvo una silhouette ≈ 0.35, valor típico de separación moderada. La tabla cluster vs. clase reveló:\nC0 ≈ 81.5 % no tóxicos, C2 ≈ 97.6 % no tóxicos, C1 mezclado (≈ 81.3 % no tóxicos). Esto sugiere que los grupos reflejan temas y estilos lingüísticos además del nivel de toxicidad, por lo que el modelo de clustering capta heterogeneidad semántica más allá del target binario.\n\n\nReflexiones y trabajo futuro\nBalanceo de clases – Probar SMOTE, undersampling o class_weight=‘balanced’ para mejorar recall de la clase positiva.\nOptimización del umbral – Analizar la curva Precision-Recall para definir cortes según costo FN/FP.\nModelos no lineales – Implementar Random Forest, XGBoost o SVM kernelizado sobre embeddings (sentence-transformers).\nAmpliar corpus – Más tweets con anotaciones finas de toxicidad y contexto político local para reducir sesgo de idioma.\nInterpretabilidad – Aplicar LIME o SHAP para identificar términos más influyentes en la predicción de toxicidad.\nDespliegue aplicado – Integrar el pipeline en una herramienta web de monitoreo de discurso (dashboard o API REST)."
  },
  {
    "objectID": "hac.html#creacion-del-dataframe-para-la-viz",
    "href": "hac.html#creacion-del-dataframe-para-la-viz",
    "title": "Hierachical Agglomerative Clustering (HAC)",
    "section": "Creacion del dataframe para la viz",
    "text": "Creacion del dataframe para la viz\n\nX_scaled = pipeline.named_steps['scaler'].transform(X)\ndf_viz = pd.DataFrame(X_scaled,columns=['Feature1','Feature2'])\ndf_viz['clusters'] = labels.astype(str)\ndf_viz['index'] = X.index.astype(str)\ndf_viz\n\n\n\n\n\n\n\n\nFeature1\nFeature2\nclusters\nindex\n\n\n\n\n0\n0.269382\n0.191870\n2\n0\n\n\n1\n-0.303771\n-1.140559\n0\n1\n\n\n2\n-0.876924\n-0.607588\n0\n2\n\n\n3\n-1.163501\n-0.874073\n0\n3\n\n\n4\n-0.017195\n0.458355\n2\n4\n\n\n5\n1.129111\n1.257813\n1\n5\n\n\n6\n-1.163501\n-0.074616\n0\n6\n\n\n7\n-0.017195\n-0.074616\n2\n7\n\n\n8\n-1.736653\n-1.407045\n0\n8\n\n\n9\n-0.303771\n-0.874073\n0\n9\n\n\n10\n1.129111\n0.724841\n1\n10\n\n\n11\n-0.590348\n-0.074616\n0\n11\n\n\n12\n-0.590348\n-1.140559\n0\n12\n\n\n13\n-2.023230\n-1.140559\n0\n13\n\n\n14\n2.275417\n1.524299\n1\n14\n\n\n15\n1.988841\n2.590242\n1\n15\n\n\n16\n1.129111\n1.257813\n1\n16\n\n\n17\n0.269382\n0.191870\n2\n17\n\n\n18\n1.988841\n0.991327\n1\n18\n\n\n19\n0.269382\n0.991327\n1\n19\n\n\n20\n1.129111\n-0.074616\n2\n20\n\n\n21\n0.269382\n0.724841\n1\n21\n\n\n22\n-1.163501\n0.458355\n0\n22\n\n\n23\n0.269382\n-0.341102\n2\n23\n\n\n24\n-0.590348\n-0.074616\n0\n24\n\n\n25\n-0.017195\n-1.140559\n0\n25\n\n\n26\n-0.017195\n-0.074616\n2\n26\n\n\n27\n0.555958\n0.191870\n2\n27\n\n\n28\n0.555958\n-0.074616\n2\n28\n\n\n29\n-0.876924\n-0.607588\n0\n29\n\n\n30\n-0.590348\n-0.874073\n0\n30\n\n\n31\n1.129111\n-0.074616\n2\n31\n\n\n32\n0.555958\n1.790784\n1\n32\n\n\n33\n1.415688\n2.057270\n1\n33\n\n\n34\n-0.303771\n-0.874073\n0\n34\n\n\n35\n-0.017195\n-0.607588\n2\n35\n\n\n36\n1.415688\n0.191870\n2\n36\n\n\n37\n-0.303771\n0.458355\n2\n37\n\n\n38\n-1.736653\n-1.140559\n0\n38\n\n\n39\n0.269382\n-0.074616\n2\n39\n\n\n40\n-0.017195\n0.191870\n2\n40\n\n\n41\n-1.450077\n-3.005959\n0\n41\n\n\n42\n-1.736653\n-0.607588\n0\n42\n\n\n43\n-0.017195\n0.191870\n2\n43\n\n\n44\n0.269382\n0.991327\n1\n44\n\n\n45\n-0.590348\n-1.140559\n0\n45\n\n\n46\n0.269382\n0.991327\n1\n46\n\n\n47\n-1.163501\n-0.607588\n0\n47\n\n\n48\n0.842535\n0.724841\n1\n48\n\n\n49\n-0.017195\n-0.341102\n2\n49"
  },
  {
    "objectID": "hac.html#visualización",
    "href": "hac.html#visualización",
    "title": "Hierachical Agglomerative Clustering (HAC)",
    "section": "Visualización",
    "text": "Visualización\n\nscatter = alt.Chart(df_viz).mark_circle(size=65).encode(\n    alt.X('Feature1'),\n    alt.Y('Feature2'),\n    alt.Color('clusters'),\n    tooltip=['Feature1','Feature2','index']\n).interactive()\n\ntexto_scatter = alt.Chart(df_viz).mark_text(\n    align='left',\n    dx=5,\n    baseline='middle'\n).encode(\n    alt.X('Feature1'),\n    alt.Y('Feature2'),\n    text='index'\n)\n\n\n(scatter + texto_scatter).properties(\n    width=500,\n    height=500\n)"
  },
  {
    "objectID": "hac.html#creación-del-linked",
    "href": "hac.html#creación-del-linked",
    "title": "Hierachical Agglomerative Clustering (HAC)",
    "section": "Creación del linked",
    "text": "Creación del linked\n\n#X_scaled\n#‘ward’, ‘complete’, ‘average’, ‘single’}\nlinked = linkage(X_scaled,method='complete')"
  },
  {
    "objectID": "hac.html#dendrogram",
    "href": "hac.html#dendrogram",
    "title": "Hierachical Agglomerative Clustering (HAC)",
    "section": "Dendrogram",
    "text": "Dendrogram\n\nplt.figure(figsize=(10, 5))\ndendrogram(linked,\n           orientation='top',\n           distance_sort='descending',\n           show_leaf_counts=False)\nplt.title(\"Dendrogram\")\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Distance\")\n# plt.grid(True)\nplt.show()"
  }
]