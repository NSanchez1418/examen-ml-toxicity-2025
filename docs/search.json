[
  {
    "objectID": "tweets_classification.html",
    "href": "tweets_classification.html",
    "title": "Tweets Classification",
    "section": "",
    "text": "import sys, matplotlib\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Matplotlib backend:\", matplotlib.get_backend())\n\nimport matplotlib.pyplot as plt\nplt.plot([0,1,2],[0,1,0])\nplt.title(\"Gráfico de prueba\")\nplt.tight_layout()\nplt.show()\n\nPython: 3.11.0\nMatplotlib backend: module://matplotlib_inline.backend_inline"
  },
  {
    "objectID": "tweets_classification.html#objetivo",
    "href": "tweets_classification.html#objetivo",
    "title": "Tweets Classification",
    "section": "## Objetivo",
    "text": "## Objetivo\nClasificar cuentas en real vs. bot a partir de tweets (texto + metadatos), creando un target heurístico y armando un pipeline con TF-IDF + OneHotEncoder + StandardScaler + LogisticRegression."
  },
  {
    "objectID": "tweets_classification.html#dataset",
    "href": "tweets_classification.html#dataset",
    "title": "Tweets Classification",
    "section": "Dataset",
    "text": "Dataset\nFuente: CSV público (Twitter) – 158,873 filas; 26 columnas."
  },
  {
    "objectID": "tweets_classification.html#selección-del-target-user_type",
    "href": "tweets_classification.html#selección-del-target-user_type",
    "title": "Tweets Classification",
    "section": "Selección del target (user_type)",
    "text": "Selección del target (user_type)\nSe definió user_type ∈ {bot, real} mediante una heurística débil basada en señales (foto de perfil, seguidores, edad de cuenta, etc.). Esta etiqueta sirve para entrenar un modelo que generalice el patrón."
  },
  {
    "objectID": "tweets_classification.html#columnas-eliminadas",
    "href": "tweets_classification.html#columnas-eliminadas",
    "title": "Tweets Classification",
    "section": "Columnas eliminadas",
    "text": "Columnas eliminadas\nSe eliminaron IDs, URLs, timestamps crudos y campos redundantes (p. ej., tweetId, tweetUrl, authorProfilePic, createdAt, mentions, hashtag s, source, etc.) por no aportar valor predictivo directo."
  },
  {
    "objectID": "tweets_classification.html#pipeline",
    "href": "tweets_classification.html#pipeline",
    "title": "Tweets Classification",
    "section": "Pipeline",
    "text": "Pipeline\n\nTexto: TfidfVectorizer(stop_words=spanish, ngram_range=(1,2), min_df=5, max_df=0.90, max_features=50000)\nCategóricas: OneHotEncoder(drop='if_binary') en isReply, authorVerified\nNuméricas: StandardScaler(with_mean=False) en authorFollowers, mentions_count, hashtags_count, time_response, content_length\nModelo: LogisticRegression(max_iter=10000, class_weight={'bot': 2.5, 'real': 1.0})"
  },
  {
    "objectID": "tweets_classification.html#exploración-del-target-potencial",
    "href": "tweets_classification.html#exploración-del-target-potencial",
    "title": "Tweets Classification",
    "section": "Exploración del target potencial",
    "text": "Exploración del target potencial\n\n# Revisar la variación de sentiment_polarity\nprint(df['sentiment_polarity'].describe())\n\nprint(\"\\nValores únicos de sentiment_polarity y su frecuencia:\")\nprint(df['sentiment_polarity'].value_counts().head(10))\n\ncount    158873.000000\nmean         -0.010103\nstd           0.118703\nmin          -1.000000\n25%           0.000000\n50%           0.000000\n75%           0.000000\nmax           1.000000\nName: sentiment_polarity, dtype: float64\n\nValores únicos de sentiment_polarity y su frecuencia:\nsentiment_polarity\n 0.000000    152896\n-1.000000      1411\n 0.200000       559\n-0.500000       549\n-0.100000       430\n 0.033333       285\n-0.400000       264\n 0.500000       178\n-0.800000       168\n 0.375000       164\nName: count, dtype: int64"
  },
  {
    "objectID": "tweets_classification.html#borrar-columnas-irrelevantes",
    "href": "tweets_classification.html#borrar-columnas-irrelevantes",
    "title": "Tweets Classification",
    "section": "Borrar columnas irrelevantes",
    "text": "Borrar columnas irrelevantes\n\n# === Eliminación explícita de columnas irrelevantes ===\nirrelevantes = [\n    'tweetId', 'tweetUrl',                 # IDs/URLs\n    'authorId', 'authorName', 'authorUsername', 'authorProfilePic',\n    'replyTo', 'conversationId', 'inReplyToId',                      # IDs de conversación\n    'createdAt', 'Date', 'authorJoinDate',                           # timestamps en texto\n    'mentions', 'hashtags',                                          # ya tenemos *_count\n    'source'                                                         # casi constante\n    # 'content_length'  # &lt;- si NO quieres usarla, descomenta y se elimina\n]\n\n# Columnas relevantes que conservaremos\nrelevantes = [\n    'content', 'isReply', 'authorVerified', 'has_profile_picture',\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'time_response', 'content_length',\n    'sentiment_polarity'\n]\n\ncols_existentes = [c for c in irrelevantes if c in df.columns]\ndf_clean = df.drop(columns=cols_existentes).copy()\ndf_clean = df_clean[relevantes].copy()\n\nprint(\"Eliminadas:\", len(cols_existentes), \"→\", cols_existentes)\nprint(\"Shape original:\", df.shape, \"→ Shape limpio:\", df_clean.shape)\ndisplay(df_clean.head(3))\n\nEliminadas: 15 → ['tweetId', 'tweetUrl', 'authorId', 'authorName', 'authorUsername', 'authorProfilePic', 'replyTo', 'conversationId', 'inReplyToId', 'createdAt', 'Date', 'authorJoinDate', 'mentions', 'hashtags', 'source']\nShape original: (158873, 26) → Shape limpio: (158873, 11)\n\n\n\n\n\n\n\n\n\ncontent\nisReply\nauthorVerified\nhas_profile_picture\nauthorFollowers\naccount_age_days\nmentions_count\nhashtags_count\ntime_response\ncontent_length\nsentiment_polarity\n\n\n\n\n0\n@DiegoPonguill10 @DanielNoboaOk @LuisaGonzalez...\nTrue\nFalse\nTrue\n145\n1151\n3\n0\n298.5\n88\n0.0\n\n\n1\n@hectorjalonm @DanielNoboaOk @LuisaGonzalezEc ...\nTrue\nFalse\nTrue\n176\n883\n3\n0\n288.5\n119\n0.0\n\n\n2\n@Gregori58965636 @yesendiaz @DanielNoboaOk Otr...\nTrue\nFalse\nTrue\n147\n1153\n3\n0\n279.5\n60\n0.0"
  },
  {
    "objectID": "tweets_classification.html#crear-target-user_type",
    "href": "tweets_classification.html#crear-target-user_type",
    "title": "Tweets Classification",
    "section": "Crear target “user_type”",
    "text": "Crear target “user_type”\n\n# Heurística para etiquetar \"bot\" vs \"real\"\ndef label_user_type(row):\n    score = 0\n    # señales fuertes\n    if not row['has_profile_picture']:\n        score += 2\n    if row['authorFollowers'] &lt; 50:\n        score += 2\n    if row['account_age_days'] &lt; 60:\n        score += 2\n    # señales adicionales\n    if row['mentions_count'] &gt;= 3:\n        score += 1\n    if row['hashtags_count'] &gt;= 3:\n        score += 1\n    if row['content_length'] &lt; 20:\n        score += 1\n    if row['isReply']:\n        score += 1\n    # verificado resta (suele ser humano/organización)\n    if row['authorVerified']:\n        score -= 2\n\n    return \"bot\" if score &gt;= 4 else \"real\"\n\ndf_bot = df_clean.copy()\ndf_bot['user_type'] = df_bot.apply(label_user_type, axis=1)\n\nprint(\"Distribución user_type:\")\nprint(df_bot['user_type'].value_counts())\nprint(\"\\nProporciones:\")\nprint(df_bot['user_type'].value_counts(normalize=True).round(3))\n\n# --- Gráfico: Distribución de clases (user_type) ---\nimport matplotlib.pyplot as plt\n\nax = df_bot['user_type'].value_counts().plot(kind='bar')\nplt.title('Distribución de clases (user_type)')\nplt.xlabel('Clase')\nplt.ylabel('Frecuencia')\nplt.tight_layout()\nplt.show()\n\n# (opcional) guardar la figura para tu informe/presentación\nfig = plt.gcf()\nfig.savefig(\"docs/fig_user_type_dist.png\", dpi=150, bbox_inches='tight')\n\nDistribución user_type:\nuser_type\nreal    126836\nbot      32037\nName: count, dtype: int64\n\nProporciones:\nuser_type\nreal    0.798\nbot     0.202\nName: proportion, dtype: float64\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "href": "tweets_classification.html#seleccion-de-columnas-para-entrenar",
    "title": "Tweets Classification",
    "section": "Seleccion de columnas para entrenar",
    "text": "Seleccion de columnas para entrenar\n\n# Definición de columnas\ntext_col = 'content'\ntarget_col = 'user_type'\n\ncat_cols = ['isReply', 'authorVerified']\nnum_cols = ['authorFollowers', 'mentions_count', 'hashtags_count', 'time_response', 'content_length']\n\nkeep_cols = [text_col, target_col] + cat_cols + num_cols\ndf_train = df_bot[keep_cols].copy()\n\n# Limpieza mínima\ndf_train[text_col] = df_train[text_col].fillna(\"\").astype(str).str.strip()\ndf_train = df_train[df_train[text_col] != \"\"]\n\nprint(\"Shape final para entrenamiento:\", df_train.shape)\ndisplay(df_train.head(3))\n\nShape final para entrenamiento: (158873, 9)\n\n\n\n\n\n\n\n\n\ncontent\nuser_type\nisReply\nauthorVerified\nauthorFollowers\nmentions_count\nhashtags_count\ntime_response\ncontent_length\n\n\n\n\n0\n@DiegoPonguill10 @DanielNoboaOk @LuisaGonzalez...\nreal\nTrue\nFalse\n145\n3\n0\n298.5\n88\n\n\n1\n@hectorjalonm @DanielNoboaOk @LuisaGonzalezEc ...\nreal\nTrue\nFalse\n176\n3\n0\n288.5\n119\n\n\n2\n@Gregori58965636 @yesendiaz @DanielNoboaOk Otr...\nreal\nTrue\nFalse\n147\n3\n0\n279.5\n60"
  },
  {
    "objectID": "tweets_classification.html#train-test-split-estratificado",
    "href": "tweets_classification.html#train-test-split-estratificado",
    "title": "Tweets Classification",
    "section": "Train/ Test Split estratificado",
    "text": "Train/ Test Split estratificado\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_train.drop(columns=[target_col])\ny = df_train[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\nprint(\"Distrib y_train:\\n\", y_train.value_counts(normalize=True).round(3))\n\nTrain: (127098, 8)  Test: (31775, 8)\nDistrib y_train:\n user_type\nreal    0.798\nbot     0.202\nName: proportion, dtype: float64"
  },
  {
    "objectID": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "href": "tweets_classification.html#columntransformer-tf-idf-ohe-escala-numérica",
    "title": "Tweets Classification",
    "section": "ColumnTransformer (TF-IDF + OHE + escala numérica)",
    "text": "ColumnTransformer (TF-IDF + OHE + escala numérica)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords as nltk_stopwords   # ⬅️ NUEVO\n\nspanish_stop = nltk_stopwords.words('spanish')        # ⬅️ NUEVO\n\ntext_vectorizer = TfidfVectorizer(\n    stop_words=spanish_stop,      # ⬅️ en lugar de 'spanish'\n    min_df=5,\n    max_df=0.90,\n    ngram_range=(1, 2),\n    max_features=50000\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', text_vectorizer, text_col),\n        ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary'), cat_cols),\n        ('num', StandardScaler(with_mean=False), num_cols)\n    ],\n    remainder='drop',\n    sparse_threshold=0.3\n)"
  },
  {
    "objectID": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "href": "tweets_classification.html#pipelines-lr-con-textometa-y-nb-solo-texto",
    "title": "Tweets Classification",
    "section": "Pipelines (LR con texto+meta, y NB solo texto)",
    "text": "Pipelines (LR con texto+meta, y NB solo texto)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Regresión Logística (texto + categóricas + numéricas)\npipe_lr = Pipeline([\n    ('prep', preprocessor),\n    ('clf', LogisticRegression(\n        max_iter=10000,\n        class_weight={'bot': 2.5, 'real': 1.0},\n        random_state=42\n    ))\n])\n\n\n# Naive Bayes SOLO TEXTO (baseline)\npipe_nb_text_only = Pipeline([\n    ('tfidf', text_vectorizer),\n    ('nb', MultinomialNB())\n])"
  },
  {
    "objectID": "tweets_classification.html#entrenar-y-evaluar",
    "href": "tweets_classification.html#entrenar-y-evaluar",
    "title": "Tweets Classification",
    "section": "Entrenar y Evaluar",
    "text": "Entrenar y Evaluar\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n# 1) Logistic Regression con todas las features\npipe_lr.fit(X_train, y_train)\ny_pred_lr = pipe_lr.predict(X_test)\nprint(\"=== Logistic Regression (texto + meta) ===\")\nprint(classification_report(y_test, y_pred_lr, digits=3))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr)\nplt.show()\n\n# 2) Naive Bayes solo texto\npipe_nb_text_only.fit(X_train[text_col], y_train)\ny_pred_nb = pipe_nb_text_only.predict(X_test[text_col])\nprint(\"\\n=== MultinomialNB (solo texto) ===\")\nprint(classification_report(y_test, y_pred_nb, digits=3))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_nb)\nplt.show()\n\n=== Logistic Regression (texto + meta) ===\n              precision    recall  f1-score   support\n\n         bot      0.557     0.543     0.550      6407\n        real      0.885     0.891     0.888     25368\n\n    accuracy                          0.821     31775\n   macro avg      0.721     0.717     0.719     31775\nweighted avg      0.819     0.821     0.820     31775\n\n\n\n\n\n\n\n\n\n\n\n=== MultinomialNB (solo texto) ===\n              precision    recall  f1-score   support\n\n         bot      0.706     0.218     0.334      6407\n        real      0.832     0.977     0.899     25368\n\n    accuracy                          0.824     31775\n   macro avg      0.769     0.598     0.616     31775\nweighted avg      0.807     0.824     0.785     31775"
  },
  {
    "objectID": "text_classification.html",
    "href": "text_classification.html",
    "title": "Text classification",
    "section": "",
    "text": "Importar librerías\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_20newsgroups\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# vectorizacion textual\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\n\nCargar el dataset\n\ncategorias = ['comp.graphics','comp.sys.mac.hardware','rec.sport.baseball','talk.politics.misc']\n\nnewsgroups = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\nprint(newsgroups.target_names)\n\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n\n\n\n\nFeatures y target\n\nX_text = newsgroups.data #features\ny = newsgroups.target #target\n\n\n\nTrain-test split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_text,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n\n\nVectorizacion de text\n\nstopwords = stopwords.words('english')\n# vectorizer = CountVectorizer(stop_words=stopwords) \nvectorizer = TfidfVectorizer(stop_words=stopwords)\n\n\n\nModelo\n\nmodel = MultinomialNB()\n\n\n\nDefinición del Pipeline\n\npipeline = Pipeline(\n    [\n        ('vectorizacion',vectorizer),\n        ('classfier',model)\n    ]\n)\n\n\n\nFit del modelo\n\npipeline.fit(X_train,y_train)\n\nPipeline(steps=[('vectorizacion',\n                 TfidfVectorizer(stop_words=['a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...])),\n                ('classfier', MultinomialNB())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vectorizacion', ...), ('classfier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    TfidfVectorizer?Documentation for TfidfVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nanalyzer \n'word'\n\n\n\nstop_words \n['a', 'about', ...]\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nnorm \n'l2'\n\n\n\nuse_idf \nTrue\n\n\n\nsmooth_idf \nTrue\n\n\n\nsublinear_tf \nFalse\n\n\n\n\n            \n        \n    MultinomialNB?Documentation for MultinomialNB\n        \n            \n                Parameters\n                \n\n\n\n\nalpha \n1.0\n\n\n\nforce_alpha \nTrue\n\n\n\nfit_prior \nTrue\n\n\n\nclass_prior \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nPredicción\n\ny_pred = pipeline.predict(X_test)\n\n\n\nReporte de clasificación\n\nprint(\"REPORTE DE CLASIFICACION\\n\",classification_report(y_test,y_pred,target_names=newsgroups.target_names))\n\nREPORTE DE CLASIFICACION\n                           precision    recall  f1-score   support\n\n             alt.atheism       0.93      0.28      0.43        96\n           comp.graphics       0.74      0.69      0.72       117\n comp.os.ms-windows.misc       0.71      0.64      0.67       118\ncomp.sys.ibm.pc.hardware       0.63      0.74      0.68       118\n   comp.sys.mac.hardware       0.80      0.75      0.77       115\n          comp.windows.x       0.75      0.82      0.78       119\n            misc.forsale       0.76      0.69      0.73       117\n               rec.autos       0.86      0.77      0.81       119\n         rec.motorcycles       0.91      0.75      0.82       120\n      rec.sport.baseball       0.88      0.82      0.85       119\n        rec.sport.hockey       0.58      0.92      0.71       120\n               sci.crypt       0.59      0.85      0.70       119\n         sci.electronics       0.83      0.70      0.76       118\n                 sci.med       0.90      0.86      0.88       119\n               sci.space       0.83      0.75      0.79       119\n  soc.religion.christian       0.40      0.94      0.56       120\n      talk.politics.guns       0.75      0.75      0.75       109\n   talk.politics.mideast       0.84      0.87      0.85       113\n      talk.politics.misc       0.97      0.32      0.48        93\n      talk.religion.misc       1.00      0.03      0.05        75\n\n                accuracy                           0.72      2263\n               macro avg       0.78      0.70      0.69      2263\n            weighted avg       0.78      0.72      0.71      2263\n\n\n\n\n\nMatriz de confusión\n\nConfusionMatrixDisplay(\n    confusion_matrix(y_test,y_pred),display_labels=newsgroups.target_names\n).plot(\n    xticks_rotation='vertical'\n)"
  },
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "test",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\n\n[nltk_data] Downloading package stopwords to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "nn.html#fit-aprende-el-vocabulari",
    "href": "nn.html#fit-aprende-el-vocabulari",
    "title": "test",
    "section": "fit: Aprende el vocabulari",
    "text": "fit: Aprende el vocabulari\n\n\nCode\nX = vectorizer.fit_transform(docs)\n\n\n\n\nCode\nvectorizer.get_feature_names_out()\n\n\narray(['ai', 'deep', 'learning', 'love', 'machine', 'part'], dtype=object)"
  },
  {
    "objectID": "nn.html#transform-crea-la-matriz-de-conteos-sparse-matrix",
    "href": "nn.html#transform-crea-la-matriz-de-conteos-sparse-matrix",
    "title": "test",
    "section": "transform: Crea la matriz de conteos (sparse matrix)",
    "text": "transform: Crea la matriz de conteos (sparse matrix)\nCada fila representa un documento, cada columna una palabra del vocabulario\n\n\nCode\nX.toarray()\n\n\narray([[0, 0, 1, 1, 1, 0],\n       [1, 0, 1, 0, 1, 1],\n       [1, 1, 1, 1, 0, 0]])"
  },
  {
    "objectID": "nn.html#visualizar-la-matriz-como-dataframe",
    "href": "nn.html#visualizar-la-matriz-como-dataframe",
    "title": "test",
    "section": "Visualizar la matriz como DataFrame",
    "text": "Visualizar la matriz como DataFrame\n\n\nCode\ndf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)\n\n\n   ai  deep  learning  love  machine  part\n0   0     0         1     1        1     0\n1   1     0         1     0        1     1\n2   1     1         1     1        0     0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proyecto Final — Machine Learning",
    "section": "",
    "text": "Bienvenido\nEste sitio contiene el desarrollo de dos tareas de aprendizaje supervisado:\n\nClasificación: incidentes (accidentes con heridos) con Regresión Logística.\nRegresión: (PM2.5) — lo agregaremos como segunda página.\n\n\nAutor: Noe Sanchez\nAño: 2025\nCurso: ML"
  },
  {
    "objectID": "bagofwords.html",
    "href": "bagofwords.html",
    "title": "Bag of Words (BoW)",
    "section": "",
    "text": "from sklearn.feature_extraction.text import CountVectorizer #BoW\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization\n\n[nltk_data] Downloading package stopwords to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "bagofwords.html#ejemplo-de-función-de-limpieza",
    "href": "bagofwords.html#ejemplo-de-función-de-limpieza",
    "title": "Bag of Words (BoW)",
    "section": "Ejemplo de función de limpieza",
    "text": "Ejemplo de función de limpieza\n\ndef get_wordnet_pos(treebank_tag):\n    # print(\"treebank_tag\",treebank_tag)\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN \n\n\ndef preprocessing_document(doc):\n    #1 transformar en minusculas\n    doc = doc.lower()\n    \n    #2 tokenizar\n    tokens = word_tokenize(doc)\n\n    #3 obtener lematizacion con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n\n    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION\n\n    #4 filtrar numeros\n    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]\n\n    #5 Lematizacion usando el pos\n    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "bagofwords.html#fit_transform",
    "href": "bagofwords.html#fit_transform",
    "title": "Bag of Words (BoW)",
    "section": "Fit_transform",
    "text": "Fit_transform\nAprende del Corpus (vocabulario) y del ser caso elimina las stop-words\n\ncorpus_cleaned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus)"
  },
  {
    "objectID": "bagofwords.html#obtener-las-palabras-finales",
    "href": "bagofwords.html#obtener-las-palabras-finales",
    "title": "Bag of Words (BoW)",
    "section": "Obtener las palabras finales",
    "text": "Obtener las palabras finales\n\nvectorizer.get_feature_names_out()\n\narray(['1990', '2000', 'ai', 'fun', 'learning', 'love', 'machine', 'part',\n       'ving'], dtype=object)"
  },
  {
    "objectID": "clasificacion_incidentes.html",
    "href": "clasificacion_incidentes.html",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Code\n# 1. Importar librerías\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    ConfusionMatrixDisplay, RocCurveDisplay\n)\n\n# 2. Cargar dataset\ndf = pd.read_csv(\"data/incidentes_clasificacion_ready.csv\")\n\ny = df[\"accidente_con_heridos\"]\nX = df.drop(columns=[\"accidente_con_heridos\"])\n\nprint(\"Dimensiones:\", X.shape)\nprint(\"Distribución de clases:\")\nprint(y.value_counts())\n\n# 3. Dividir en train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 4. Entrenar modelo SIN pipeline\nmodel = LogisticRegression(max_iter=100000, class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\n\n# 5. Predicciones\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:,1]\n\n# 6. Métricas\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n==== Resultados SIN Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\n# 7. Matriz de confusión\nConfusionMatrixDisplay.from_predictions(y_test, y_pred)\nplt.title(\"Matriz de Confusión — Sin Pipeline\")\nplt.show()\n\n# 8. Curva ROC\nRocCurveDisplay.from_predictions(y_test, y_proba)\nplt.title(\"Curva ROC — Sin Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# ======== CON PIPELINE ========\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=100000, class_weight=\"balanced\"))\n])\n\npipe.fit(X_train, y_train)\ny_pred_pipe = pipe.predict(X_test)\ny_proba_pipe = pipe.predict_proba(X_test)[:,1]\n\naccuracy = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe, zero_division=0)\nrecall = recall_score(y_test, y_pred_pipe, zero_division=0)\nf1 = f1_score(y_test, y_pred_pipe, zero_division=0)\n\nprint(\"\\n==== Resultados CON Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_pipe)\nplt.title(\"Matriz de Confusión — Con Pipeline\")\nplt.show()\n\nRocCurveDisplay.from_predictions(y_test, y_proba_pipe)\nplt.title(\"Curva ROC — Con Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\nDimensiones: (268065, 4)\nDistribución de clases:\naccidente_con_heridos\n0    265740\n1      2325\nName: count, dtype: int64\n\n==== Resultados SIN Pipeline ====\nAccuracy : 0.5142\nPrecision: 0.0107\nRecall   : 0.6022\nF1 Score : 0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== Resultados CON Pipeline ====\nAccuracy : 0.5142\nPrecision: 0.0107\nRecall   : 0.6022\nF1 Score : 0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResumen del problema.\nSe modeló la probabilidad de accidente con heridos (clase 1) usando Regresión Logística, con un flujo estándar: carga del dataset, división train_test_split con stratify, entrenamiento sin y con Pipeline(StandardScaler + LogisticRegression), evaluación con Accuracy, Precision, Recall, F1 y visualizaciones (Matriz de confusión y Curva ROC).\nResultados clave (lo observado en este experimento): - El accuracy es poco informativo dado el fuerte desbalance (la clase positiva es muy rara).\n- Se obtuvo Recall relativamente alto (capacidad de encontrar positivos) a costa de Precision baja (muchos falsos positivos).\n- Con y sin Pipeline las métricas son muy similares —esperable en un modelo lineal cuando las variables ya están en rangos similares y el desbalance domina el comportamiento."
  },
  {
    "objectID": "clasificacion_incidentes.html#conclusiones-e-interpretación",
    "href": "clasificacion_incidentes.html#conclusiones-e-interpretación",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Resumen del problema.\nSe modeló la probabilidad de accidente con heridos (clase 1) usando Regresión Logística, con un flujo estándar: carga del dataset, división train_test_split con stratify, entrenamiento sin y con Pipeline(StandardScaler + LogisticRegression), evaluación con Accuracy, Precision, Recall, F1 y visualizaciones (Matriz de confusión y Curva ROC).\nResultados clave (lo observado en este experimento): - El accuracy es poco informativo dado el fuerte desbalance (la clase positiva es muy rara).\n- Se obtuvo Recall relativamente alto (capacidad de encontrar positivos) a costa de Precision baja (muchos falsos positivos).\n- Con y sin Pipeline las métricas son muy similares —esperable en un modelo lineal cuando las variables ya están en rangos similares y el desbalance domina el comportamiento."
  },
  {
    "objectID": "ml_text.html",
    "href": "ml_text.html",
    "title": "NLP Key Concepts",
    "section": "",
    "text": "Cargar librerías\n\nimport pandas as pd\nimport requests\n\n# Natural Language Toolkit\nimport nltk\n# downloading some additional packages and corpora\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n[nltk_data] Downloading package punkt_tab to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     C:\\Users\\Martin Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package words to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue\n\n\n\n\nCargar data\n\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\nr = requests.get(url)\nr.encoding = 'utf-8'\n\nstory = r.text\nstory\n\n'The seventh Sally or how Trurl\\'s own perfection led to no good\\nBy StanisÅ‚aw Lem, 1965.\\nTranslated by Michael Kandel, 1974.\\n\\nThe Universe is infinite but bounded, and therefore a beam of light, in whatever direction it may travel, will after billions of centuries return -  if powerful enough - to the point of its departure; and it is no different with rumor, that flies about from star to star and makes the rounds of every planet. One day Trurl heard distant reports of two mighty constructor-benefactors, so wise and so accomplished that they had no equal; with this news he ran to Klapaucius, who explained to him that these were not mysterious rivals, but only themselves, for their fame had circumnavigated space. Fame, however, has this fault, that it says nothing of one\\'s failures, even when those very failures are the product of a great perfection. And he who would doubt this, let him recall the last of the seven sallies of Trurl, which was undertaken without Klapaucius, whom certain urgent duties kept at home at the time.\\n\\nIn those days Trurl was exceedingly vain, receiving all marks of veneration and honor paid to him as his due and a perfectly normal thing. He was heading north in his ship, as he was the least familiar with that region, and had flown through the void for quite some time, passing spheres full of the clamor of war as well as spheres that had finally obtained the perfect peace of desolation, when suddenly a little planet came into view, really more of a stray fragment of matter than a planet.\\n\\nOn the surface of this chunk of rock someone was running back and forth, jumping and waving his arms in the strangest way. Astonished by a scene of such total loneliness and concerned by those wild gestures of despair, and perhaps of anger as well, Trurl quickly landed.\\n\\nHe was approached by a personage of tremendous hauteur, iridium and vanadium all over and with a great deal of clanging and clanking, who introduced himself as Excelsius the Tartarian, ruler of Pancreon and Cyspenderora; the inhabitants of both these kingdoms had, in a fit of regicidal madness, driven His Highness from the throne and exiled him to this barren asteroid, eternally adrift among the dark swells and currents of gravitation.\\n\\nLearning in turn the identity of his visitor, the deposed monarch began to insist that Trurl - who after all was something of a professional when it came to good deeds - immediately restore him to his former position. The thought of such a turn of events brought the flame of vengeance to the monarch\\'s eyes, and his iron fingers clutched the air, as if already closing around the throats of his beloved subjects.\\n\\nNow Trurl had no intention of complying with this request of Excelsius, as doing so would bring about untold evil and suffering, yet at the same time he wished somehow to comfort and console the humiliated king. Thinking a moment or two, he came to the conclusion that, even in this case, not all was lost, for it would be possible to satisfy the king completely - without putting his former subjects in jeopardy. And so, rolling up his sleeves and summoning up all his mastery, Trurl built the king an entirely new kingdom. There were plenty of towns, rivers, mountains, forests, and brooks, a sky with clouds, armies full of derring-do, citadels, castles, and ladies\\' chambers; and there were marketplaces, gaudy and gleaming in the sun, days of back-breaking labor, nights full of dancing and song until dawn, and the gay clatter of swordplay. Trurl also carefully set into this kingdom a fabulous capital, all in marble and alabaster, and assembled a council of hoary sages, and winter palaces and summer villas, plots, conspirators, false witnesses, nurses, informers, teams of magnificent steeds, and plumes waving crimson in the wind; and then he crisscrossed that atmosphere with silver fanfares and twenty-one gun salutes, also threw in the necessary handful of traitors, another of heroes, added a pinch of prophets and seers, and one messiah and one great poet each, after which he bent over and set the works in motion, deftly making last-minute adjustments with his microscopic tools as it ran, and he gave the women of that kingdom beauty, the men - sullen silence and surliness when drunk, the officials - arrogance and servility, the astronomers - an enthusiasm for stars, and the children - a great capacity for noise. And all of this, connected, mounted and ground to precision, fit into a box, and not a very large box, but just the size that could be carried about with ease. This Trurl presented to Excelsius, to rule and have dominion over forever; but first he showed him where the input and output of his brand-new kingdom were, and how to program wars, quell rebellions, exact tribute, collect taxes, and also instructed him in the critical points and transition states of that microminiaturized society - in other words the maxima and minima of palace coups and revolutions -  and explained everything so well that the king, an old hand in the running of tyrannies, instantly grasped the directions and, without hesitation, while the constructor watched, issued a few trial proclamations, correctly manipulating the control knobs, which were carved with imperial eagles and regal lions. These proclamations declared a state of emergency, martial law, a curfew, and a special levy. After a year had passed in the kingdom, which amounted to hardly a minute for Trurl and the king, by an act of the greatest magnanimity - that is, by a flick of the finger at the controls - the king abolished one death penalty, lightened the levy, and deigned to annul the state of emergency, whereupon a tumultuous cry of gratitude, like the squeaking of tiny mice lifted by their tails, rose up from the box, and through its curved glass cover one could see, on the dusty highways and along the banks of lazy rivers that reflected the fluffy clouds, the people rejoicing and praising the great and unsurpassed benevolence of their sovereign lord.\\n\\nAnd so, though at first he had felt insulted by Trurl\\'s gift, in that the kingdom was too small and very like a child\\'s toy, the monarch saw that the thick glass lid made everything inside seem large; perhaps too he dully understood that size was not what mattered here, for government is not measured in meters and kilograms, and emotions are somehow the same, whether experienced by giants or dwarfs - and so he thanked the constructor, if somewhat stiffly. Who knows, he might even have liked to order him thrown in chains and tortured to death, just to be safe - that would have been a sure way of nipping in the bud any gossip about how some common vagabond tinkerer presented a mighty monarch with a kingdom. Excelsius was sensible enough, however, to see that this was out of the question, owing to a very fundamental disproportion, for fleas could sooner take their host into captivity than the king\\'s army seize Trurl. So with another cold nod, he stuck his orb and scepter under his arm, lifted the box kingdom with a grunt, and took it to his humble hut of exile. And as blazing day alternated with murky night outside, according to the rhythm of the asteroid\\'s rotation, the king, who was acknowledged by his subjects as the greatest in the world, diligently reigned, bidding this, forbidding that, beheading, rewarding - in all these ways incessantly spurring his little ones on to perfect fealty and worship of the throne.\\n\\nAs for Trurl, he returned home and related to his friend Klapaucius, not without pride, how he had employed his constructor\\'s genius to indulge the autocratic aspirations of Excelsius and, at the same time, safeguard the democratic aspirations of his former subjects. But Klapaucius, surprisingly enough, had no words of praise for Trurl; in fact, there seemed to be rebuke in his expression.\\n\\n\"Have I understood you correctly?\" he said at last. \"You gave that brutal despot, that born slave master, that slavering sadist of a pain- monger, you gave him a whole civilization to rule and have dominion over forever? And you tell me, moreover, of the cries of joy brought on by the repeal of a fraction of his cruel decrees! Trurl, how could you have done such a thing?\"\\n\\n\"You must be joking!\" Trurl exclaimed. \"Really, the whole kingdom fits into a box three feet by two by two and a half ... it\\'s only a model....\"\\n\\n\"A model of what?\"\\n\\n\"What do you mean, of what? Of a civilization, obviously, except that it\\'s a hundred million times smaller.â€\\x9d\\n\\n\"And how do you know there aren\\'t civilizations a hundred million times larger than our own? And if there were, would ours then be a model? And what importance do dimensions have anyway? In that box kingdom, doesn\\'t a journey from the capital to one of the corners take months - for those inhabitants? And don\\'t they suffer, don\\'t they know the burden of labor, don\\'t they die?\"\\n\\n\"Now just a minute, you know yourself that all these processes take place only because I programmed them, and so they aren\\'t genuine....\"\\n\\n\"Aren\\'t genuine? You mean to say the box is empty, and the parades, tortures, and beheadings are merely an illusion?\"\\n\\n\"Not an illusion, no, since they have reality, though purely as certain microscopic phenomena, which I produced by manipulating atoms,\" said Trurl. \"The point is, these births, loves, acts of heroism, and denunciations are nothing but the minuscule capering of electrons in space, precisely arranged by the skill of my nonlinear craft, which - \"\\n\\n\"Enough of your boasting, not another word!\" Klapaucius snapped. \"Are these processes self-organizing or not?\"\\n\\n\"Of course they are!\"\\n\\n\"And they occur among infinitesimal clouds of electrical charge?\"\\n\\n\"You know they do.\"\"And the phenomenological events of dawns, sunsets, and bloody battles are generated by the concatenation of real variables?\"\\n\\n\"Certainly.\"\\n\\n\"And are not we as well, if you examine us physically, mechanistically, statistically, and meticulously, nothing but the miniscule capering of electron clouds? Positive and negative charges arranged in space? And is our existence not the result of subatomic collisions and the interplay of particles, though we ourselves perceive those molecular cartwheels as fear, longing, or meditation? And when you daydream, what transpires within your brain but the binary algebra of connecting and disconnecting circuits, the continual meandering of electrons?\"\\n\\n\"What, Klapaucius, would you equate our existence with that of an imitation kingdom locked up in some glass box?!\" cried Trurl. \"No, really, that\\'s going too far! My purpose was simply to fashion a simulator of statehood, a model cybernetically perfect, nothing more!\"\\n\\n\"Trurl! Our perfection is our curse, for it draws down upon our every endeavor no end of unforeseeable consequences!\" Klapaucius said in a stentorian voice. \"If an imperfect imitator, wishing to inflict pain, were to build himself a crude idol of wood or wax, and further give it some makeshift semblance of a sentient being, his torture of the thing would be a paltry mockery indeed! But consider a succession of improvements on this practice! Consider the next sculptor, who builds a doll with a recording in its belly, that it may groan beneath his blows; consider a doll which, when beaten, begs for mercy, no longer a crude idol, but a homeostat; consider a doll that sheds tears, a doll that bleeds, a doll that fears death, though it also longs for the peace that only death can bring! Don\\'t you see, when the imitator is perfect, so must be the imitation, and the semblance becomes the truth, the pretense a reality! Trurl, you took an untold number of creatures capable of suffering and abandoned them forever to the rule of a wicked tyrant.... Trurl, you have committed a terrible crime!\"\\n\\n\"Sheer sophistry!\" shouted Trurl, all the louder because he felt the force of his friend\\'s argument. \"Electrons meander not only in our brains, but in phonograph records as well, which proves nothing, and certainly gives no grounds for such hypostatical analogies! The subjects of that monster Excelsius do in fact die when decapitated, sob, fight, and fall in love, since that is how I set up the parameters, but it\\'s impossible to say, Klapaucius, that they feel anything in the process - the electrons jumping around in their heads will tell you nothing of that!\"\\n\\n\"And if I were to look inside your head, I would also see nothing but electrons,\" replied Klapaucius. \"Come now, don\\'t pretend not to understand what I\\'m saying, I know you\\'re not that stupid! A phonograph record won\\'t run errands for you, won\\'t beg for mercy or fall on its knees! You say there\\'s no way of knowing whether Excelsius\\'s subjects groan, when beaten, purely because of the electrons hopping about inside - like wheels grinding out the mimicry of a voice - or whether they really groan, that is, because they honestly experience the pain? A pretty distinction, this! No, Trurl, a sufferer is not one who hands you his suffering, that you may touch it, weigh it, bite it like a coin; a sufferer is one who behaves like a sufferer! Prove to me here and now, once and for all, that they do not feel, that they do not think, that they do not in any way exist as being conscious of their enclosure between the two abysses of oblivion - the abyss before birth and the abyss that follows death - prove this to me, Trurl, and I\\'ll leave you be! Prove that you only imitated suffering, and did not create it!\\n\\n\"You know perfectly well that\\'s impossible,\" answered Trurl quietly. \"Even before I took my instruments in hand, when the box was still empty, I had to anticipate the possibility of precisely such a proof - in order to rule it out. For otherwise the monarch of that kingdom sooner or later would have gotten the impression that his subjects were not real subjects at all, but puppets, marionettes. Try to understand, there was no other way to do it! Anything that would have destroyed in the littlest way the illusion of complete reality would have also destroyed the importance, the dignity of governing, and turned it into nothing but a mechanical game....\"\\n\\n\"I understand, I understand all too well!\" cried Klapaucius. \"Your intentions were the noblest - you only sought to construct a kingdom as lifelike as possible, so similar to a real kingdom, that no one, absolutely no one, could ever tell the difference, and in this, I am afraid, you were successful! Only hours have passed since your return, but for them, the ones imprisoned in that box, whole centuries have gone by - how many beings, how many lives wasted, and all to gratify and feed the vanity of King Excelsius!\"\\n\\nWithout another word Trurl rushed back to his ship, but saw that his friend was coming with him. When he had blasted off into space, pointed the bow between two great clusters of eternal flame and opened the throttle all the way, Klapaucius said:\\n\\n\"Trurl, you\\'re hopeless. You always act first, think later. And now what do you intend to do when we get there?\"\\n\\n\"I\\'ll take the kingdom away from him!\"\\n\\n\"And what will you do with it?\"\\n\\n\"Destroy it!\" Trurl was about to shout, but choked on the first syllable when he realized what he was saying. Finally he mumbled:\\n\\n\"I\\'ll hold an election. Let them choose just rulers from among themselves.\"\\n\\n\"You programmed them all to be feudal lords or shiftless vassals. What good would an election do? First you\\'d have to undo the entire structure of the kingdom, then assemble from scratch ...\"\"And where,\" exclaimed Trurl, \"does the changing of structures end and the tampering with minds begin?!\" Klapaucius had no answer for this, and they flew on in gloomy silence, till the planet of Excelsius came into view. As they circled it, preparing to land, they beheld a most amazing sight.\\n\\nThe entire planet was covered with countless signs of intelligent life. Microscopic bridges, like tiny lines, spanned every rill and rivulet, while the puddles, reflecting the stars, were full of microscopic boats like floating chips.... The night side of the sphere was dotted with glimmering cities, and on the day side one could make out flourishing metropolises, though the inhabitants themselves were much too little to observe, even through the strongest lens. Of the king there was not a trace, as if the earth had swallowed him up.\\n\\n\"He isn\\'t here,\" said Trurl in an awed whisper. \"What have they done with him? Somehow they managed to break through the walls of their box and occupy the asteroid....\"\\n\\n\"Look!\" said Klapaucius, pointing to a little cloud no larger than a thimble and shaped like a mushroom; it slowly rose into the atmosphere. \"They\\'ve discovered atomic energy.... And over there - you see that bit of glass? It\\'s the remains of the box, they\\'ve made it into some sort of temple....\"\\n\\n\"I don\\'t understand. It was only a model, after all. A process with a large number of parameters, a simulation, a mock-up for a monarch to practice on, with the necessary feedback, variables, multistats ...\" muttered Trurl, dumbfounded.\\n\\n\"Yes. But you made the unforgivable mistake of overperfecting your replica. Not wanting to build a mere clocklike mechanism, you inadvertently - in your punctilious way - created that which was possible, logical, and inevitable, that which became the very antithesis of a mechanism....\"\\n\\n\"Please, no more!\" cried Trurl. And they looked out upon the asteroid in silence, when suddenly something bumped their ship, or rather grazed it slightly. They saw this object, for it was illuminated by the thin ribbon of flame that issued from its tail. A ship, probably, or perhaps an artificial satellite, though remarkably similar to one of those steel boots the tyrant Excelsius used to wear. And when the constructors raised their eyes, they beheld a heavenly body shining high above the tiny planet -  it hadn\\'t been there previously - and they recognized, in that cold, pale orb, the stern features of Excelsius himself, who had in this way become the Moon of the Microminians.'\n\n\n\n\nTokenización\n\nfrom nltk import word_tokenize\n\nwords = word_tokenize(story)\nwords[:20]\n\n['The',\n 'seventh',\n 'Sally',\n 'or',\n 'how',\n 'Trurl',\n \"'s\",\n 'own',\n 'perfection',\n 'led',\n 'to',\n 'no',\n 'good',\n 'By',\n 'StanisÅ‚aw',\n 'Lem',\n ',',\n '1965',\n '.',\n 'Translated']\n\n\n\n\nStemming and Lemmatization\n\nfrom nltk.stem import PorterStemmer as stemmer\nfrom nltk.stem import WordNetLemmatizer as lemmatizer\nfrom nltk.corpus import wordnet\n\npalabra = \"changing\"\nprint(\"PALABRA: \", palabra)\n\n#stemming\nprint(\"STEMMING: \",stemmer().stem(palabra))\n\n#lemmatizatio\nprint(\"LEMMATIZATION: \", lemmatizer().lemmatize(palabra,pos=wordnet.VERB))\n\nPALABRA:  changing\nSTEMMING:  chang\nLEMMATIZATION:  change\n\n\n\n\nPART OF SPEECH - POS TAG\n\nfrom nltk import pos_tag\npos = pos_tag(words)\npos[:20]\n\n[('The', 'DT'),\n ('seventh', 'JJ'),\n ('Sally', 'NNP'),\n ('or', 'CC'),\n ('how', 'WRB'),\n ('Trurl', 'NNP'),\n (\"'s\", 'POS'),\n ('own', 'JJ'),\n ('perfection', 'NN'),\n ('led', 'VBD'),\n ('to', 'TO'),\n ('no', 'DT'),\n ('good', 'JJ'),\n ('By', 'IN'),\n ('StanisÅ‚aw', 'NNP'),\n ('Lem', 'NNP'),\n (',', ','),\n ('1965', 'CD'),\n ('.', '.'),\n ('Translated', 'VBN')]\n\n\n\n\nSTOP WORDS\n\nfrom nltk.corpus import stopwords as stop\n\nstopwords = stop.words(\"english\")\nstopwords[:20]\n\n# for item in stop.words(\"spanish\"):\n#     print(item)\n\n['a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been']\n\n\n\n\nSTOP WORDS IN STORY\n\ntokens = nltk.word_tokenize(story.lower())\n# tokens[:20]\n\n# Limpieza depende del contexto de su problemática\n\n# limpieza de numeros\nlettertokens = [word for word in tokens if word.isalpha()]\n\nwithout_stopwords = [word for word in lettertokens if word not in stopwords]\n\nwithout_stopwords[:20]\n\n['seventh',\n 'sally',\n 'trurl',\n 'perfection',\n 'led',\n 'good',\n 'lem',\n 'translated',\n 'michael',\n 'kandel',\n 'universe',\n 'infinite',\n 'bounded',\n 'therefore',\n 'beam',\n 'light',\n 'whatever',\n 'direction',\n 'may',\n 'travel']"
  },
  {
    "objectID": "regresion_pm25.html#cargar-y-explorar",
    "href": "regresion_pm25.html#cargar-y-explorar",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "1) Cargar y explorar",
    "text": "1) Cargar y explorar\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Ruta al dataset limpio que generamos antes\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head(8))\n\n# Nos quedamos SOLO con columnas numéricas (la consigna pide evitar categóricas)\nnum = df.select_dtypes(include=[np.number]).copy()\nprint(\"Numéricas:\", list(num.columns))\ndisplay(num.describe().T)\n\n\n## 2) Seleccionar variables (X, y) y train/test split\nfrom sklearn.model_selection import train_test_split\n\n# Elegir la y:\n# Si existe una columna 'pm25', la usamos. Si no existe, tomamos la última columna numérica como fallback.\ntarget_name = \"pm25\" if \"pm25\" in num.columns else num.columns[-1]\n\ny = num[target_name].values\nX = num.drop(columns=[target_name]).values\n\nprint(\"Objetivo (y):\", target_name)\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\n## 3) Pipeline + LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),  # en regresión lineal no siempre es obligatorio, pero es buena práctica\n    (\"linreg\", LinearRegression())\n])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n\n## 4) Métricas (MSE y R²)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nr2  = r2_score(y_test, y_pred)\n\nprint(f\"MSE : {mse:,.4f}\")\nprint(f\"R²  : {r2:,.4f}\")\n\n\n## 5) Visualización — Real vs Predicho\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(y_test, y_pred, alpha=0.4)\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predicho\")\nplt.title(\"PM2.5 — Real vs Predicho\")\nminv = float(np.min([y_test.min(), y_pred.min()]))\nmaxv = float(np.max([y_test.max(), y_pred.max()]))\nplt.plot([minv, maxv], [minv, maxv])  # línea y=x\nplt.tight_layout()\nplt.show()\n\n\n## 6) Curva de aprendizaje (opcional recomendado)\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    pipe, X, y, cv=5, scoring=\"r2\", n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 6), shuffle=True, random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\nvalid_mean = valid_scores.mean(axis=1)\n\nplt.figure()\nplt.plot(train_sizes, train_mean, marker=\"o\", label=\"Train R²\")\nplt.plot(train_sizes, valid_mean, marker=\"s\", label=\"Valid R²\")\nplt.xlabel(\"Tamaño de entrenamiento\")\nplt.ylabel(\"R²\")\nplt.title(\"Curva de aprendizaje\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nShape: (30983, 14)\n\n\n\n\n\n\n\n\n\nanio_report\nanio\ncodmes\nmes\ndia\ncod_prov\nprov\ncod_cant\ncant\ncod_tipo\ntipo\ncod_est\nestacion\npm2.5\n\n\n\n\n0\n2021\n2005\n1\nEnero\n1\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n37,45\n\n\n1\n2021\n2005\n1\nEnero\n2\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n29,52\n\n\n2\n2021\n2005\n1\nEnero\n3\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n23,77\n\n\n3\n2021\n2005\n1\nEnero\n5\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n19,71\n\n\n4\n2021\n2005\n1\nEnero\n6\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n21,24\n\n\n5\n2021\n2005\n1\nEnero\n7\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n20,83\n\n\n6\n2021\n2005\n1\nEnero\n8\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,43\n\n\n7\n2021\n2005\n1\nEnero\n9\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,65\n\n\n\n\n\n\n\nNuméricas: ['anio_report', 'anio', 'codmes', 'dia', 'cod_prov', 'cod_cant', 'cod_tipo', 'cod_est']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nanio_report\n30983.0\n2021.000000\n0.000000\n2021.0\n2021.0\n2021.0\n2021.0\n2021.0\n\n\nanio\n30983.0\n2012.464577\n4.240917\n2005.0\n2009.0\n2013.0\n2016.0\n2018.0\n\n\ncodmes\n30983.0\n6.593777\n3.435656\n1.0\n4.0\n7.0\n10.0\n12.0\n\n\ndia\n30983.0\n15.726495\n8.806857\n1.0\n8.0\n16.0\n23.0\n31.0\n\n\ncod_prov\n30983.0\n15.570571\n4.563800\n1.0\n17.0\n17.0\n17.0\n17.0\n\n\ncod_cant\n30983.0\n1558.057096\n456.379990\n101.0\n1701.0\n1701.0\n1701.0\n1701.0\n\n\ncod_tipo\n30983.0\n1.000000\n0.000000\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\ncod_est\n30983.0\n6.365749\n8.658158\n1.0\n2.0\n4.0\n5.0\n35.0\n\n\n\n\n\n\n\nObjetivo (y): cod_est\nX shape: (30983, 7) | y shape: (30983,)\nMSE : 4.9328\nR²  : 0.9324"
  },
  {
    "objectID": "regresion_pm25.html#conclusiones-regresión-pm2.5-quito",
    "href": "regresion_pm25.html#conclusiones-regresión-pm2.5-quito",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "Conclusiones — Regresión (PM2.5 Quito)",
    "text": "Conclusiones — Regresión (PM2.5 Quito)\n\n\nCode\n# --- Asegurar que el objetivo (y) sea la medida de PM2.5 ---\n# Ajusta aquí al nombre REAL de tu columna objetivo en data/pm25_ecuador_clean.csv\n# Ejemplos comunes: \"pm25_mean_anual\", \"pm25_max_anual\"\n# Si no existe, hace fallback a \"pm25\" si está; y si tampoco, a la última numérica.\n\nimport pandas as pd, numpy as np\n\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\nnum = df.select_dtypes(include=[np.number]).copy()\n\n# --- Elige tu objetivo aquí ---\ntarget_candidates = [\"pm25_mean_anual\", \"pm25_max_anual\", \"pm25\"]\ntarget_name = None\nfor c in target_candidates:\n    if c in num.columns:\n        target_name = c\n        break\nif target_name is None:\n    target_name = num.columns[-1]  # fallback\n\nX = num.drop(columns=[target_name]).values\ny = num[target_name].values\n\nprint(f\"Objetivo (y): {target_name}\")\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\n\nObjetivo (y): cod_est\nX shape: (30983, 7) | y shape: (30983,)"
  },
  {
    "objectID": "regresion_pm25.html#resumen-del-flujo-aplicado.",
    "href": "regresion_pm25.html#resumen-del-flujo-aplicado.",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "Resumen del flujo aplicado.",
    "text": "Resumen del flujo aplicado.\nSplit: train_test_split(…, test_size=0.25, random_state=42)\nModelo (Pipeline): StandardScaler() + LinearRegression()\nMétricas: MSE y R² (proporción de varianza explicada).\nGráfico: Real vs Predicho con línea identidad.\nResultados de esta corrida (ejemplo):\nMSE bajo y R² alto indican buen ajuste en el agregado anual por estación. Si no obtienes métricas satisfactorias, revisa el objetivo y las columnas numéricas disponibles.\nInterpretación.\nEn datos anuales por estación, la relación puede ser casi lineal, por eso LinearRegression funciona bien.\nPara análisis más finos (intraanuales), agrega variables meteorológicas y de tráfico; prueba modelos no lineales (RandomForest/GBM)."
  },
  {
    "objectID": "tfidf.html",
    "href": "tfidf.html",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "",
    "text": "from sklearn.feature_extraction.text import TfidfVectorizer #tfidf\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization\n\n[nltk_data] Downloading package stopwords to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to C:\\Users\\Martin\n[nltk_data]     Sanchez\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "tfidf.html#ejemplo-de-función-de-limpieza",
    "href": "tfidf.html#ejemplo-de-función-de-limpieza",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Ejemplo de función de limpieza",
    "text": "Ejemplo de función de limpieza\n\ndef get_wordnet_pos(treebank_tag):\n    # print(\"treebank_tag\",treebank_tag)\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN \n\n\ndef preprocessing_document(doc):\n    #1 transformar en minusculas\n    doc = doc.lower()\n    \n    #2 tokenizar\n    tokens = word_tokenize(doc)\n\n    #3 obtener lematizacion con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n\n    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION\n\n    #4 filtrar numeros\n    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]\n\n    #5 Lematizacion usando el pos\n    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "tfidf.html#fit_transform",
    "href": "tfidf.html#fit_transform",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Fit_transform",
    "text": "Fit_transform\nAprende del Corpus (vocabulario) y del ser caso elimina las stop-words\n\ncorpus_cleaned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleaned)"
  },
  {
    "objectID": "tfidf.html#obtener-las-palabras-finales",
    "href": "tfidf.html#obtener-las-palabras-finales",
    "title": "Term-Frequency - Inverse Document Frequency (TF-IDF)",
    "section": "Obtener las palabras finales",
    "text": "Obtener las palabras finales\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)"
  },
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "knn.html#generación-de-pipeline-por-tipo-de-variable",
    "href": "knn.html#generación-de-pipeline-por-tipo-de-variable",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Generación de pipeline por tipo de variable",
    "text": "Generación de pipeline por tipo de variable\n\ncategorical_pipeline = Pipeline([\n    ('imputacion_cat',SimpleImputer(strategy='most_frequent')),\n    ('encodage_cat',OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n# Mujer, Hombre =&gt; (1,0) =&gt; StandarScaler() NO HACER\n\nnumerical_pipeline = Pipeline([\n    ('imputacion_num',SimpleImputer(strategy='mean')),\n    ('escalamiento',StandardScaler())\n])"
  },
  {
    "objectID": "knn.html#aplicar-columntransformer",
    "href": "knn.html#aplicar-columntransformer",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Aplicar ColumnTransformer",
    "text": "Aplicar ColumnTransformer\n\npreprocessor = ColumnTransformer([\n    ('cat',categorical_pipeline,cat_cols),\n    ('num',numerical_pipeline,num_cols)\n])"
  },
  {
    "objectID": "knn.html#definir-el-param_grid",
    "href": "knn.html#definir-el-param_grid",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Definir el param_grid",
    "text": "Definir el param_grid\n\nparam_grid = {\n    'classificador__n_neighbors':list(range(1,5)),\n    'classificador__weights':['uniform','distance'],\n    'classificador__metric':['minkowski','euclidean','manhattan']\n}"
  },
  {
    "objectID": "knn.html#realizar-el-gridseach",
    "href": "knn.html#realizar-el-gridseach",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Realizar el Gridseach",
    "text": "Realizar el Gridseach\n\ngrid = GridSearchCV(pipeline,param_grid,cv=5,scoring='f1')\ngrid.fit(X_train,y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('cat',\n                                                                         Pipeline(steps=[('imputacion_cat',\n                                                                                          SimpleImputer(strategy='most_frequent')),\n                                                                                         ('encodage_cat',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['sex',\n                                                                          'embarked']),\n                                                                        ('num',\n                                                                         Pipeline(steps=[('imputacion_num',\n                                                                                          SimpleImputer()),\n                                                                                         ('escalamiento',\n                                                                                          StandardScaler())]),\n                                                                         ['age',\n                                                                          'sibsp',\n                                                                          'parch',\n                                                                          'pclass'])])),\n                                       ('classificador',\n                                        KNeighborsClassifier())]),\n             param_grid={'classificador__metric': ['minkowski', 'euclidean',\n                                                   'manhattan'],\n                         'classificador__n_neighbors': [1, 2, 3, 4],\n                         'classificador__weights': ['uniform', 'distance']},\n             scoring='f1')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nPipeline(step...lassifier())])\n\n\n\nparam_grid \n{'classificador__metric': ['minkowski', 'euclidean', ...], 'classificador__n_neighbors': [1, 2, ...], 'classificador__weights': ['uniform', 'distance']}\n\n\n\nscoring \n'f1'\n\n\n\nn_jobs \nNone\n\n\n\nrefit \nTrue\n\n\n\ncv \n5\n\n\n\nverbose \n0\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nerror_score \nnan\n\n\n\nreturn_train_score \nFalse\n\n\n\n\n            \n        \n    best_estimator_: Pipelinepreprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('cat', ...), ('num', ...)]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    cat['sex', 'embarked']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'most_frequent'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    num['age', 'sibsp', 'parch', 'pclass']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'mean'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    KNeighborsClassifier?Documentation for KNeighborsClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n3\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'manhattan'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone"
  },
  {
    "objectID": "knn.html#obtener-el-mejor-best_estimator",
    "href": "knn.html#obtener-el-mejor-best_estimator",
    "title": "K-Nearest Neighbors (KNN)",
    "section": "Obtener el mejor best_estimator",
    "text": "Obtener el mejor best_estimator\n\nbest_model = grid.best_estimator_"
  },
  {
    "objectID": "altair.html",
    "href": "altair.html",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\nGráfico de barras\ncount() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n)\n\n\n\n\n\n\n\n\nmean() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\n\n\n\n\n\n\nGráfico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n).interactive()"
  },
  {
    "objectID": "altair.html#importar-el-dataset",
    "href": "altair.html#importar-el-dataset",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA"
  },
  {
    "objectID": "altair.html#visualización-en-altair",
    "href": "altair.html#visualización-en-altair",
    "title": "Introducción a Altair",
    "section": "",
    "text": "Code\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\nGráfico de barras\ncount() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n)\n\n\n\n\n\n\n\n\nmean() de cada una de los origines\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\n\n\n\n\n\n\nGráfico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n).interactive()"
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "K-means",
    "section": "",
    "text": "import altair as alt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n#---\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow",
    "href": "kmeans.html#método-del-codo-elbow",
    "title": "K-means",
    "section": "Método del codo (Elbow)",
    "text": "Método del codo (Elbow)\n\nCalcular el Sum of Squared Errors (SSE)\n\nsse = [] #inertia\nk_range = range(2,11) #valores posibles de k\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k,random_state=42)\n    kmeans.fit(df[['age','annual_income($)']])\n    sse.append(kmeans.inertia_)\n\nprint(\"Inertia:\\n\",sse)\n\nInertia:\n [275802340.8465648, 103879919.55, 82797577.45817567, 38460868.83850575, 27144269.551419172, 19757179.065581053, 14468762.582687736, 11828359.755827505, 9333390.053670242]\n\n\n\n\nVisualizar el Sum of Squared Errors (SSE) vs K\n\nelbow_df = pd.DataFrame(\n    {\n        'K':list(k_range),\n        'SSE':sse\n    }\n)\n\n# elbow_df\n\nalt.Chart(elbow_df).mark_line(point=True).encode(\n    alt.X(\"K\"),\n    alt.Y(\"SSE\"),\n    tooltip=[\"K\",\"SSE\"]\n).properties(\n    title=\"Método del Codo\"\n).interactive()"
  },
  {
    "objectID": "kmeans.html#clusterización",
    "href": "kmeans.html#clusterización",
    "title": "K-means",
    "section": "Clusterización",
    "text": "Clusterización\n\nk_clusters = 3\nclusters = KMeans(n_clusters=k_clusters,random_state=42)\ndf['clusters'] = clusters.fit_predict(df[['age','annual_income($)']])\ndf.head()\n\n\n\n\n\n\n\n\nage\nannual_income($)\nclusters\n\n\n\n\n0\n17\n17123\n2\n\n\n1\n17\n18375\n2\n\n\n2\n16\n18557\n2\n\n\n3\n23\n17721\n2\n\n\n4\n22\n17291\n2\n\n\n\n\n\n\n\n\nVisualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters:N'),\n    tooltip=['age','annual_income($)','clusters:N']\n).properties(\n    title='CLUSTERIZACION'\n).interactive()"
  },
  {
    "objectID": "kmeans.html#columntransformer",
    "href": "kmeans.html#columntransformer",
    "title": "K-means",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\n\nnum_cols = ['age','annual_income($)']\n\npreprocessor = ColumnTransformer(\n    [\n        ('standar',StandardScaler(),num_cols)\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#pipeline",
    "href": "kmeans.html#pipeline",
    "title": "K-means",
    "section": "Pipeline",
    "text": "Pipeline\n\npipeline = Pipeline(\n    [\n        ('scaler',preprocessor),\n        ('kmeans',KMeans(random_state=42))\n    ]\n)"
  },
  {
    "objectID": "kmeans.html#método-del-codo-elbow-silhouette",
    "href": "kmeans.html#método-del-codo-elbow-silhouette",
    "title": "K-means",
    "section": "Método del codo (Elbow) + Silhouette",
    "text": "Método del codo (Elbow) + Silhouette\n\nCalcular el Sum of Squared Errors (SSE) + Silhouette\n\nk_range = range(2,11)\nsse_standar = []\nsilhouette_scores = []\n\nfor k in k_range:\n    pipeline.set_params(kmeans__n_clusters=k)\n    pipeline.fit(df[num_cols])\n    sse_standar.append(\n        pipeline.named_steps['kmeans'].inertia_\n    )\n\n    pred = pipeline.predict(df[num_cols])\n    print(\"K:\",k)\n    print(\"PRED\",pred)\n    score = silhouette_score(pipeline.named_steps['scaler'].transform(df[num_cols]),pred)\n    print(\"SILHOUETTE SCORE\",score)\n    print(\"\\n\")\n    silhouette_scores.append(score)\n\n#silhouette_score\n\nK: 2\nPRED [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1]\nSILHOUETTE SCORE 0.5584253334367935\n\n\nK: 3\nPRED [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1]\nSILHOUETTE SCORE 0.48862751188830494\n\n\nK: 4\nPRED [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.6197880502586566\n\n\nK: 5\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5870741352565944\n\n\nK: 6\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 5 5 5 5 5 1 5\n 5 5 5 1 5 1 1 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5496403913716196\n\n\nK: 7\nPRED [2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 2 4 2 4 2 2 2 4 4 4 4 2 2 4 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 6 6 6 5 6 6 5 6 5 6 1 5\n 6 6 5 5 5 5 5 6 5 6 6 6 6 6 6 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3]\nSILHOUETTE SCORE 0.5351154400530848\n\n\nK: 8\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 0 0 4 4 4 4 0 4 0 0 0 4 4 4 5 5 7 7 7 5 5 5 5 5 5 5 1 5\n 7 5 5 5 5 5 5 7 5 7 5 5 7 5 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.49714020005833653\n\n\nK: 9\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 0 0 4 4 4 4 0 4 0 0 0 4 4 4 8 5 7 7 7 8 5 5 8 5 5 5 8 8\n 7 5 8 8 5 8 8 7 8 7 5 5 7 5 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.48277202641477174\n\n\nK: 10\nPRED [3 3 3 3 3 6 3 6 6 3 3 3 3 6 6 6 3 6 3 6 3 3 3 6 6 6 6 3 3 6 4 4 0 4 0 4 0\n 4 4 0 0 0 4 0 0 0 4 0 4 4 4 4 0 4 0 0 0 4 4 4 9 9 7 7 5 9 5 5 9 5 9 5 8 9\n 5 5 9 8 5 9 9 7 9 5 5 5 7 5 7 5 1 8 8 8 1 1 8 1 8 8 8 8 1 8 1 8 1 8 8 1 1\n 8 8 8 8 8 1 8 8 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nSILHOUETTE SCORE 0.49114927636373373"
  },
  {
    "objectID": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "href": "kmeans.html#visualizar-el-sum-of-squared-errors-sse-silhouette-vs-k",
    "title": "K-means",
    "section": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K",
    "text": "Visualizar el Sum of Squared Errors (SSE), Silhouette vs K\n\nsse_standard_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SSE_STANDARD':sse_standar\n    }\n)\n\nsilhouette_df = pd.DataFrame(\n    {\n        'K':k_range,\n        'SILHOUETTE':silhouette_scores\n    }\n)\n\nviz_sse_standard = alt.Chart(sse_standard_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SSE_STANDARD'),\n    tooltip=['K',\"SSE_STANDARD\"]\n).properties(\n    title='METODO DEL CODO CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_silhouette = alt.Chart(silhouette_df).mark_line(point=True).encode(\n    alt.X('K'),\n    alt.Y('SILHOUETTE'),\n    tooltip=['K',\"SILHOUETTE\"]\n).properties(\n    title='SILHOUETTE SCORE CON PIPELINE + ESTANDARIZACION'\n).interactive()\n\n\nviz_sse_standard | viz_silhouette"
  },
  {
    "objectID": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "href": "kmeans.html#clusterizacion-con-pipeline-standardscaler",
    "title": "K-means",
    "section": "Clusterizacion con Pipeline + StandardScaler()",
    "text": "Clusterizacion con Pipeline + StandardScaler()\n\nk_optimo = k_range[silhouette_scores.index(max(silhouette_scores))]\nprint(\"K_OPTIMO\",k_optimo)\n\npipeline.set_params(kmeans__n_clusters=k_optimo)\ndf['clusters_standard'] = pipeline.fit_predict(df[num_cols])\ndf.head()\n\nK_OPTIMO 4\n\n\n\n\n\n\n\n\n\nage\nannual_income($)\nclusters\nclusters_standard\n\n\n\n\n0\n17\n17123\n2\n2\n\n\n1\n17\n18375\n2\n2\n\n\n2\n16\n18557\n2\n2\n\n\n3\n23\n17721\n2\n2\n\n\n4\n22\n17291\n2\n2"
  },
  {
    "objectID": "kmeans.html#visualizar-la-clusterización-1",
    "href": "kmeans.html#visualizar-la-clusterización-1",
    "title": "K-means",
    "section": "Visualizar la clusterización",
    "text": "Visualizar la clusterización\n\nalt.Chart(df).mark_circle(size=20).encode(\n    alt.X('age'),\n    alt.Y('annual_income($)'),\n    alt.Color('clusters_standard:N'),\n    tooltip=['age','annual_income($)','clusters_standard:N']\n).properties(\n    title='CLUSTERIZACION CON PIPELINE + STANDARSCALER()'\n).interactive()"
  },
  {
    "objectID": "hac.html",
    "href": "hac.html",
    "title": "Hierarchical Aglomerative Clustering (HAC)",
    "section": "",
    "text": "Importar librerias\n\nimport pandas as pd\nimport altair as alt\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\n\nCargar el Dataset\n\niris = load_iris()\niris\n\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'iris.csv',\n 'data_module': 'sklearn.datasets.data'}"
  }
]