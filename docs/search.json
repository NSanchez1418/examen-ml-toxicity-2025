[
  {
    "objectID": "regresion_pm25.html",
    "href": "regresion_pm25.html",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\n\n# Ruta al dataset limpio que generamos antes\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head(8))\n\n# Nos quedamos SOLO con columnas numéricas (la consigna pide evitar categóricas)\nnum = df.select_dtypes(include=[np.number]).copy()\nprint(\"Numéricas:\", list(num.columns))\ndisplay(num.describe().T)\n\n## 2) Seleccionar variables (X, y) y train/test split\nfrom sklearn.model_selection import train_test_split\n\n# Elegir la y:\n# Si existe una columna 'pm25', la usamos. Si no existe, tomamos la última columna numérica como fallback.\ntarget_name = \"pm25\" if \"pm25\" in num.columns else num.columns[-1]\n\ny = num[target_name].values\nX = num.drop(columns=[target_name]).values\n\nprint(\"Objetivo (y):\", target_name)\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\n## 3) Pipeline + LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),  # en regresión lineal no siempre es obligatorio, pero es buena práctica\n    (\"linreg\", LinearRegression())\n])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n\n## 4) Métricas (MSE y R²)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nr2  = r2_score(y_test, y_pred)\n\nprint(f\"MSE : {mse:,.4f}\")\nprint(f\"R²  : {r2:,.4f}\")\n\n\n## 5) Visualización — Real vs Predicho\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(y_test, y_pred, alpha=0.4)\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predicho\")\nplt.title(\"PM2.5 — Real vs Predicho\")\nminv = float(np.min([y_test.min(), y_pred.min()]))\nmaxv = float(np.max([y_test.max(), y_pred.max()]))\nplt.plot([minv, maxv], [minv, maxv])  # línea y=x\nplt.tight_layout()\nplt.show()\n\n\n## 6) Curva de aprendizaje (opcional recomendado)\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    pipe, X, y, cv=5, scoring=\"r2\", n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 6), shuffle=True, random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\nvalid_mean = valid_scores.mean(axis=1)\n\nplt.figure()\nplt.plot(train_sizes, train_mean, marker=\"o\", label=\"Train R²\")\nplt.plot(train_sizes, valid_mean, marker=\"s\", label=\"Valid R²\")\nplt.xlabel(\"Tamaño de entrenamiento\")\nplt.ylabel(\"R²\")\nplt.title(\"Curva de aprendizaje\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nShape: (30983, 14)\n\n\n\n\n\n\n\n\n\nanio_report\nanio\ncodmes\nmes\ndia\ncod_prov\nprov\ncod_cant\ncant\ncod_tipo\ntipo\ncod_est\nestacion\npm2.5\n\n\n\n\n0\n2021\n2005\n1\nEnero\n1\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n37,45\n\n\n1\n2021\n2005\n1\nEnero\n2\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n29,52\n\n\n2\n2021\n2005\n1\nEnero\n3\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n23,77\n\n\n3\n2021\n2005\n1\nEnero\n5\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n19,71\n\n\n4\n2021\n2005\n1\nEnero\n6\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n21,24\n\n\n5\n2021\n2005\n1\nEnero\n7\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n20,83\n\n\n6\n2021\n2005\n1\nEnero\n8\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,43\n\n\n7\n2021\n2005\n1\nEnero\n9\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,65\n\n\n\n\n\n\n\nNuméricas: ['anio_report', 'anio', 'codmes', 'dia', 'cod_prov', 'cod_cant', 'cod_tipo', 'cod_est']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nanio_report\n30983.0\n2021.000000\n0.000000\n2021.0\n2021.0\n2021.0\n2021.0\n2021.0\n\n\nanio\n30983.0\n2012.464577\n4.240917\n2005.0\n2009.0\n2013.0\n2016.0\n2018.0\n\n\ncodmes\n30983.0\n6.593777\n3.435656\n1.0\n4.0\n7.0\n10.0\n12.0\n\n\ndia\n30983.0\n15.726495\n8.806857\n1.0\n8.0\n16.0\n23.0\n31.0\n\n\ncod_prov\n30983.0\n15.570571\n4.563800\n1.0\n17.0\n17.0\n17.0\n17.0\n\n\ncod_cant\n30983.0\n1558.057096\n456.379990\n101.0\n1701.0\n1701.0\n1701.0\n1701.0\n\n\ncod_tipo\n30983.0\n1.000000\n0.000000\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\ncod_est\n30983.0\n6.365749\n8.658158\n1.0\n2.0\n4.0\n5.0\n35.0\n\n\n\n\n\n\n\nObjetivo (y): cod_est\nX shape: (30983, 7) | y shape: (30983,)\nMSE : 4.9328\nR²  : 0.9324\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##7) Conclusiones rápidas #Objetivo: predecir el valor continuo de PM2.5 usando solo variables numéricas.\n#Modelo: Pipeline(StandardScaler + LinearRegression).\n#Métricas: reporta MSE y R²; idealmente R² &gt; 0 para que el modelo supere a una media constante.\n#Si el R² es bajo o negativo:\n#Revisa variables: quizá faltan características relevantes (meteorología, ubicación, hora, etc.).\n#Considera limpiar outliers o agregar transformaciones.\n#Intenta otro modelo (p.ej., RandomForestRegressor) si la consigna lo permite."
  },
  {
    "objectID": "regresion_pm25.html#cargar-y-explorar",
    "href": "regresion_pm25.html#cargar-y-explorar",
    "title": "Regresión — PM2.5 (Ecuador)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\n\n# Ruta al dataset limpio que generamos antes\ndf = pd.read_csv(\"data/pm25_ecuador_clean.csv\")\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head(8))\n\n# Nos quedamos SOLO con columnas numéricas (la consigna pide evitar categóricas)\nnum = df.select_dtypes(include=[np.number]).copy()\nprint(\"Numéricas:\", list(num.columns))\ndisplay(num.describe().T)\n\n## 2) Seleccionar variables (X, y) y train/test split\nfrom sklearn.model_selection import train_test_split\n\n# Elegir la y:\n# Si existe una columna 'pm25', la usamos. Si no existe, tomamos la última columna numérica como fallback.\ntarget_name = \"pm25\" if \"pm25\" in num.columns else num.columns[-1]\n\ny = num[target_name].values\nX = num.drop(columns=[target_name]).values\n\nprint(\"Objetivo (y):\", target_name)\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\n## 3) Pipeline + LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),  # en regresión lineal no siempre es obligatorio, pero es buena práctica\n    (\"linreg\", LinearRegression())\n])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n\n## 4) Métricas (MSE y R²)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nr2  = r2_score(y_test, y_pred)\n\nprint(f\"MSE : {mse:,.4f}\")\nprint(f\"R²  : {r2:,.4f}\")\n\n\n## 5) Visualización — Real vs Predicho\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(y_test, y_pred, alpha=0.4)\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predicho\")\nplt.title(\"PM2.5 — Real vs Predicho\")\nminv = float(np.min([y_test.min(), y_pred.min()]))\nmaxv = float(np.max([y_test.max(), y_pred.max()]))\nplt.plot([minv, maxv], [minv, maxv])  # línea y=x\nplt.tight_layout()\nplt.show()\n\n\n## 6) Curva de aprendizaje (opcional recomendado)\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    pipe, X, y, cv=5, scoring=\"r2\", n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 6), shuffle=True, random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\nvalid_mean = valid_scores.mean(axis=1)\n\nplt.figure()\nplt.plot(train_sizes, train_mean, marker=\"o\", label=\"Train R²\")\nplt.plot(train_sizes, valid_mean, marker=\"s\", label=\"Valid R²\")\nplt.xlabel(\"Tamaño de entrenamiento\")\nplt.ylabel(\"R²\")\nplt.title(\"Curva de aprendizaje\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nShape: (30983, 14)\n\n\n\n\n\n\n\n\n\nanio_report\nanio\ncodmes\nmes\ndia\ncod_prov\nprov\ncod_cant\ncant\ncod_tipo\ntipo\ncod_est\nestacion\npm2.5\n\n\n\n\n0\n2021\n2005\n1\nEnero\n1\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n37,45\n\n\n1\n2021\n2005\n1\nEnero\n2\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n29,52\n\n\n2\n2021\n2005\n1\nEnero\n3\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n23,77\n\n\n3\n2021\n2005\n1\nEnero\n5\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n19,71\n\n\n4\n2021\n2005\n1\nEnero\n6\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n21,24\n\n\n5\n2021\n2005\n1\nEnero\n7\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n20,83\n\n\n6\n2021\n2005\n1\nEnero\n8\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,43\n\n\n7\n2021\n2005\n1\nEnero\n9\n17\nPichincha\n1701\nDistrito Metropolitano de Quito\n1\nAutomática\n3\nBelisario\n17,65\n\n\n\n\n\n\n\nNuméricas: ['anio_report', 'anio', 'codmes', 'dia', 'cod_prov', 'cod_cant', 'cod_tipo', 'cod_est']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nanio_report\n30983.0\n2021.000000\n0.000000\n2021.0\n2021.0\n2021.0\n2021.0\n2021.0\n\n\nanio\n30983.0\n2012.464577\n4.240917\n2005.0\n2009.0\n2013.0\n2016.0\n2018.0\n\n\ncodmes\n30983.0\n6.593777\n3.435656\n1.0\n4.0\n7.0\n10.0\n12.0\n\n\ndia\n30983.0\n15.726495\n8.806857\n1.0\n8.0\n16.0\n23.0\n31.0\n\n\ncod_prov\n30983.0\n15.570571\n4.563800\n1.0\n17.0\n17.0\n17.0\n17.0\n\n\ncod_cant\n30983.0\n1558.057096\n456.379990\n101.0\n1701.0\n1701.0\n1701.0\n1701.0\n\n\ncod_tipo\n30983.0\n1.000000\n0.000000\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\ncod_est\n30983.0\n6.365749\n8.658158\n1.0\n2.0\n4.0\n5.0\n35.0\n\n\n\n\n\n\n\nObjetivo (y): cod_est\nX shape: (30983, 7) | y shape: (30983,)\nMSE : 4.9328\nR²  : 0.9324\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##7) Conclusiones rápidas #Objetivo: predecir el valor continuo de PM2.5 usando solo variables numéricas.\n#Modelo: Pipeline(StandardScaler + LinearRegression).\n#Métricas: reporta MSE y R²; idealmente R² &gt; 0 para que el modelo supere a una media constante.\n#Si el R² es bajo o negativo:\n#Revisa variables: quizá faltan características relevantes (meteorología, ubicación, hora, etc.).\n#Considera limpiar outliers o agregar transformaciones.\n#Intenta otro modelo (p.ej., RandomForestRegressor) si la consigna lo permite."
  },
  {
    "objectID": "clasificacion_incidentes.html",
    "href": "clasificacion_incidentes.html",
    "title": "Classification — Incidentes",
    "section": "",
    "text": "Problemática: detectar si un incidente será un “accidente con heridos” usando solo variables numéricas\n\n\nCode\n# 1. Importar librerías\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    ConfusionMatrixDisplay, RocCurveDisplay\n)\n\n# 2. Cargar dataset\ndf = pd.read_csv(\"data/incidentes_clasificacion_ready.csv\")\n\ny = df[\"accidente_con_heridos\"]\nX = df.drop(columns=[\"accidente_con_heridos\"])\n\nprint(\"Dimensiones:\", X.shape)\nprint(\"Distribución de clases:\")\nprint(y.value_counts())\n\n# 3. Dividir en train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 4. Entrenar modelo SIN pipeline\nmodel = LogisticRegression(max_iter=100000, class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\n\n# 5. Predicciones\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:,1]\n\n# 6. Métricas\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n==== Resultados SIN Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\n# 7. Matriz de confusión\nConfusionMatrixDisplay.from_predictions(y_test, y_pred)\nplt.title(\"Matriz de Confusión — Sin Pipeline\")\nplt.show()\n\n# 8. Curva ROC\nRocCurveDisplay.from_predictions(y_test, y_proba)\nplt.title(\"Curva ROC — Sin Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# ======== CON PIPELINE ========\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=100000, class_weight=\"balanced\"))\n])\n\npipe.fit(X_train, y_train)\ny_pred_pipe = pipe.predict(X_test)\ny_proba_pipe = pipe.predict_proba(X_test)[:,1]\n\naccuracy = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe, zero_division=0)\nrecall = recall_score(y_test, y_pred_pipe, zero_division=0)\nf1 = f1_score(y_test, y_pred_pipe, zero_division=0)\n\nprint(\"\\n==== Resultados CON Pipeline ====\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_pipe)\nplt.title(\"Matriz de Confusión — Con Pipeline\")\nplt.show()\n\nRocCurveDisplay.from_predictions(y_test, y_proba_pipe)\nplt.title(\"Curva ROC — Con Pipeline\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\nDimensiones: (268065, 4)\nDistribución de clases:\naccidente_con_heridos\n0    265740\n1      2325\nName: count, dtype: int64\n\n==== Resultados SIN Pipeline ====\nAccuracy : 0.5142\nPrecision: 0.0107\nRecall   : 0.6022\nF1 Score : 0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== Resultados CON Pipeline ====\nAccuracy : 0.5142\nPrecision: 0.0107\nRecall   : 0.6022\nF1 Score : 0.0210"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proyecto Final — Machine Learning",
    "section": "",
    "text": "Bienvenido\nEste sitio contiene el desarrollo de dos tareas de aprendizaje supervisado:\n\nClasificación: incidentes (accidentes con heridos) con Regresión Logística.\nRegresión: (PM2.5) — lo agregaremos como segunda página.\n\n\nAutor: Noe Sanchez\nAño: 2025\nCurso: ML"
  }
]