{
  "hash": "71fd841f81fa36d5a8241e8f72dcb06c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Examen – ML con Tweets (TOXICITY)\"\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    code-fold: true\n    fig-align: center\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n# Dataset y objetivo\n\nEn este examen trabajamos con **1 500 tweets** en español (Ecuador) con metadatos y una columna continua **`TOXICITY`** (0–1) generada con Perspective API.  \nEl flujo incluye **EDA, preprocesamiento, clasificación, regresión y clustering**, más conclusiones.\n\n::: {#6812e307 .cell execution_count=1}\n``` {.python .cell-code}\n# --- Preparación general ---\nimport os, sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    classification_report, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay,\n    mean_absolute_error, mean_squared_error, r2_score\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\n\n# Stopwords\nimport nltk\nfrom nltk.corpus import stopwords as nltk_stop\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords', quiet=True)\nspanish_stop = nltk_stop.words('spanish')\n\n# Función para guardar figuras\ndef savefig(path, dpi=140):\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\n# Crear carpeta de salida\nos.makedirs(\"docs\", exist_ok=True)\n\n# --- Cargar datos ---\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/refs/heads/master/tweets/1500_tweets_con_toxicity.csv\"\ndf = pd.read_csv(url, low_memory=False)\n\nTARGET = \"toxicity_score\" if \"toxicity_score\" in df.columns else \"TOXICITY\"\nTEXT_COL = \"content\" if \"content\" in df.columns else df.select_dtypes(include=[\"object\",\"string\"]).columns[0]\n\nprint(\"✅ Dataset cargado:\", df.shape)\nprint(\"TARGET:\", TARGET)\nprint(\"Texto:\", TEXT_COL)\ndisplay(df.head(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✅ Dataset cargado: (1500, 27)\nTARGET: toxicity_score\nTexto: content\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweetId</th>\n      <th>tweetUrl</th>\n      <th>content</th>\n      <th>isReply</th>\n      <th>replyTo</th>\n      <th>createdAt</th>\n      <th>authorId</th>\n      <th>authorName</th>\n      <th>authorUsername</th>\n      <th>authorVerified</th>\n      <th>...</th>\n      <th>inReplyToId</th>\n      <th>Date</th>\n      <th>time_response</th>\n      <th>account_age_days</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>content_length</th>\n      <th>has_profile_picture</th>\n      <th>sentiment_polarity</th>\n      <th>toxicity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1878630970745900800</td>\n      <td>https://x.com/Pableins15/status/18786309707459...</td>\n      <td>@DanielNoboaOk @DiegoBorjaPC Lávate el hocico ...</td>\n      <td>True</td>\n      <td>DanielNoboaOk</td>\n      <td>2025-01-13 02:31:00</td>\n      <td>176948611</td>\n      <td>Pablo Balarezo</td>\n      <td>Pableins15</td>\n      <td>False</td>\n      <td>...</td>\n      <td>1878539079249547520</td>\n      <td>2025-01-12 20:26:32</td>\n      <td>364.466667</td>\n      <td>5261</td>\n      <td>2</td>\n      <td>0</td>\n      <td>309</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.543256</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1904041877503984128</td>\n      <td>https://x.com/solma1201/status/190404187750398...</td>\n      <td>@DanielNoboaOk De esa arrastrada no te levanta...</td>\n      <td>True</td>\n      <td>DanielNoboaOk</td>\n      <td>2025-03-24 05:25:00</td>\n      <td>1368663286582030336</td>\n      <td>Solma1201</td>\n      <td>solma1201</td>\n      <td>False</td>\n      <td>...</td>\n      <td>1904003201143115776</td>\n      <td>2025-03-24 02:51:52</td>\n      <td>153.133333</td>\n      <td>1399</td>\n      <td>1</td>\n      <td>0</td>\n      <td>70</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>0.426917</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1877463444649046016</td>\n      <td>https://x.com/Mediterran67794/status/187746344...</td>\n      <td>@LuisaGonzalezEc @RC5Oficial Protegiendo a los...</td>\n      <td>True</td>\n      <td>LuisaGonzalezEc</td>\n      <td>2025-01-09 21:12:00</td>\n      <td>1851005619106451712</td>\n      <td>Médico Escritor Filósofo Hermeneútico</td>\n      <td>Mediterran67794</td>\n      <td>False</td>\n      <td>...</td>\n      <td>1877158437236228352</td>\n      <td>2025-01-09 01:00:22</td>\n      <td>1211.633333</td>\n      <td>68</td>\n      <td>2</td>\n      <td>0</td>\n      <td>122</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>0.555970</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 27 columns</p>\n</div>\n```\n:::\n:::\n\n\n# EDA resumido\n\n::: {#5f0ad174 .cell execution_count=2}\n``` {.python .cell-code}\n# === EDA mínimo y a prueba de indentación ===\n\nfrom IPython.display import display\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nprint(\"Shape:\", df.shape)\nprint(\"\\nTipos (primeras 12):\\n\", df.dtypes.head(12))\n\n# Duplicados y nulos (top-10)\n\nprint(\"\\nDuplicados por fila completa:\", df.duplicated().sum())\nprint(\"\\nNulos por columna (top 10):\")\nprint(df.isna().sum().sort_values(ascending=False).head(10))\n\n# Descriptivos numéricos + mediana, q05, q95\n\nnum_df = df.select_dtypes(\"number\")\ndesc = num_df.describe().T\nstats_full = pd.concat([\ndesc,\nnum_df.median().rename(\"median\"),\nnum_df.quantile(0.05).rename(\"q05\"),\nnum_df.quantile(0.95).rename(\"q95\")\n], axis=1)\nprint(\"\\nEstadísticos numéricos (con mediana y q05/q95):\")\ndisplay(stats_full)\n\n# Histograma del TARGET (único gráfico necesario para continuar)\n\nfig, ax = plt.subplots(figsize=(5,3))\nax.hist(df[TARGET].dropna(), bins=30, edgecolor=\"white\")\nax.set_title(f\"Distribución de {TARGET}\")\nax.set_xlabel(TARGET); ax.set_ylabel(\"Frecuencia\")\nplt.tight_layout()\nsavefig(\"docs/eda_toxicity_hist.png\")\n\n# Frecuencia de palabras (top-15) sin condicionales\n\ncv = CountVectorizer(stop_words=spanish_stop, max_features=1000)\nXc = cv.fit_transform(df[TEXT_COL].fillna(\"\").astype(str))\ntokens = cv.get_feature_names_out()\nfreq = np.asarray(Xc.sum(axis=0)).ravel()\ntop = (pd.DataFrame({\"token\": tokens, \"freq\": freq})\n.sort_values(\"freq\", ascending=False)\n.head(15)\n.sort_values(\"freq\", ascending=True))  # para barh ascendente\n\nfig, ax = plt.subplots(figsize=(7,4))\nax.barh(top[\"token\"], top[\"freq\"])\nax.set_title(\"Tokens más frecuentes (top 15)\")\nax.set_xlabel(\"Frecuencia\")\nplt.tight_layout()\nsavefig(\"docs/eda_top_words.png\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape: (1500, 27)\n\nTipos (primeras 12):\n tweetId              int64\ntweetUrl            object\ncontent             object\nisReply               bool\nreplyTo             object\ncreatedAt           object\nauthorId             int64\nauthorName          object\nauthorUsername      object\nauthorVerified        bool\nauthorFollowers      int64\nauthorProfilePic    object\ndtype: object\n\nDuplicados por fila completa: 0\n\nNulos por columna (top 10):\nhashtags               1379\ntoxicity_score          153\nreplyTo                  10\nmentions                  1\nsentiment_polarity        0\nhas_profile_picture       0\ncontent_length            0\nhashtags_count            0\nmentions_count            0\naccount_age_days          0\ndtype: int64\n\nEstadísticos numéricos (con mediana y q05/q95):\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n      <th>median</th>\n      <th>q05</th>\n      <th>q95</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tweetId</th>\n      <td>1500.0</td>\n      <td>1.893421e+18</td>\n      <td>1.166584e+16</td>\n      <td>1.876011e+18</td>\n      <td>1.883283e+18</td>\n      <td>1.887876e+18</td>\n      <td>1.904588e+18</td>\n      <td>1.922145e+18</td>\n      <td>1.887876e+18</td>\n      <td>1.878436e+18</td>\n      <td>1.909080e+18</td>\n    </tr>\n    <tr>\n      <th>authorId</th>\n      <td>1500.0</td>\n      <td>9.838015e+17</td>\n      <td>7.900284e+17</td>\n      <td>1.159021e+06</td>\n      <td>9.690383e+08</td>\n      <td>1.318014e+18</td>\n      <td>1.710233e+18</td>\n      <td>1.908229e+18</td>\n      <td>1.318014e+18</td>\n      <td>1.731490e+08</td>\n      <td>1.880975e+18</td>\n    </tr>\n    <tr>\n      <th>authorFollowers</th>\n      <td>1500.0</td>\n      <td>3.625721e+03</td>\n      <td>1.184447e+05</td>\n      <td>0.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>4.300000e+01</td>\n      <td>1.992500e+02</td>\n      <td>4.577730e+06</td>\n      <td>4.300000e+01</td>\n      <td>0.000000e+00</td>\n      <td>1.151750e+03</td>\n    </tr>\n    <tr>\n      <th>conversationId</th>\n      <td>1500.0</td>\n      <td>1.893127e+18</td>\n      <td>1.161854e+16</td>\n      <td>1.876001e+18</td>\n      <td>1.883238e+18</td>\n      <td>1.887344e+18</td>\n      <td>1.904198e+18</td>\n      <td>1.910014e+18</td>\n      <td>1.887344e+18</td>\n      <td>1.877875e+18</td>\n      <td>1.908589e+18</td>\n    </tr>\n    <tr>\n      <th>inReplyToId</th>\n      <td>1500.0</td>\n      <td>1.893147e+18</td>\n      <td>1.162094e+16</td>\n      <td>1.876001e+18</td>\n      <td>1.883238e+18</td>\n      <td>1.887355e+18</td>\n      <td>1.904234e+18</td>\n      <td>1.910015e+18</td>\n      <td>1.887355e+18</td>\n      <td>1.877921e+18</td>\n      <td>1.908671e+18</td>\n    </tr>\n    <tr>\n      <th>time_response</th>\n      <td>1500.0</td>\n      <td>1.170038e+03</td>\n      <td>3.273930e+03</td>\n      <td>1.333333e-01</td>\n      <td>1.363000e+02</td>\n      <td>5.156250e+02</td>\n      <td>1.265717e+03</td>\n      <td>6.356900e+04</td>\n      <td>5.156250e+02</td>\n      <td>1.858000e+01</td>\n      <td>3.076286e+03</td>\n    </tr>\n    <tr>\n      <th>account_age_days</th>\n      <td>1500.0</td>\n      <td>2.271134e+03</td>\n      <td>1.984157e+03</td>\n      <td>-9.000000e+01</td>\n      <td>4.557500e+02</td>\n      <td>1.538000e+03</td>\n      <td>4.420750e+03</td>\n      <td>6.506000e+03</td>\n      <td>1.538000e+03</td>\n      <td>-1.505000e+01</td>\n      <td>5.271200e+03</td>\n    </tr>\n    <tr>\n      <th>mentions_count</th>\n      <td>1500.0</td>\n      <td>1.723333e+00</td>\n      <td>9.462483e-01</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+01</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>3.000000e+00</td>\n    </tr>\n    <tr>\n      <th>hashtags_count</th>\n      <td>1500.0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>content_length</th>\n      <td>1500.0</td>\n      <td>1.165280e+02</td>\n      <td>7.749371e+01</td>\n      <td>1.700000e+01</td>\n      <td>5.700000e+01</td>\n      <td>9.600000e+01</td>\n      <td>1.500000e+02</td>\n      <td>6.840000e+02</td>\n      <td>9.600000e+01</td>\n      <td>3.695000e+01</td>\n      <td>2.900500e+02</td>\n    </tr>\n    <tr>\n      <th>sentiment_polarity</th>\n      <td>1500.0</td>\n      <td>-7.906765e-03</td>\n      <td>1.197964e-01</td>\n      <td>-1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>toxicity_score</th>\n      <td>1347.0</td>\n      <td>2.538788e-01</td>\n      <td>2.439420e-01</td>\n      <td>1.939886e-03</td>\n      <td>2.844395e-02</td>\n      <td>1.883923e-01</td>\n      <td>4.269174e-01</td>\n      <td>9.391453e-01</td>\n      <td>1.883923e-01</td>\n      <td>4.438961e-03</td>\n      <td>7.495444e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n![](docs/eda_toxicity_hist.png){width=70%}\n![](docs/eda_top_words.png){width=70%}\n![](docs/eda_cat_bar.png){width=70%}\n![](docs/eda_scatter.png){width=60%}\n\n**Hallazgos (EDA):**\n1) **Calidad de datos:** sin duplicados completos; nulos concentrados en `hashtags` y en menor medida en `toxicity_score`.  \n2) **Distribución de `toxicity_score`:** rango [0,1]; la forma de la distribución afecta el balance al binarizar (umbral 0.5).  \n3) **Categóricas y texto:** `source` está concentrada en pocas categorías; el top de tokens muestra vocabulario útil para representación TF-IDF.\n\n\n# Preprocesamiento\n\n::: {#ab644149 .cell execution_count=3}\n``` {.python .cell-code}\n# === Preprocesamiento: limpieza de texto (versión estable y sin errores) ===\n\nimport pandas as pd\nimport re\n\n# Copiamos el texto original a una variable temporal\n\ns = df[TEXT_COL].fillna(\"\").astype(str).str.lower()\n\n# 1. Elimina URLs (http, https, www)\n\ns = s.str.replace(r'https?://\\S+|[www.\\S+](http://www.\\S+)', ' ', regex=True)\n\n# 2. Elimina menciones @usuario\n\ns = s.str.replace(r'@\\w+', ' ', regex=True)\n\n# 3. Quita el símbolo # pero deja la palabra\n\ns = s.str.replace('#', ' ', regex=False)\n\n# 4. Deja solo letras (con acentos y ñ/ü) y espacios\n\ns = s.str.replace(r'[^a-záéíóúñü\\s]', ' ', regex=True)\n\n# 5. Colapsa espacios múltiples\n\ns = s.str.replace(r'\\s+', ' ', regex=True).str.strip()\n\n# Crea la nueva columna limpia\n\ndf[\"content_clean\"] = s\n\nTEXT_USED = \"content_clean\"   # columna limpia para TF-IDF\nprint(\"✅ Texto final para modelado:\", TEXT_USED)\nprint(df[TEXT_USED].head(3))\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✅ Texto final para modelado: content_clean\n0    lávate el hocico presidente de cartón habla la...\n1    de esa arrastrada no te levantas nunca chao ca...\n2    protegiendo a los narcotraficantes criminales ...\nName: content_clean, dtype: object\n```\n:::\n:::\n\n\n::: {#61c244b8 .cell execution_count=4}\n``` {.python .cell-code}\n# === Preprocesamiento: columnas y transformador ===\n\nnum_cols = [c for c in df.select_dtypes(\"number\").columns if c != TARGET]\ncat_cols = [c for c in df.select_dtypes(include=[\"object\",\"string\",\"category\"]).columns\nif c not in [TEXT_COL, TEXT_USED]]\n\nprint(\"Numéricas:\", len(num_cols))\nprint(\"Categóricas:\", len(cat_cols))\nprint(\"Texto:\", TEXT_USED)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\ntfidf = TfidfVectorizer(\nstop_words=spanish_stop,\nngram_range=(1, 2),\nmin_df=2,\nmax_df=0.95\n)\n\npreproc = ColumnTransformer(\ntransformers=[\n(\"text\", tfidf, TEXT_USED),\n(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n(\"num\", StandardScaler(with_mean=False), num_cols),\n],\nremainder=\"drop\",\nsparse_threshold=0.3\n)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNuméricas: 11\nCategóricas: 11\nTexto: content_clean\n```\n:::\n:::\n\n\n**Decisiones de preprocesamiento:**\n- **Texto:** limpieza mínima (URLs, @, `#`, caracteres no alfabéticos, espacios) → columna `content_clean`.  \n- **Representación:** **TF-IDF (1–2-gramas)** con stopwords en español: balance entre interpretabilidad y desempeño.  \n- **Categóricas:** **OneHotEncoder(handle_unknown=\"ignore\")** para robustez ante categorías nuevas.  \n- **Numéricas:** **StandardScaler(with_mean=False)** para integrarse sin conflictos a la matriz esparsa.\n- **Estructura:** **ColumnTransformer + Pipeline** para reproducibilidad y para evitar fugas de datos entre train/test.\n\n\n# Clasificación (binaria a partir de TOXICITY)\n\n::: {#26599c6f .cell execution_count=5}\n``` {.python .cell-code}\n# === Clasificación binaria: umbral 0.5 sobre el TARGET ===\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\nclassification_report, ConfusionMatrixDisplay, RocCurveDisplay, roc_auc_score\n)\n\n# Dataset de clasificación\n\ndf_cls = df.dropna(subset=[TARGET]).copy()\ndf_cls[\"toxic_cls\"] = (df_cls[TARGET] >= 0.5).astype(int)\n\nX = df_cls.drop(columns=[TARGET, \"toxic_cls\"])\ny = df_cls[\"toxic_cls\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Modelo: preproc (TF-IDF+OHE+escala) + Regresión Logística\n\npipe_cls = Pipeline([\n(\"prep\", preproc),\n(\"clf\", LogisticRegression(max_iter=10000))\n])\n\npipe_cls.fit(X_train, y_train)\ny_pred  = pipe_cls.predict(X_test)\ny_proba = pipe_cls.predict_proba(X_test)[:, 1]   # <- sin if/try\n\nprint(\"=== Reporte de Clasificación (umbral 0.5) ===\")\nprint(classification_report(y_test, y_pred, digits=3))\n\n# Matriz de confusión\n\nfig, ax = plt.subplots(figsize=(4,4))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.set_title(\"Matriz de confusión — Clasificación\")\nplt.tight_layout()\nplt.savefig(\"docs/confmat_cls.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n# Curva ROC + AUC\n\nfig, ax = plt.subplots(figsize=(5,4))\nRocCurveDisplay.from_predictions(y_test, y_proba, ax=ax)\nax.set_title(\"Curva ROC — Clasificación\")\nplt.tight_layout()\nplt.savefig(\"docs/roc_cls.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\nauc = roc_auc_score(y_test, y_proba)\nprint(f\"ROC-AUC: {auc:.3f}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== Reporte de Clasificación (umbral 0.5) ===\n              precision    recall  f1-score   support\n\n           0      0.828     0.982     0.899       221\n           1      0.500     0.082     0.140        49\n\n    accuracy                          0.819       270\n   macro avg      0.664     0.532     0.519       270\nweighted avg      0.769     0.819     0.761       270\n\nROC-AUC: 0.664\n```\n:::\n:::\n\n\n**Matriz de confusión**  \n![](docs/confmat_cls.png){width=60%}\n\n**Curva ROC**  \n![](docs/roc_cls.png){width=60%}\n\n**Justificación — Clasificación:**\n- **Target binario:** `toxicity_score >= 0.5` como umbral neutro en [0,1]. Si el costo de FN/FP cambia, puede ajustarse el umbral con la curva ROC/PR.  \n- **Métricas:**  \n  - *Accuracy* como línea base.  \n  - *Precision/Recall/F1 (macro)* para evaluar equilibrio entre clases.  \n  - *ROC-AUC* para medir ranking independiente del umbral.  \n- **Lectura de la matriz de confusión:** identifica sesgos del modelo (por ejemplo, FP altos).  \n- **Curva ROC:** permite seleccionar umbral operativo según trade-off TPR/FPR.\n\n\n# Regresión (TOXICITY continua)\n\n::: {#569cd93a .cell execution_count=6}\n``` {.python .cell-code}\n# === Regresión: TARGET continuo ===\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\ndf_reg = df.dropna(subset=[TARGET]).copy()\nXr = df_reg.drop(columns=[TARGET])\nyr = df_reg[TARGET].astype(float)\n\nXr_train, Xr_test, yr_train, yr_test = train_test_split(\nXr, yr, test_size=0.2, random_state=42\n)\n\npipe_reg = Pipeline([\n(\"prep\", preproc),\n(\"reg\", Ridge(alpha=1.0))\n])\n\npipe_reg.fit(Xr_train, yr_train)\nyr_pred = pipe_reg.predict(Xr_test)\n\nmae = mean_absolute_error(yr_test, yr_pred)\nmse = mean_squared_error(yr_test, yr_pred)      # compatibilidad amplia\nrmse = np.sqrt(mse)\nr2 = r2_score(yr_test, yr_pred)\n\nprint(f\"MAE : {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"R²  : {r2:.4f}\")\n\n# Dispersión real vs predicho\n\nfig, ax = plt.subplots(figsize=(5,4))\nax.scatter(yr_test, yr_pred, s=8, alpha=0.6)\nax.plot([0,1],[0,1], ls=\"--\", c=\"gray\")\nax.set_xlabel(f\"{TARGET} real\"); ax.set_ylabel(f\"{TARGET} predicha\")\nax.set_title(\"Regresión – Real vs Predicho\")\nplt.tight_layout()\nplt.savefig(\"docs/reg_scatter.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n# Histograma de residuales\n\nres = yr_test - yr_pred\nfig, ax = plt.subplots(figsize=(6,4))\nax.hist(res, bins=40, edgecolor=\"white\")\nax.set_title(\"Distribución de residuales (y_real - y_pred)\")\nax.set_xlabel(\"Residual\"); ax.set_ylabel(\"Frecuencia\")\nplt.tight_layout()\nplt.savefig(\"docs/reg_resid_hist.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAE : 0.1622\nRMSE: 0.1963\nR²  : 0.3279\n```\n:::\n:::\n\n\n**Regresión – Real vs. Predicho**  \n![](docs/reg_scatter.png){width=60%}\n\n**Distribución de residuales**  \n![](docs/reg_resid_hist.png){width=60%}\n\n**Justificación — Regresión:**\n- *MAE* (interpretable en unidades de toxicidad; robusto a outliers).  \n- *RMSE* (penaliza más los errores grandes).  \n- *R²* (proporción de varianza explicada).  \n- El **scatter real vs. predicho** revela sesgo/varianza; el **histograma de residuales** debería centrarse en 0 sin colas extremas.\n\n\n# Clustering (K-Means sobre texto)\n\n::: {#a00049e5 .cell execution_count=7}\n``` {.python .cell-code}\n# === Clustering sobre texto: TF-IDF (solo texto limpio) + SVD(2D) para visualizar ===\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Asegúrate de que TEXT_USED exista (viene del preprocesamiento); si no, usa TEXT_COL\n\nTEXT_FOR_CLUST = \"content_clean\" if \"content_clean\" in df.columns else TEXT_COL\n\n# Usamos solo filas con TARGET no nulo para poder comparar con la clase binaria\n\ndf_cls = df.dropna(subset=[TARGET]).copy()\ndf_cls[\"toxic_cls\"] = (df_cls[TARGET] >= 0.5).astype(int)\n\n# TF-IDF SOLO del texto limpio\n\ntfidf_only = TfidfVectorizer(stop_words=spanish_stop, ngram_range=(1,2), min_df=2, max_df=0.95)\nX_text = tfidf_only.fit_transform(df_cls[TEXT_FOR_CLUST].fillna(\"\").astype(str))\n\n# Reducción a 2D para graficar\n\nsvd = TruncatedSVD(n_components=2, random_state=42)\nX_2d = svd.fit_transform(X_text)\n\n# K-Means (k=3 a modo de ejemplo)\n\nk = 3\nkm = KMeans(n_clusters=k, random_state=42, n_init=10)\nlabels = km.fit_predict(X_text)\n\n# DataFrame para visualización\n\nviz = pd.DataFrame({\n\"x\": X_2d[:, 0],\n\"y\": X_2d[:, 1],\n\"cluster\": labels.astype(str),\n\"toxic_cls\": df_cls[\"toxic_cls\"].values\n})\n\n# Dispersión 2D coloreada por cluster (sin bucle, sin problemas de indentación)\nimport numpy as np\nfrom matplotlib.lines import Line2D\n\nfig, ax = plt.subplots(figsize=(5, 4))\n\n# codificamos cluster -> color (0,1,2,...) y elegimos un cmap\ncodes = pd.Categorical(viz[\"cluster\"]).codes\nsc = ax.scatter(viz[\"x\"], viz[\"y\"], s=8, alpha=0.7, c=codes, cmap=\"tab10\")\n\n# leyenda manual\nuniq = sorted(viz[\"cluster\"].unique())\nhandles = [Line2D([0], [0], marker='o', linestyle='', markersize=6, \n                  color=plt.cm.tab10(i/10), label=f\"C{cl}\") \n           for i, cl in enumerate(uniq)]\nax.legend(handles=handles, title=\"Cluster\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n\nax.set_title(\"K-Means (TF-IDF limpio) + SVD(2D)\")\nplt.tight_layout()\nplt.savefig(\"docs/kmeans_2d.png\", dpi=140, bbox_inches=\"tight\")\nplt.close()\n\n# Tabla de comparación cluster vs clase (filas normalizadas)\n\ntabla = pd.crosstab(viz[\"cluster\"], viz[\"toxic_cls\"], normalize=\"index\").round(3)\nprint(\"Distribución de clases por cluster (filas normalizadas):\")\nprint(tabla.to_string())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistribución de clases por cluster (filas normalizadas):\ntoxic_cls      0      1\ncluster                \n0          0.815  0.185\n1          0.813  0.187\n2          0.976  0.024\n```\n:::\n:::\n\n\n**K-Means sobre TF-IDF limpio (SVD 2D)**  \n![](docs/kmeans_2d.png){width=60%}\n\n**Lectura — Clustering:**\n- **Silhouette/inercia** (opcional) orientan la calidad intrínseca; aquí mostramos la separación visual en 2D (SVD).  \n- La **crosstab cluster vs. clase** indica si los clusters se alinean con la toxicidad o capturan otros ejes (temas, cuentas, contexto).  \n- Útil para exploración y segmentación previa a modelos supervisados.\n\n\n# Conclusiones\n\n**Conclusiones:**\n- **Datos:** sin duplicados; nulos manejables; texto utilizable tras limpieza ligera.  \n- **Clasificación:** modelo base (LogReg) con TF-IDF + OHE + escala; métricas reportadas y visualizaciones (confusión/ROC).  \n- **Regresión:** errores (MAE/RMSE) y ajuste (R²) razonables para una primera pasada; residuales sin patrones fuertes.  \n- **Clustering:** agrupaciones comprensibles; alineación parcial con la toxicidad.  \n**Futuro:** ajustar umbral por costos, *class_weight='balanced'*, tuning de hiperparámetros, embeddings (sentence-transformers) y modelos no lineales (árboles/GBM).\n\n",
    "supporting": [
      "examen_ml_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}